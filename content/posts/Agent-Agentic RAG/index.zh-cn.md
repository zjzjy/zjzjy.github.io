---
weight: 1
title: "Agentic RAG - Usecase"
subtitle: ""
date: 2025-06-22T12:03:24+08:00
draft: false
author: "June"
authorLink: "https://github.com/zjzjy"
description: ""
images: []
resources:
- name: "featured-image"
  src: "featured-image.jpg"

tags: ["Agent","Agentic RAG"]
categories: ["Agent"]
lightgallery: true

hiddenFromHomePage: false
hiddenFromSearch: false

featuredImage: ""
featuredImagePreview: ""

license: '<a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a>'
toc:
  auto: false
---
æœ¬æ–‡æ ¹æ®[Hugging Faceä¸Šçš„Agentè¯¾ç¨‹](https://huggingface.co/learn/agents-course/unit3/agentic-rag/introduction)ç¼–å†™è€Œæˆï¼ŒåŒ…æ‹¬        ã€‚
åœ¨æœ¬ç« èŠ‚ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Agentic RAG åˆ›å»ºä¸€ä¸ªå·¥å…·æ¥å¸®åŠ©ä¸»æŒæ™šä¼šçš„å‹å¥½ç»çºªäºº Alfredï¼Œè¯¥å·¥å…·å¯ç”¨äºå›ç­”æœ‰å…³æ™šä¼šå˜‰å®¾çš„é—®é¢˜ã€‚
# éš¾å¿˜çš„ç››ä¼š
ä½ å†³å®šä¸¾åŠä¸€åœºæœ¬ä¸–çºªæœ€å¥¢åã€æœ€å¥¢åçš„æ´¾å¯¹ã€‚ è¿™æ„å‘³ç€ä¸°ç››çš„å®´å¸­ã€è¿·äººçš„èˆè€…ã€çŸ¥å DJã€ç²¾è‡´çš„é¥®å“ã€ä»¤äººå¹ä¸ºè§‚æ­¢çš„çƒŸç«è¡¨æ¼”ç­‰ç­‰ã€‚æˆ‘ä»¬å§”æ‰˜ç®¡å®¶Alfredæ¥å…¨æƒä¸¾åŠè¿™ä¸ªç››ä¼šã€‚ä¸ºæ­¤ï¼Œä»–éœ€è¦æŒæ¡æ´¾å¯¹çš„æ‰€æœ‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬èœå•ã€å®¾å®¢ã€æ—¥ç¨‹å®‰æ’ã€å¤©æ°”é¢„æŠ¥ç­‰ç­‰ï¼ä¸ä»…å¦‚æ­¤ï¼Œä»–è¿˜éœ€è¦ç¡®ä¿èšä¼šå–å¾—æˆåŠŸï¼Œå› æ­¤ä»–éœ€è¦èƒ½å¤Ÿåœ¨èšä¼šæœŸé—´å›ç­”æœ‰å…³èšä¼šçš„ä»»ä½•é—®é¢˜ ï¼ŒåŒæ—¶å¤„ç†å¯èƒ½å‡ºç°çš„æ„å¤–æƒ…å†µã€‚ä»–æ— æ³•ç‹¬è‡ªå®Œæˆè¿™é¡¹å·¥ä½œï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ç¡®ä¿é˜¿å°”å¼—é›·å¾·èƒ½å¤Ÿè·å¾—ä»–æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯å’Œå·¥å…·ã€‚  
é¦–å…ˆï¼Œæˆ‘ä»¬ç»™ä»–åˆ—ä¸€ä»½è”æ¬¢æ™šä¼šçš„ç¡¬æ€§è¦æ±‚æ¸…å•ï¼šåœ¨æ–‡è‰ºå¤å…´æ—¶æœŸï¼Œå—è¿‡è‰¯å¥½æ•™è‚²çš„äººéœ€è¦å…·å¤‡ä¸‰ä¸ªä¸»è¦ç‰¹è´¨ï¼šå¯¹ä½“è‚²ã€æ–‡åŒ–å’Œç§‘å­¦çŸ¥è¯†çš„æ·±åšé€ è¯£ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ç”¨æˆ‘ä»¬çš„çŸ¥è¯†ç»™å®¾å®¢ç•™ä¸‹æ·±åˆ»å°è±¡ï¼Œä¸ºä»–ä»¬æ‰“é€ ä¸€åœºçœŸæ­£éš¾å¿˜çš„ç››ä¼šã€‚ç„¶è€Œï¼Œä¸ºäº†é¿å…å†²çªï¼Œ åœ¨ç››ä¼šä¸Šåº”è¯¥é¿å…è®¨è®ºæ”¿æ²»å’Œå®—æ•™ç­‰è¯é¢˜ã€‚ ç››ä¼šéœ€è¦å……æ»¡ä¹è¶£ï¼Œé¿å…ä¸ä¿¡ä»°å’Œç†æƒ³ç›¸å…³çš„å†²çªã€‚æŒ‰ç…§ç¤¼ä»ªï¼Œ ä¸€ä½å¥½çš„ä¸»äººåº”è¯¥äº†è§£å®¾å®¢çš„èƒŒæ™¯ ï¼ŒåŒ…æ‹¬ä»–ä»¬çš„å…´è¶£å’Œäº‹ä¸šã€‚ä¸€ä½å¥½çš„ä¸»äººä¹Ÿä¼šä¸å®¾å®¢ä»¬é—²èŠå…«å¦ï¼Œåˆ†äº«ä»–ä»¬çš„æ•…äº‹ã€‚æœ€åï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿è‡ªå·±æŒæ¡ä¸€äº›å¤©æ°”å¸¸è¯† ï¼Œä»¥ä¾¿èƒ½å¤ŸæŒç»­è·å¾—å®æ—¶æ›´æ–°ï¼Œç¡®ä¿åœ¨æœ€ä½³æ—¶æœºç‡ƒæ”¾çƒŸèŠ±ï¼Œå¹¶ä»¥ä¸€å£°å·¨å“ç»“æŸåº†å…¸ï¼ğŸ†
# åˆ›å»ºå·¥å…·
é¦–å…ˆï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª RAG å·¥å…·ï¼Œç”¨äºæ£€ç´¢å—é‚€è€…çš„æœ€æ–°è¯¦ç»†ä¿¡æ¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¼€å‘ç”¨äºç½‘é¡µæœç´¢ã€å¤©æ°”æ›´æ–°å’Œ Hugging Face Hub æ¨¡å‹ä¸‹è½½ç»Ÿè®¡çš„å·¥å…·ã€‚
## ä¸ºæ¥å®¾åˆ›å»º RAG å·¥å…·
æˆ‘ä»¬å°†åœ¨[HF Space](https://huggingface.co/spaces/agents-course/Unit_3_Agentic_RAG)å¼€å‘æˆ‘ä»¬çš„Agentã€‚
- tools.pyï¼šä¸ºAgentæä¾›è¾…åŠ©å·¥å…·ã€‚
- retriever.pyï¼šå®ç°æ£€ç´¢åŠŸèƒ½ï¼Œæ”¯æŒçŸ¥è¯†è®¿é—®ã€‚
- app.pyï¼šå°†æ‰€æœ‰ç»„ä»¶é›†æˆåˆ°åŠŸèƒ½é½å…¨çš„agentä¸­ã€‚


ä½¿ç”¨çš„[dataset](https://huggingface.co/datasets/agents-course/unit3-invitees)ï¼Œæ¯ä¸ªè®¿å®¢åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- Name: å®¢äººçš„å…¨å
- Relation: å®¢äººä¸ä¸»äººçš„å…³ç³»
- Descriptionï¼šå…³äºå®¢äººçš„ç®€çŸ­ä¼ è®°æˆ–æœ‰è¶£çš„äº‹å®
- Email Addressï¼šå‘é€é‚€è¯·æˆ–åç»­æ´»åŠ¨çš„è”ç³»ä¿¡æ¯

### Step1: åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†
æˆ‘ä»¬æä¾›äº†ä¸‰ç§ä¸åŒ Agent åº“çš„å®ç°æ–¹å¼ï¼Œä½ å¯ä»¥å±•å¼€ä¸‹é¢çš„æŠ˜å æ¡†æŸ¥çœ‹å„è‡ªçš„ä»£ç ã€‚
{{< admonition type=note title="smolagents" open=false >}}
æˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face datasets é›†åº“æ¥åŠ è½½æ•°æ®é›†å¹¶å°†å…¶è½¬æ¢ä¸ºæ¥è‡ª langchain.docstore.document æ¨¡å—çš„ Document å¯¹è±¡åˆ—è¡¨ã€‚
```python
import datasets
from langchain.docstore.document import Document

# Load the dataset
guest_dataset = datasets.load_dataset("agents-course/unit3-invitees", split="train")

# Convert dataset entries into document objects
docs = [
  Document(
    page_content = "\n".join([
      f"Name: {guest['name']}",
      f"Relation: {guest['relation']}",
      f"Description: {guest['description']}",
      f"Email: {guest['email']}"
    ]),
    metadata={"name": guest["name"]}
  )
  for guest in guest_dataset
]
```
{{< /admonition >}}

{{< admonition type=note title="llama-index" open=false >}}
æˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face datasets é›†åº“æ¥åŠ è½½æ•°æ®é›†å¹¶å°†å…¶è½¬æ¢ä¸ºæ¥è‡ª llama_index.core.schema æ¨¡å—çš„ Document å¯¹è±¡åˆ—è¡¨ã€‚
```python
import datasets
from llama_index.core.schema import Document

# Load the dataset
guest_dataset = datasets.load_dataset("agents-course/unit3-invitees", split="train")

# Convert dataset entries into Document objects
docs = [
    Document(
        text="\n".join([
            f"Name: {guest_dataset['name'][i]}",
            f"Relation: {guest_dataset['relation'][i]}",
            f"Description: {guest_dataset['description'][i]}",
            f"Email: {guest_dataset['email'][i]}"
        ]),
        metadata={"name": guest_dataset['name'][i]}
    )
    for i in range(len(guest_dataset))
]
```
{{< /admonition >}}

{{< admonition type=note title="langgraph" open=false >}}
æˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face datasets é›†åº“æ¥åŠ è½½æ•°æ®é›†å¹¶å°†å…¶è½¬æ¢ä¸ºæ¥è‡ª langchain.docstore.document æ¨¡å—çš„ Document å¯¹è±¡åˆ—è¡¨ã€‚
```python
import datasets
from langchain.docstore.document import Document

# Load the dataset
guest_dataset = datasets.load_dataset("agents-course/unit3-invitees", split="train")

# Convert dataset entries into document objects
docs = [
  Document(
    page_content = "\n".join([
      f"Name: {guest['name']}",
      f"Relation: {guest['relation']}",
      f"Description: {guest['description']}",
      f"Email: {guest['email']}"
    ]),
    metadata={"name": guest["name"]}
  )
  for guest in guest_dataset
]
```
{{< /admonition >}}

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬ï¼šåŠ è½½æ•°æ®é›†ï¼Œå°†æ¯ä¸ªå®¢äººæ¡ç›®è½¬æ¢ä¸ºå…·æœ‰æ ¼å¼åŒ–å†…å®¹çš„ Document å¯¹è±¡ï¼Œå°† Document å¯¹è±¡å­˜å‚¨åœ¨åˆ—è¡¨ä¸­ã€‚

### Step2: åˆ›å»ºæ£€ç´¢å·¥å…·
{{< admonition type=note title="smolagents" open=false >}}
æˆ‘ä»¬å°†ä½¿ç”¨ langchain_community.retrievers æ¨¡å—ä¸­çš„ BM25Retriever æ¥åˆ›å»ºæ£€ç´¢å·¥å…·ã€‚BM25æ˜¯ç›¸å…³æ€§æœç´¢ï¼Œå¦‚æœè¦æ›´é«˜çº§çš„è¯­ä¹‰æœç´¢ï¼Œå¯ä»¥è€ƒè™‘embeddingæ£€ç´¢å™¨ï¼Œä¾‹å¦‚[sentence-transformers ](https://www.sbert.net/)ã€‚
```python
from solagents import Tool
from langchain_community.retrievers import BM25Retriever

class GuestInfoRetrieverTool(Tool):
  # å·¥å…·çš„å…ƒæ•°æ®æè¿°
  name = "guest_info_retriever"
  description = "Retrieves detailed information about gala guests based on their name or relation."
  inputs = {
    "query": {
      "type": "string",
      "description": "The name or relation of the guest you want information about."
    }
  }
  output_type = "string"

  def __init__(self, docs):
    self.is_initialized = False
    self.retriever = BM25Retriever.from_document(docs)

  def forward(self, query: str):
    results = self.retriever.get_relevant_documents(query)
    if results:
      return "\n\n".join([doc.page_content for doc in results[:3]])
    else:
      return "No matching guest information found."

# Initialize the tool
guest_info_tool = GuestInfoRetrieverTool(docs)
```
{{< /admonition >}}

{{< admonition type=note title="llama-index" open=false >}}
```python
from llama_index.core.tools import FunctionTool
from llama_index.retrievers.bm25 import BM25Retriever

bm25_retriever = BM25Retriever.from_defaults(nodes = docs)

def get_guest_info_retriever(query: str) -> str:
  """Retrieves detailed information about gala guests based on their name or relation."""
  results = bm25_retriever(query)
  if results:
        return "\n\n".join([doc.text for doc in results[:3]])
  else:
        return "No matching guest information found."

# Initialize the tool
guest_info_tool = FunctionTool.from_defaults(get_guest_info_retriever) 
```
{{< /admonition >}}

{{< admonition type=note title="langgraph" open=false >}}
```python
from langchain_community.retrievers import BM25Retriever
from langchain.tools import Tool

bm25_retriever = BM25Retriever.from_documents(docs)

def extract_text(query: str) -> str:
    """Retrieves detailed information about gala guests based on their name or relation."""
    results = bm25_retriever.invoke(query)
    if results:
        return "\n\n".join([doc.page_content for doc in results[:3]])
    else:
        return "No matching guest information found."

guest_info_tool = Tool(
    name="guest_info_retriever",
    func=extract_text,
    description="Retrieves detailed information about gala guests based on their name or relation."
)
```
{{< /admonition >}}

### Step3ï¼šå°†å·¥å…·ä¸Alfredé›†æˆ
æœ€åï¼Œè®©æˆ‘ä»¬é€šè¿‡åˆ›å»ºä»£ç†å¹¶ä¸ºå…¶é…å¤‡è‡ªå®šä¹‰å·¥å…·æ¥å°†æ‰€æœ‰å†…å®¹æ•´åˆåœ¨ä¸€èµ·ï¼š
{{< admonition type=note title="smolagents" open=false >}}
```python
from smolagents import CodeAgent, InferenceClientModel

# Initialize the Hugging Face model
model = InferenceClientModel()

# Create Alfred, our gala agent, with the guest info tool
alfred = CodeAgent(tools=[guest_info_tool], model=model)

# Example query Alfred might receive during the gala
response = alfred.run("Tell me about our guest named 'Lady Ada Lovelace'.")

print("ğŸ© Alfred's Response:")
print(response)
#ğŸ© Alfred's Response:
#Based on the information I retrieved, Lady Ada Lovelace is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine. Her email address is ada.lovelace@example.com.
```
{{< /admonition >}}

{{< admonition type=note title="llama-index" open=false >}}
```python
from llama_index.core.agent.workflow import AgentWorkflow
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

# Initialize the Hugging Face model
llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")

# Create Alfred, our gala agent, with the guest info tool
alfred = AgentWorkflow.from_tools_or_functions(
    [guest_info_tool],
    llm=llm,
)

# Example query Alfred might receive during the gala
response = await alfred.run("Tell me about our guest named 'Lady Ada Lovelace'.")

print("ğŸ© Alfred's Response:")
print(response)
#ğŸ© Alfred's Response:
#Lady Ada Lovelace is an esteemed mathematician and friend, renowned for her pioneering work in mathematics and computing. She is celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine. Her email is ada.lovelace@example.com.
```
{{< /admonition >}}

{{< admonition type=note title="langgraph" open=false >}}
```python
from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage
from langgraph.prebuilt import ToolNode
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import tools_condition
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace

# Generate the chat interface, including the tools
llm = HuggingFaceEndpoint(
    repo_id="Qwen/Qwen2.5-Coder-32B-Instruct",
    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
)

chat = ChatHuggingFace(llm=llm, verbose=True)
tools = [guest_info_tool]
chat_with_tools = chat.bind_tools(tools)

# Generate the AgentState and Agent graph
class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

def assistant(state: AgentState):
    return {
        "messages": [chat_with_tools.invoke(state["messages"])],
    }

## The graph
builder = StateGraph(AgentState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine how the control flow moves
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message requires a tool, route to tools
    # Otherwise, provide a direct response
    tools_condition,
)
builder.add_edge("tools", "assistant")
alfred = builder.compile()

messages = [HumanMessage(content="Tell me about our guest named 'Lady Ada Lovelace'.")]
response = alfred.invoke({"messages": messages})

print("ğŸ© Alfred's Response:")
print(response['messages'][-1].content)
#ğŸ© Alfred's Response:
#Lady Ada Lovelace is an esteemed mathematician and pioneer in computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.
```
{{< /admonition >}}

æœ€åä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆï¼š

æˆ‘ä»¬ä½¿ç”¨ HuggingFaceEndpoint ç±»åˆå§‹åŒ– HuggingFace æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜ç”Ÿæˆäº†ä¸€ä¸ªèŠå¤©ç•Œé¢å¹¶é™„åŠ äº†ä¸€äº›å·¥å…·ã€‚

æˆ‘ä»¬å°†ä»£ç†ï¼ˆAlfredï¼‰åˆ›å»ºä¸º StateGraph ï¼Œå®ƒä½¿ç”¨è¾¹ç»„åˆ 2 ä¸ªèŠ‚ç‚¹ï¼ˆ assistant ã€ tools ï¼‰

æˆ‘ä»¬è¦æ±‚é˜¿å°”å¼—é›·å¾·æ£€ç´¢æœ‰å…³ä¸€ä½åå«â€œLady Ada Lovelaceâ€çš„å®¢äººçš„ä¿¡æ¯ã€‚


ç°åœ¨ Alfred å¯ä»¥æ£€ç´¢å®¢äººä¿¡æ¯ï¼Œè¯·è€ƒè™‘å¦‚ä½•å¢å¼ºæ­¤ç³»ç»Ÿï¼š
- æ”¹è¿›æ£€ç´¢å™¨ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•ï¼Œä¾‹å¦‚å¥å­è½¬æ¢å™¨
- å®ç°å¯¹è¯è®°å¿† ï¼Œä»¥ä¾¿ Alfred è®°ä½ä¹‹å‰çš„äº’åŠ¨
- ç»“åˆç½‘ç»œæœç´¢è·å–é™Œç”Ÿå®¢äººçš„æœ€æ–°ä¿¡æ¯
- æ•´åˆå¤šä¸ªç´¢å¼• ï¼Œä»ç»è¿‡éªŒè¯çš„æ¥æºè·å–æ›´å®Œæ•´çš„ä¿¡æ¯ã€‚
### å°ç»“
å…³äºsolagents,llama-index, langgraph å°TIPs:
- æ•°æ®åŠ è½½è¿™å—ï¼šsmolagentå’Œlanggraphå…±ç”¨`from langchain.docstore.document import Document`ï¼Œlamma-indexç”¨`from llama_index.core.schema import Document`åŠ è½½documentæ¨¡å—è½¬åŒ–ä¸ºDocument objectã€‚
- åˆ›å»ºæ£€ç´¢å·¥å…·ï¼š
  - æ£€ç´¢å·¥å…·å¯¼å…¥ï¼šSmolagentå’Œlanggraphä½¿ç”¨langchainçš„BM25æ£€ç´¢å·¥å…·ï¼›llama-indexä½¿ç”¨`from llama_index.retrievers.bm25 import BM25Retriever`
  - Smolagentçš„Toolåº“ï¼Œä½¿ç”¨æ–¹æ³•ï¼šå®šä¹‰ä¸€ä¸ªå·¥å…·ç±»ç»§æ‰¿è‡ª`Tool`ï¼Œæ·»åŠ å·¥å…·çš„å…ƒæ•°æ®æè¿°(name, description, inputs)ï¼Œå®šä¹‰`forward`æ–¹æ³•ã€‚
  - llama-indexçš„`from llama_index.core.tools import FunctionTool`ï¼Œç›´æ¥å®šä¹‰pythonå‡½æ•°å³å¯ï¼ˆæ³¨æ„è¦æ·»åŠ å‡½æ•°æè¿°ï¼‰ï¼Œæœ€å`FunctionTool.from_defaults(get_guest_info_retriever) `åˆå§‹åŒ–å·¥å…·
  - langgraphçš„`from langchain.tools import Tool`ï¼Œå®šä¹‰pythonå‡½æ•°ï¼ˆæ³¨æ„è¦æ·»åŠ å‡½æ•°æè¿°ï¼‰ï¼Œåˆå§‹åŒ–å·¥å…·æ­¥éª¤éœ€è¦æ‰‹åŠ¨æè¿°(name, func,description), like
  ```python
  guest_info_tool = Tool(
    name="guest_info_retriever",
    func=extract_text,
    description="Retrieves detailed information about gala guests based on their name or relation.")
   ```
- å·¥å…·é›†æˆï¼š
  - Smolagentï¼š`from smolagents import CodeAgent, InferenceClientModel`ï¼Œåˆå§‹åŒ–modelï¼Œç›´æ¥ä½¿ç”¨CodeAgentå³å¯ï¼Œ`alfred = CodeAgent(tools=[guest_info_tool], model=model)`ã€‚
  - llama-indexï¼š`from llama_index.core.agent.workflow import AgentWorkflow`ï¼Œ`from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI`ï¼Œåˆå§‹åŒ–å®šä¹‰å¥½llmï¼Œç›´æ¥ç”¨Agentflowå³å¯`alfred = AgentWorkflow.from_tools_or_functions([guest_info_tool],llm=llm,)`ã€‚
  - langgraphï¼šæ˜¯ä¸ªå¤§å·¥ç¨‹ã€‚éœ€è¦å…ˆåˆå§‹åŒ–llmï¼Œå·¥å…·åˆ—è¡¨ï¼Œå°†llmä¸å·¥å…·è¡¨ç»‘å®šã€‚åˆå§‹åŒ–graphï¼Œå®šä¹‰nodeï¼Œå®šä¹‰edgeã€‚
## ä¸ºæ‚¨çš„Agentæ„å»ºå’Œé›†æˆå·¥å…·
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æˆäºˆ Alfred è®¿é—®ç½‘ç»œçš„æƒé™ï¼Œä½¿ä»–èƒ½å¤ŸæŸ¥æ‰¾æœ€æ–°æ–°é—»å’Œå…¨çƒåŠ¨æ€ã€‚æ­¤å¤–ï¼Œä»–è¿˜å°†èƒ½å¤Ÿè®¿é—®å¤©æ°”æ•°æ®å’Œ Hugging Face ä¸­å¿ƒæ¨¡å‹çš„ä¸‹è½½ç»Ÿè®¡æ•°æ®ï¼Œä»¥ä¾¿ä»–å°±çƒ­é—¨è¯é¢˜è¿›è¡Œç›¸å…³å¯¹è¯ã€‚
### Give Your Agent Access to the Web
æˆ‘ä»¬éœ€è¦ç¡®ä¿Alfredå¾·èƒ½å¤Ÿè·å–æœ‰å…³ä¸–ç•Œçš„æœ€æ–°æ–°é—»å’Œä¿¡æ¯ã€‚  
è®©æˆ‘ä»¬ä»ä¸º Alfred åˆ›å»ºä¸€ä¸ªç½‘ç»œæœç´¢å·¥å…·å¼€å§‹å§ï¼
{{< admonition type=note title="solagents" open=false >}}
```python
from smolagents import DuckDuckGoSearchTool

search_tool = DuckDuckGoSearchTool()

# Example usage
results = search_tool("Who's the current President of France?")
print(resultts)
# The current President of France in Emmanuel Macron.
```
{{< /admonition >}}

{{< admonition type=note title="llama-index" open=false >}}
```python
from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec
from llama_index.core.tools import FunctionTool

# Initialize the DuckDuckGo search  tool
tool_spec = DuckDuckGoSearchToolSpec()

search_tool = FunctionTool.from_defaults(tool_spec.duckduckgo_full_search)

# Example usage
results = search_tool("Who's the current President of France?")
print(resultts.raw_output[-1]['body'])
# The President of the French Republic is the head of state of France. The current President is Emmanuel Macron since 14 May 2017 defeating Marine Le Pen in the second round of the presidential election on 7 May 2017. List of French presidents (Fifth Republic) NÂ° Portrait Name ...
```
{{< /admonition >}}

{{< admonition type=note title="langgraph" open=false >}}
```python
from langchain.community.tools import DuckDuckGoSearchRun

search_tool = DuckDuckGoSearchRun()

# Example usage
results = search_tool.invoke("Who's the current President of France?")
print(resultts)
# Emmanuel Macron (born December 21, 1977, Amiens, France) is a French banker and politician who was elected president of France in 2017...
```
{{< /admonition >}}

### åˆ›å»ºè‡ªå®šä¹‰å·¥å…·æ¥è·å–å¤©æ°”ä¿¡æ¯ä»¥å®‰æ’çƒŸèŠ±è¡¨æ¼”
å®Œç¾çš„åº†å…¸åº”è¯¥æ˜¯åœ¨æ™´æœ—çš„å¤©ç©ºä¸‹ç‡ƒæ”¾çƒŸèŠ±ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿çƒŸèŠ±ä¸ä¼šå› ä¸ºæ¶åŠ£çš„å¤©æ°”è€Œå–æ¶ˆã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰å·¥å…·ï¼Œå¯ç”¨äºè°ƒç”¨å¤–éƒ¨å¤©æ°” API å¹¶è·å–ç»™å®šä½ç½®çš„å¤©æ°”ä¿¡æ¯ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åœ¨æœ¬ä¾‹ä¸­ä½¿ç”¨äº†ä¸€ä¸€ä¸ªè™šæ‹Ÿå¤©æ°”APIï¼Œå¦‚æœæ‚¨æƒ³ç”¨çœŸå®çš„å¤©æ°”APIï¼Œå¯ä»¥ä½¿ç”¨OpenWeatherMap APIç­‰ã€‚
{{< admonition type=note title="smolagents" open=false>}}
```python
from smolagents import Tool
import random

class WeatherInfoTool(Tool):
  name = "weather_info"
  description = "Fetches dummy weather information for a given location."
  inputs = {
    "location" : {
      "type" : "string",
      "description": "The location to get weather information for."
    }
  }
  output_type = "string"

  def forward(self, location:str):
    # Dummy weather data
    weather_conditions = [
      {"condition": "Rainy", "temp_c": 15},
      {"condition": "Clear", "temp_c": 25},
      {"condition": "Windy", "temp_c": 20}
    ]
    # Randomly select a weather condition
    data = random.choice(weather_conditions)
    return f"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C"

# Initialize the tool
weather_info_tool = WeatherInfoTool()
 ```
{{< /admonition >}}

{{< admonition type=note title="llama-index" open=false>}}
```python
import random
from llama_index.core.tools import FunctionTool

def get_weather_info(location: str) -> str:
    """Fetches dummy weather information for a given location."""
    # Dummy weather data
    weather_conditions = [
        {"condition": "Rainy", "temp_c": 15},
        {"condition": "Clear", "temp_c": 25},
        {"condition": "Windy", "temp_c": 20}
    ]
    # Randomly select a weather condition
    data = random.choice(weather_conditions)
    return f"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C"

# Initialize the tool
weather_info_tool = FunctionTool.from_defaults(get_weather_info)
```
{{< /admonition >}}

{{< admonition type=note title="langgraph" open=false>}}
```python
from langchain.tools import Tool
import random

def get_weather_info(location: str) -> str:
    """Fetches dummy weather information for a given location."""
    # Dummy weather data
    weather_conditions = [
        {"condition": "Rainy", "temp_c": 15},
        {"condition": "Clear", "temp_c": 25},
        {"condition": "Windy", "temp_c": 20}
    ]
    # Randomly select a weather condition
    data = random.choice(weather_conditions)
    return f"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C"

# Initialize the tool
weather_info_tool = Tool(
    name="get_weather_info",
    func=get_weather_info,
    description="Fetches dummy weather information for a given location."
)
```
{{< /admonition >}}

### ä¸ºAI Buildersåˆ›å»º Hub Stats Tool
å‡ºå¸­æ­¤æ¬¡ç››ä¼šçš„éƒ½æ˜¯ AI å¼€å‘è€…çš„ç²¾è‹±ã€‚Alfred å¸Œæœ›é€šè¿‡è®¨è®ºä»–ä»¬æœ€å—æ¬¢è¿çš„æ¨¡å‹ã€æ•°æ®é›†å’Œç©ºé—´æ¥ç»™ä»–ä»¬ç•™ä¸‹æ·±åˆ»å°è±¡ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå·¥å…·ï¼Œæ ¹æ®ç”¨æˆ·åä» Hugging Face Hub è·å–æ¨¡å‹ç»Ÿè®¡æ•°æ®ã€‚
{{< admonition type=note title="smolagents" open=false>}}
```python
from solagents import Tool
from huggingface_hub import list_models

class HubStatsTool(Tool):
  name = "hub_stats"
  description = "Fetches the most downloaded model from a specific author on the Hugging Face Hub."
  inputs = {
    "author":{
      "type": "string",
      "description": "The username of the model author/organization to find models from."
    }
  }
  output_type = "string"

  def forward(self, author: str):
    try:
      # List Models from the specified author, sorted by downloads
      models = list(list_models(author = author, sort = "sdownloads", direction=-1, limit=1))

      if models:
        model = models[0]
        return f"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads."
      else:
        return f"Nomodels found for author {author}"
    except Exception as e:
      return f"Error fetching models for {author}: {str(e)}"
# Initialize the tool
hub_stats_tool = HubStatsTool()

# Example usage
print(hub_stats_tool("facebook")) # Example: Get the most downloaded model by Facebook
# The most downloaded model by facebook is facebook/esmfold_v1 with 12,544,550 downloads.
```
{{< /adminiton >}}

{{< admonition  type=note title="llama-index" open=false>}}
```python
import random
from llama_index.core.tools import FunctionTool
from huggingface_hub import list_models

def get_hub_stats(author: str) -> str:
    """Fetches the most downloaded model from a specific author on the Hugging Face Hub."""
    try:
        # List models from the specified author, sorted by downloads
        models = list(list_models(author=author, sort="downloads", direction=-1, limit=1))

        if models:
            model = models[0]
            return f"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads."
        else:
            return f"No models found for author {author}."
    except Exception as e:
        return f"Error fetching models for {author}: {str(e)}"

# Initialize the tool
hub_stats_tool = FunctionTool.from_defaults(get_hub_stats)

# Example usage
print(hub_stats_tool("facebook")) # Example: Get the most downloaded model by Facebook
```
{{< /adminiton >}}
{{< admonition type=note title="langgraph" open=false>}}
```python
from langchain.tools import Tool
from huggingface_hub import list_models

def get_hub_stats(author: str) -> str:
    """Fetches the most downloaded model from a specific author on the Hugging Face Hub."""
    try:
        # List models from the specified author, sorted by downloads
        models = list(list_models(author=author, sort="downloads", direction=-1, limit=1))

        if models:
            model = models[0]
            return f"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads."
        else:
            return f"No models found for author {author}."
    except Exception as e:
        return f"Error fetching models for {author}: {str(e)}"

# Initialize the tool
hub_stats_tool = Tool(
    name="get_hub_stats",
    func=get_hub_stats,
    description="Fetches the most downloaded model from a specific author on the Hugging Face Hub."
)

# Example usage
print(hub_stats_tool("facebook")) # Example: Get the most downloaded model by Facebook
```
{{< /adminition >}}
### å·¥å…·é›†æˆ
ç°åœ¨æˆ‘ä»¬å·²ç»æ‹¥æœ‰äº†æ‰€æœ‰çš„å·¥å…·ï¼Œè®©æˆ‘ä»¬å°†å®ƒä»¬é›†æˆåˆ° Alfred çš„ä»£ç†ä¸­ï¼š
{{< abminition type=note title="smolagents" open=false>}}
```python
from smolagents import CodeAgent, InferenceClientModel

model = InferenceModel()

alfred = CodeAgent(
  tools = [search_tool, weather_info_tool, hub_stats_tool], 
  model = model
)
# Example query Alfred might receive during the gala
response = alfred.run("What is Facebook and what's their most popular model?")

print("ğŸ© Alfred's Response:")
print(response)
```
{{< /admintion >}}
{{< abminition type=note title="llama-index" open=false>}}
```python
from llama_index.core.agent.workflow import AgentWorkflow
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

# Initialize the Hugging Face model
llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")
# Create Alfred with all the tools
alfred = AgentWorkflow.from_tools_or_functions(
    [search_tool, weather_info_tool, hub_stats_tool],
    llm=llm
)

# Example query Alfred might receive during the gala
response = await alfred.run("What is Facebook and what's their most popular model?")

print("ğŸ© Alfred's Response:")
print(response)
```
{{< /admintion >}}
{{< abminition type=note title="langgraph" open=false>}}
```python
from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage
from langgraph.prebulit import ToolNode
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import tools_condition
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace

# Generate the chat interface, including the tools
llm = HuggingFaceEndpoint(
    repo_id="Qwen/Qwen2.5-Coder-32B-Instruct",
    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
)

chat = ChatHuggingFace(llm = llm, verbose = True)
tools = [search_tool, weather_info_tool, hub_stats_tool]
chat_with_tools = chat.bind_tools(tools)

# Generate the AgentState and Agent graph
class AgentState(TypedDict):
  messages: Annotated[list[AnyMessage], add_messages]

def assistant(state: AgentState):
  return {
    "messages": [chat_with_tools.invoke(state["messages"])],
  }

# The graph
builder = StateGraph(AgentState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine how control flow moves
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message requires a tool, route to tools
    # Otherwise, provide a direct response
    tools_condition,
)
builder.add_edge("tools", "assistant")
alfred = builder.compile()

messages = [HumanMessages(content="Who is Facebook and what's their most popular model?")]
response = alfred.invoke({"messages": messages})

print("ğŸ© Alfred's Response:")
print(response['messages'][-1].content)
```
{{< /admintion >}}