---
weight: 1
title: "Agentic RAG - Usecase"
subtitle: ""
date: 2025-06-22T12:03:24+08:00
draft: false
author: "June"
authorLink: "https://github.com/zjzjy"
description: ""
images: []
resources:
- name: "featured-image"
  src: "featured-image.jpg"

tags: ["Agent","Agentic RAG"]
categories: ["Agent"]
lightgallery: true

hiddenFromHomePage: false
hiddenFromSearch: false

featuredImage: ""
featuredImagePreview: ""

license: '<a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a>'
toc:
  auto: false
---
æœ¬æ–‡æ ¹æ®[Hugging Faceä¸Šçš„Agentè¯¾ç¨‹](https://huggingface.co/learn/agents-course/unit3/agentic-rag/introduction)ç¼–å†™è€Œæˆã€‚
åœ¨æœ¬ç« èŠ‚ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Agentic RAG åˆ›å»ºä¸€ä¸ªå·¥å…·æ¥å¸®åŠ©ä¸»æŒæ™šä¼šçš„å‹å¥½ç»çºªäºº Alfredï¼Œè¯¥å·¥å…·å¯ç”¨äºå›ç­”æœ‰å…³æ™šä¼šå˜‰å®¾çš„é—®é¢˜ã€‚
# éš¾å¿˜çš„ç››ä¼š
ä½ å†³å®šä¸¾åŠä¸€åœºæœ¬ä¸–çºªæœ€å¥¢åã€æœ€å¥¢åçš„æ´¾å¯¹ã€‚ è¿™æ„å‘³ç€ä¸°ç››çš„å®´å¸­ã€è¿·äººçš„èˆè€…ã€çŸ¥å DJã€ç²¾è‡´çš„é¥®å“ã€ä»¤äººå¹ä¸ºè§‚æ­¢çš„çƒŸç«è¡¨æ¼”ç­‰ç­‰ã€‚æˆ‘ä»¬å§”æ‰˜ç®¡å®¶Alfredæ¥å…¨æƒä¸¾åŠè¿™ä¸ªç››ä¼šã€‚ä¸ºæ­¤ï¼Œä»–éœ€è¦æŒæ¡æ´¾å¯¹çš„æ‰€æœ‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬èœå•ã€å®¾å®¢ã€æ—¥ç¨‹å®‰æ’ã€å¤©æ°”é¢„æŠ¥ç­‰ç­‰ï¼ä¸ä»…å¦‚æ­¤ï¼Œä»–è¿˜éœ€è¦ç¡®ä¿èšä¼šå–å¾—æˆåŠŸï¼Œå› æ­¤ä»–éœ€è¦èƒ½å¤Ÿåœ¨èšä¼šæœŸé—´å›ç­”æœ‰å…³èšä¼šçš„ä»»ä½•é—®é¢˜ ï¼ŒåŒæ—¶å¤„ç†å¯èƒ½å‡ºç°çš„æ„å¤–æƒ…å†µã€‚ä»–æ— æ³•ç‹¬è‡ªå®Œæˆè¿™é¡¹å·¥ä½œï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ç¡®ä¿é˜¿å°”å¼—é›·å¾·èƒ½å¤Ÿè·å¾—ä»–æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯å’Œå·¥å…·ã€‚  
é¦–å…ˆï¼Œæˆ‘ä»¬ç»™ä»–åˆ—ä¸€ä»½è”æ¬¢æ™šä¼šçš„ç¡¬æ€§è¦æ±‚æ¸…å•ï¼šåœ¨æ–‡è‰ºå¤å…´æ—¶æœŸï¼Œå—è¿‡è‰¯å¥½æ•™è‚²çš„äººéœ€è¦å…·å¤‡ä¸‰ä¸ªä¸»è¦ç‰¹è´¨ï¼šå¯¹ä½“è‚²ã€æ–‡åŒ–å’Œç§‘å­¦çŸ¥è¯†çš„æ·±åšé€ è¯£ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿ç”¨æˆ‘ä»¬çš„çŸ¥è¯†ç»™å®¾å®¢ç•™ä¸‹æ·±åˆ»å°è±¡ï¼Œä¸ºä»–ä»¬æ‰“é€ ä¸€åœºçœŸæ­£éš¾å¿˜çš„ç››ä¼šã€‚ç„¶è€Œï¼Œä¸ºäº†é¿å…å†²çªï¼Œ åœ¨ç››ä¼šä¸Šåº”è¯¥é¿å…è®¨è®ºæ”¿æ²»å’Œå®—æ•™ç­‰è¯é¢˜ã€‚ ç››ä¼šéœ€è¦å……æ»¡ä¹è¶£ï¼Œé¿å…ä¸ä¿¡ä»°å’Œç†æƒ³ç›¸å…³çš„å†²çªã€‚æŒ‰ç…§ç¤¼ä»ªï¼Œ ä¸€ä½å¥½çš„ä¸»äººåº”è¯¥äº†è§£å®¾å®¢çš„èƒŒæ™¯ ï¼ŒåŒ…æ‹¬ä»–ä»¬çš„å…´è¶£å’Œäº‹ä¸šã€‚ä¸€ä½å¥½çš„ä¸»äººä¹Ÿä¼šä¸å®¾å®¢ä»¬é—²èŠå…«å¦ï¼Œåˆ†äº«ä»–ä»¬çš„æ•…äº‹ã€‚æœ€åï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿è‡ªå·±æŒæ¡ä¸€äº›å¤©æ°”å¸¸è¯† ï¼Œä»¥ä¾¿èƒ½å¤ŸæŒç»­è·å¾—å®æ—¶æ›´æ–°ï¼Œç¡®ä¿åœ¨æœ€ä½³æ—¶æœºç‡ƒæ”¾çƒŸèŠ±ï¼Œå¹¶ä»¥ä¸€å£°å·¨å“ç»“æŸåº†å…¸ï¼ğŸ†
# åˆ›å»ºå·¥å…·
é¦–å…ˆï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª RAG å·¥å…·ï¼Œç”¨äºæ£€ç´¢å—é‚€è€…çš„æœ€æ–°è¯¦ç»†ä¿¡æ¯ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¼€å‘ç”¨äºç½‘é¡µæœç´¢ã€å¤©æ°”æ›´æ–°å’Œ Hugging Face Hub æ¨¡å‹ä¸‹è½½ç»Ÿè®¡çš„å·¥å…·ã€‚
## ä¸ºæ¥å®¾åˆ›å»º RAG å·¥å…·
æˆ‘ä»¬å°†åœ¨[HF Space](https://huggingface.co/spaces/agents-course/Unit_3_Agentic_RAG)å¼€å‘æˆ‘ä»¬çš„Agentã€‚
- tools.pyï¼šä¸ºAgentæä¾›è¾…åŠ©å·¥å…·ã€‚
- retriever.pyï¼šå®ç°æ£€ç´¢åŠŸèƒ½ï¼Œæ”¯æŒçŸ¥è¯†è®¿é—®ã€‚
- app.pyï¼šå°†æ‰€æœ‰ç»„ä»¶é›†æˆåˆ°åŠŸèƒ½é½å…¨çš„agentä¸­ã€‚


ä½¿ç”¨çš„[dataset](https://huggingface.co/datasets/agents-course/unit3-invitees)ï¼Œæ¯ä¸ªè®¿å®¢åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- Name: å®¢äººçš„å…¨å
- Relation: å®¢äººä¸ä¸»äººçš„å…³ç³»
- Descriptionï¼šå…³äºå®¢äººçš„ç®€çŸ­ä¼ è®°æˆ–æœ‰è¶£çš„äº‹å®
- Email Addressï¼šå‘é€é‚€è¯·æˆ–åç»­æ´»åŠ¨çš„è”ç³»ä¿¡æ¯

### Step1: åŠ è½½å¹¶å‡†å¤‡æ•°æ®é›†
æˆ‘ä»¬æä¾›äº†ä¸‰ç§ä¸åŒ Agent åº“çš„å®ç°æ–¹å¼ï¼Œä½ å¯ä»¥å±•å¼€ä¸‹é¢çš„æŠ˜å æ¡†æŸ¥çœ‹å„è‡ªçš„ä»£ç ã€‚
{{< admonition type=note title=smolagents open=false >}}
æˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face datasets é›†åº“æ¥åŠ è½½æ•°æ®é›†å¹¶å°†å…¶è½¬æ¢ä¸ºæ¥è‡ª langchain.docstore.document æ¨¡å—çš„ Document å¯¹è±¡åˆ—è¡¨ã€‚
```python
import datasets
from langchain.docstore.document import Document

# Load the dataset
guest_dataset = datasets.load_dataset("agents-course/unit3-invitees", split="train")

# Convert dataset entries into document objects
docs = [
  Document(
    page_content = "\n".join([
      f"Name: {guest['name']}",
      f"Relation: {guest['relation']}",
      f"Description: {guest['description']}",
      f"Email: {guest['email']}"
    ]),
    metadata={"name": guest["name"]}
  )
  for guest in guest_dataset
]
```
{{< /admonition >}}

{{< admonition type=note title=llama-index open=false >}}
æˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face datasets é›†åº“æ¥åŠ è½½æ•°æ®é›†å¹¶å°†å…¶è½¬æ¢ä¸ºæ¥è‡ª llama_index.core.schema æ¨¡å—çš„ Document å¯¹è±¡åˆ—è¡¨ã€‚
```python
import datasets
from llama_index.core.schema import Document

# Load the dataset
guest_dataset = datasets.load_dataset("agents-course/unit3-invitees", split="train")

# Convert dataset entries into Document objects
docs = [
    Document(
        text="\n".join([
            f"Name: {guest_dataset['name'][i]}",
            f"Relation: {guest_dataset['relation'][i]}",
            f"Description: {guest_dataset['description'][i]}",
            f"Email: {guest_dataset['email'][i]}"
        ]),
        metadata={"name": guest_dataset['name'][i]}
    )
    for i in range(len(guest_dataset))
]
```
{{< /admonition >}}

{{< admonition type=note title=langgraph open=false >}}
æˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face datasets é›†åº“æ¥åŠ è½½æ•°æ®é›†å¹¶å°†å…¶è½¬æ¢ä¸ºæ¥è‡ª langchain.docstore.document æ¨¡å—çš„ Document å¯¹è±¡åˆ—è¡¨ã€‚
```python
import datasets
from langchain.docstore.document import Document

# Load the dataset
guest_dataset = datasets.load_dataset("agents-course/unit3-invitees", split="train")

# Convert dataset entries into document objects
docs = [
  Document(
    page_content = "\n".join([
      f"Name: {guest['name']}",
      f"Relation: {guest['relation']}",
      f"Description: {guest['description']}",
      f"Email: {guest['email']}"
    ]),
    metadata={"name": guest["name"]}
  )
  for guest in guest_dataset
]
```
{{< /admonition >}}

åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬ï¼šåŠ è½½æ•°æ®é›†ï¼Œå°†æ¯ä¸ªå®¢äººæ¡ç›®è½¬æ¢ä¸ºå…·æœ‰æ ¼å¼åŒ–å†…å®¹çš„ Document å¯¹è±¡ï¼Œå°† Document å¯¹è±¡å­˜å‚¨åœ¨åˆ—è¡¨ä¸­ã€‚

### Step2: åˆ›å»ºæ£€ç´¢å·¥å…·
{{< admonition type=note title=smolagents open=false >}}
æˆ‘ä»¬å°†ä½¿ç”¨ langchain_community.retrievers æ¨¡å—ä¸­çš„ BM25Retriever æ¥åˆ›å»ºæ£€ç´¢å·¥å…·ã€‚BM25æ˜¯ç›¸å…³æ€§æœç´¢ï¼Œå¦‚æœè¦æ›´é«˜çº§çš„è¯­ä¹‰æœç´¢ï¼Œå¯ä»¥è€ƒè™‘embeddingæ£€ç´¢å™¨ï¼Œä¾‹å¦‚[sentence-transformers ](https://www.sbert.net/)ã€‚
```python
from solagents import Tool
from langchain_community.retrievers import BM25Retriever

class GuestInfoRetrieverTool(Tool):
  # å·¥å…·çš„å…ƒæ•°æ®æè¿°
  name = "guest_info_retriever"
  description = "Retrieves detailed information about gala guests based on their name or relation."
  inputs = {
    "query": {
      "type": "string",
      "description": "The name or relation of the guest you want information about."
    }
  }
  output_type = "string"

  def __init__(self, docs):
    self.is_initialized = False
    self.retriever = BM25Retriever.from_document(docs)

  def forward(self, query: str):
    results = self.retriever.get_relevant_documents(query)
    if results:
      return "\n\n".join([doc.page_content for doc in results[:3]])
    else:
      return "No matching guest information found."

# Initialize the tool
guest_info_tool = GuestInfoRetrieverTool(docs)
```
{{< /admonition >}}

{{< admonition type=note title=llama-index open=false >}}
```python
from llama_index.core.tools import FunctionTool
from llama_index.retrievers.bm25 import BM25Retriever

bm25_retriever = BM25Retriever.from_defaults(nodes = docs)

def get_guest_info_retriever(query: str) -> str:
  """Retrieves detailed information about gala guests based on their name or relation."""
  results = bm25_retriever(query)
  if results:
        return "\n\n".join([doc.text for doc in results[:3]])
  else:
        return "No matching guest information found."

# Initialize the tool
guest_info_tool = FunctionTool.from_defaults(get_guest_info_retriever) 
```
{{< /admonition >}}

{{< admonition type=note title=langgraph open=false >}}
```python
from langchain_community.retrievers import BM25Retriever
from langchain.tools import Tool

bm25_retriever = BM25Retriever.from_documents(docs)

def extract_text(query: str) -> str:
    """Retrieves detailed information about gala guests based on their name or relation."""
    results = bm25_retriever.invoke(query)
    if results:
        return "\n\n".join([doc.page_content for doc in results[:3]])
    else:
        return "No matching guest information found."

guest_info_tool = Tool(
    name="guest_info_retriever",
    func=extract_text,
    description="Retrieves detailed information about gala guests based on their name or relation."
)
```
{{< /admonition >}}

### Step3ï¼šå°†å·¥å…·ä¸Alfredé›†æˆ
æœ€åï¼Œè®©æˆ‘ä»¬é€šè¿‡åˆ›å»ºä»£ç†å¹¶ä¸ºå…¶é…å¤‡è‡ªå®šä¹‰å·¥å…·æ¥å°†æ‰€æœ‰å†…å®¹æ•´åˆåœ¨ä¸€èµ·ï¼š
{{< admonition type=note title=smolagents open=false >}}
```python
from smolagents import CodeAgent, InferenceClientModel

# Initialize the Hugging Face model
model = InferenceClientModel()

# Create Alfred, our gala agent, with the guest info tool
alfred = CodeAgent(tools=[guest_info_tool], model=model)

# Example query Alfred might receive during the gala
response = alfred.run("Tell me about our guest named 'Lady Ada Lovelace'.")

print("ğŸ© Alfred's Response:")
print(response)
#ğŸ© Alfred's Response:
#Based on the information I retrieved, Lady Ada Lovelace is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine. Her email address is ada.lovelace@example.com.
```
{{< /admonition >}}

{{< admonition type=note title=llama-index open=false >}}
```python
from llama_index.core.agent.workflow import AgentWorkflow
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

# Initialize the Hugging Face model
llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")

# Create Alfred, our gala agent, with the guest info tool
alfred = AgentWorkflow.from_tools_or_functions(
    [guest_info_tool],
    llm=llm,
)

# Example query Alfred might receive during the gala
response = await alfred.run("Tell me about our guest named 'Lady Ada Lovelace'.")

print("ğŸ© Alfred's Response:")
print(response)
#ğŸ© Alfred's Response:
#Lady Ada Lovelace is an esteemed mathematician and friend, renowned for her pioneering work in mathematics and computing. She is celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine. Her email is ada.lovelace@example.com.
```
{{< /admonition >}}

{{< admonition type=note title=langgraph open=false >}}
```python
from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage
from langgraph.prebuilt import ToolNode
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import tools_condition
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace

# Generate the chat interface, including the tools
llm = HuggingFaceEndpoint(
    repo_id="Qwen/Qwen2.5-Coder-32B-Instruct",
    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
)

chat = ChatHuggingFace(llm=llm, verbose=True)
tools = [guest_info_tool]
chat_with_tools = chat.bind_tools(tools)

# Generate the AgentState and Agent graph
class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

def assistant(state: AgentState):
    return {
        "messages": [chat_with_tools.invoke(state["messages"])],
    }

## The graph
builder = StateGraph(AgentState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine how the control flow moves
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message requires a tool, route to tools
    # Otherwise, provide a direct response
    tools_condition,
)
builder.add_edge("tools", "assistant")
alfred = builder.compile()

messages = [HumanMessage(content="Tell me about our guest named 'Lady Ada Lovelace'.")]
response = alfred.invoke({"messages": messages})

print("ğŸ© Alfred's Response:")
print(response['messages'][-1].content)
#ğŸ© Alfred's Response:
#Lady Ada Lovelace is an esteemed mathematician and pioneer in computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.
```
{{< /admonition >}}

æœ€åä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆï¼š

æˆ‘ä»¬ä½¿ç”¨ HuggingFaceEndpoint ç±»åˆå§‹åŒ– HuggingFace æ¨¡å‹ã€‚æˆ‘ä»¬è¿˜ç”Ÿæˆäº†ä¸€ä¸ªèŠå¤©ç•Œé¢å¹¶é™„åŠ äº†ä¸€äº›å·¥å…·ã€‚

æˆ‘ä»¬å°†ä»£ç†ï¼ˆAlfredï¼‰åˆ›å»ºä¸º StateGraph ï¼Œå®ƒä½¿ç”¨è¾¹ç»„åˆ 2 ä¸ªèŠ‚ç‚¹ï¼ˆ assistant ã€ tools ï¼‰

æˆ‘ä»¬è¦æ±‚é˜¿å°”å¼—é›·å¾·æ£€ç´¢æœ‰å…³ä¸€ä½åå«â€œLady Ada Lovelaceâ€çš„å®¢äººçš„ä¿¡æ¯ã€‚


ç°åœ¨ Alfred å¯ä»¥æ£€ç´¢å®¢äººä¿¡æ¯ï¼Œè¯·è€ƒè™‘å¦‚ä½•å¢å¼ºæ­¤ç³»ç»Ÿï¼š
- æ”¹è¿›æ£€ç´¢å™¨ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•ï¼Œä¾‹å¦‚å¥å­è½¬æ¢å™¨
- å®ç°å¯¹è¯è®°å¿† ï¼Œä»¥ä¾¿ Alfred è®°ä½ä¹‹å‰çš„äº’åŠ¨
- ç»“åˆç½‘ç»œæœç´¢è·å–é™Œç”Ÿå®¢äººçš„æœ€æ–°ä¿¡æ¯
- æ•´åˆå¤šä¸ªç´¢å¼• ï¼Œä»ç»è¿‡éªŒè¯çš„æ¥æºè·å–æ›´å®Œæ•´çš„ä¿¡æ¯ã€‚
### å°ç»“
å…³äºsolagents,llama-index, langgraph å°TIPs:
- æ•°æ®åŠ è½½è¿™å—ï¼šsmolagentå’Œlanggraphå…±ç”¨`from langchain.docstore.document import Document`ï¼Œlamma-indexç”¨`from llama_index.core.schema import Document`åŠ è½½documentæ¨¡å—è½¬åŒ–ä¸ºDocument objectã€‚
- åˆ›å»ºæ£€ç´¢å·¥å…·ï¼š
  - æ£€ç´¢å·¥å…·å¯¼å…¥ï¼šSmolagentå’Œlanggraphä½¿ç”¨langchainçš„BM25æ£€ç´¢å·¥å…·ï¼›llama-indexä½¿ç”¨`from llama_index.retrievers.bm25 import BM25Retriever`
  - Smolagentçš„Toolåº“ï¼Œä½¿ç”¨æ–¹æ³•ï¼šå®šä¹‰ä¸€ä¸ªå·¥å…·ç±»ç»§æ‰¿è‡ª`Tool`ï¼Œæ·»åŠ å·¥å…·çš„å…ƒæ•°æ®æè¿°(name, description, inputs)ï¼Œå®šä¹‰`forward`æ–¹æ³•ã€‚
  - llama-indexçš„`from llama_index.core.tools import FunctionTool`ï¼Œç›´æ¥å®šä¹‰pythonå‡½æ•°å³å¯ï¼ˆæ³¨æ„è¦æ·»åŠ å‡½æ•°æè¿°ï¼‰ï¼Œæœ€å`FunctionTool.from_defaults(get_guest_info_retriever) `åˆå§‹åŒ–å·¥å…·
  - langgraphçš„`from langchain.tools import Tool`ï¼Œå®šä¹‰pythonå‡½æ•°ï¼ˆæ³¨æ„è¦æ·»åŠ å‡½æ•°æè¿°ï¼‰ï¼Œåˆå§‹åŒ–å·¥å…·æ­¥éª¤éœ€è¦æ‰‹åŠ¨æè¿°(name, func,description), like
  ```python
  guest_info_tool = Tool(
    name="guest_info_retriever",
    func=extract_text,
    description="Retrieves detailed information about gala guests based on their name or relation.")
   ```
- å·¥å…·é›†æˆï¼š
  - Smolagentï¼š`from smolagents import CodeAgent, InferenceClientModel`ï¼Œåˆå§‹åŒ–modelï¼Œç›´æ¥ä½¿ç”¨CodeAgentå³å¯ï¼Œ`alfred = CodeAgent(tools=[guest_info_tool], model=model)`ã€‚
  - llama-indexï¼š`from llama_index.core.agent.workflow import AgentWorkflow`ï¼Œ`from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI`ï¼Œåˆå§‹åŒ–å®šä¹‰å¥½llmï¼Œç›´æ¥ç”¨Agentflowå³å¯`alfred = AgentWorkflow.from_tools_or_functions([guest_info_tool],llm=llm,)`ã€‚
  - langgraphï¼šæ˜¯ä¸ªå¤§å·¥ç¨‹ã€‚éœ€è¦å…ˆåˆå§‹åŒ–llmï¼Œå·¥å…·åˆ—è¡¨ï¼Œå°†llmä¸å·¥å…·è¡¨ç»‘å®šã€‚åˆå§‹åŒ–graphï¼Œå®šä¹‰nodeï¼Œå®šä¹‰edgeã€‚
## ä¸ºæ‚¨çš„Agentæ„å»ºå’Œé›†æˆå·¥å…·
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æˆäºˆ Alfred è®¿é—®ç½‘ç»œçš„æƒé™ï¼Œä½¿ä»–èƒ½å¤ŸæŸ¥æ‰¾æœ€æ–°æ–°é—»å’Œå…¨çƒåŠ¨æ€ã€‚æ­¤å¤–ï¼Œä»–è¿˜å°†èƒ½å¤Ÿè®¿é—®å¤©æ°”æ•°æ®å’Œ Hugging Face ä¸­å¿ƒæ¨¡å‹çš„ä¸‹è½½ç»Ÿè®¡æ•°æ®ï¼Œä»¥ä¾¿ä»–å°±çƒ­é—¨è¯é¢˜è¿›è¡Œç›¸å…³å¯¹è¯ã€‚
### Give Your Agent Access to the Web
æˆ‘ä»¬éœ€è¦ç¡®ä¿Alfredå¾·èƒ½å¤Ÿè·å–æœ‰å…³ä¸–ç•Œçš„æœ€æ–°æ–°é—»å’Œä¿¡æ¯ã€‚  
è®©æˆ‘ä»¬ä»ä¸º Alfred åˆ›å»ºä¸€ä¸ªç½‘ç»œæœç´¢å·¥å…·å¼€å§‹å§ï¼
{{< admonition type=note title=solagents open=false >}}
```python
from smolagents import DuckDuckGoSearchTool

search_tool = DuckDuckGoSearchTool()

# Example usage
results = search_tool("Who's the current President of France?")
print(resultts)
# The current President of France in Emmanuel Macron.
```
{{< /admonition >}}

{{< admonition type=note title=llama-index open=false >}}
```python
from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec
from llama_index.core.tools import FunctionTool

# Initialize the DuckDuckGo search  tool
tool_spec = DuckDuckGoSearchToolSpec()

search_tool = FunctionTool.from_defaults(tool_spec.duckduckgo_full_search)

# Example usage
results = search_tool("Who's the current President of France?")
print(resultts.raw_output[-1]['body'])
# The President of the French Republic is the head of state of France. The current President is Emmanuel Macron since 14 May 2017 defeating Marine Le Pen in the second round of the presidential election on 7 May 2017. List of French presidents (Fifth Republic) NÂ° Portrait Name ...
```
{{< /admonition >}}

{{< admonition note "langgraph" false>}}
```python
from langchain.community.tools import DuckDuckGoSearchRun

search_tool = DuckDuckGoSearchRun()

# Example usage
results = search_tool.invoke("Who's the current President of France?")
print(resultts)
# Emmanuel Macron (born December 21, 1977, Amiens, France) is a French banker and politician who was elected president of France in 2017...
```
{{< /admonition >}}

### åˆ›å»ºè‡ªå®šä¹‰å·¥å…·æ¥è·å–å¤©æ°”ä¿¡æ¯ä»¥å®‰æ’çƒŸèŠ±è¡¨æ¼”
å®Œç¾çš„åº†å…¸åº”è¯¥æ˜¯åœ¨æ™´æœ—çš„å¤©ç©ºä¸‹ç‡ƒæ”¾çƒŸèŠ±ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿çƒŸèŠ±ä¸ä¼šå› ä¸ºæ¶åŠ£çš„å¤©æ°”è€Œå–æ¶ˆã€‚è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰å·¥å…·ï¼Œå¯ç”¨äºè°ƒç”¨å¤–éƒ¨å¤©æ°” API å¹¶è·å–ç»™å®šä½ç½®çš„å¤©æ°”ä¿¡æ¯ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åœ¨æœ¬ä¾‹ä¸­ä½¿ç”¨äº†ä¸€ä¸€ä¸ªè™šæ‹Ÿå¤©æ°”APIï¼Œå¦‚æœæ‚¨æƒ³ç”¨çœŸå®çš„å¤©æ°”APIï¼Œå¯ä»¥ä½¿ç”¨OpenWeatherMap APIç­‰ã€‚
{{< admonition note "smolagents" false>}}
```python
from smolagents import Tool
import random

class WeatherInfoTool(Tool):
  name = "weather_info"
  description = "Fetches dummy weather information for a given location."
  inputs = {
    "location" : {
      "type" : "string",
      "description": "The location to get weather information for."
    }
  }
  output_type = "string"

  def forward(self, location:str):
    # Dummy weather data
    weather_conditions = [
      {"condition": "Rainy", "temp_c": 15},
      {"condition": "Clear", "temp_c": 25},
      {"condition": "Windy", "temp_c": 20}
    ]
    # Randomly select a weather condition
    data = random.choice(weather_conditions)
    return f"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C"

# Initialize the tool
weather_info_tool = WeatherInfoTool()
 ```
{{< /admonition >}}

{{< admonition note "llama-index" false>}}
```python
import random
from llama_index.core.tools import FunctionTool

def get_weather_info(location: str) -> str:
    """Fetches dummy weather information for a given location."""
    # Dummy weather data
    weather_conditions = [
        {"condition": "Rainy", "temp_c": 15},
        {"condition": "Clear", "temp_c": 25},
        {"condition": "Windy", "temp_c": 20}
    ]
    # Randomly select a weather condition
    data = random.choice(weather_conditions)
    return f"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C"

# Initialize the tool
weather_info_tool = FunctionTool.from_defaults(get_weather_info)
```
{{< /admonition >}}

{{< admonition note "langgraph" false>}}
```python
from langchain.tools import Tool
import random

def get_weather_info(location: str) -> str:
    """Fetches dummy weather information for a given location."""
    # Dummy weather data
    weather_conditions = [
        {"condition": "Rainy", "temp_c": 15},
        {"condition": "Clear", "temp_c": 25},
        {"condition": "Windy", "temp_c": 20}
    ]
    # Randomly select a weather condition
    data = random.choice(weather_conditions)
    return f"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C"

# Initialize the tool
weather_info_tool = Tool(
    name="get_weather_info",
    func=get_weather_info,
    description="Fetches dummy weather information for a given location."
)
```
{{< /admonition >}}

### ä¸ºAI Buildersåˆ›å»º Hub Stats Tool
å‡ºå¸­æ­¤æ¬¡ç››ä¼šçš„éƒ½æ˜¯ AI å¼€å‘è€…çš„ç²¾è‹±ã€‚Alfred å¸Œæœ›é€šè¿‡è®¨è®ºä»–ä»¬æœ€å—æ¬¢è¿çš„æ¨¡å‹ã€æ•°æ®é›†å’Œç©ºé—´æ¥ç»™ä»–ä»¬ç•™ä¸‹æ·±åˆ»å°è±¡ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå·¥å…·ï¼Œæ ¹æ®ç”¨æˆ·åä» Hugging Face Hub è·å–æ¨¡å‹ç»Ÿè®¡æ•°æ®ã€‚
{{< admonition note "smolagents" false>}}
```python
from solagents import Tool
from huggingface_hub import list_models

class HubStatsTool(Tool):
  name = "hub_stats"
  description = "Fetches the most downloaded model from a specific author on the Hugging Face Hub."
  inputs = {
    "author":{
      "type": "string",
      "description": "The username of the model author/organization to find models from."
    }
  }
  output_type = "string"

  def forward(self, author: str):
    try:
      # List Models from the specified author, sorted by downloads
      models = list(list_models(author = author, sort = "sdownloads", direction=-1, limit=1))

      if models:
        model = models[0]
        return f"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads."
      else:
        return f"Nomodels found for author {author}"
    except Exception as e:
      return f"Error fetching models for {author}: {str(e)}"
# Initialize the tool
hub_stats_tool = HubStatsTool()

# Example usage
print(hub_stats_tool("facebook")) # Example: Get the most downloaded model by Facebook
# The most downloaded model by facebook is facebook/esmfold_v1 with 12,544,550 downloads.
```
{{< /admonition >}}

{{< admonition note "llama-index" false>}}
```python
import random
from llama_index.core.tools import FunctionTool
from huggingface_hub import list_models

def get_hub_stats(author: str) -> str:
    """Fetches the most downloaded model from a specific author on the Hugging Face Hub."""
    try:
        # List models from the specified author, sorted by downloads
        models = list(list_models(author=author, sort="downloads", direction=-1, limit=1))

        if models:
            model = models[0]
            return f"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads."
        else:
            return f"No models found for author {author}."
    except Exception as e:
        return f"Error fetching models for {author}: {str(e)}"

# Initialize the tool
hub_stats_tool = FunctionTool.from_defaults(get_hub_stats)

# Example usage
print(hub_stats_tool("facebook")) # Example: Get the most downloaded model by Facebook
```
{{< /admonition >}}
{{< admonition note "langgraph" false>}}
```python
from langchain.tools import Tool
from huggingface_hub import list_models

def get_hub_stats(author: str) -> str:
    """Fetches the most downloaded model from a specific author on the Hugging Face Hub."""
    try:
        # List models from the specified author, sorted by downloads
        models = list(list_models(author=author, sort="downloads", direction=-1, limit=1))

        if models:
            model = models[0]
            return f"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads."
        else:
            return f"No models found for author {author}."
    except Exception as e:
        return f"Error fetching models for {author}: {str(e)}"

# Initialize the tool
hub_stats_tool = Tool(
    name="get_hub_stats",
    func=get_hub_stats,
    description="Fetches the most downloaded model from a specific author on the Hugging Face Hub."
)

# Example usage
print(hub_stats_tool("facebook")) # Example: Get the most downloaded model by Facebook
```
{{< /admonition >}}
### å·¥å…·é›†æˆ
ç°åœ¨æˆ‘ä»¬å·²ç»æ‹¥æœ‰äº†æ‰€æœ‰çš„å·¥å…·ï¼Œè®©æˆ‘ä»¬å°†å®ƒä»¬é›†æˆåˆ° Alfred çš„ä»£ç†ä¸­ï¼š
{{< admonition note "smolagents" false>}}
```python
from smolagents import CodeAgent, InferenceClientModel

model = InferenceModel()

alfred = CodeAgent(
  tools = [search_tool, weather_info_tool, hub_stats_tool], 
  model = model
)
# Example query Alfred might receive during the gala
response = alfred.run("What is Facebook and what's their most popular model?")

print("ğŸ© Alfred's Response:")
print(response)
```
{{< /admonition >}}
{{< admonition note "llama-index" false>}}
```python
from llama_index.core.agent.workflow import AgentWorkflow
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

# Initialize the Hugging Face model
llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")
# Create Alfred with all the tools
alfred = AgentWorkflow.from_tools_or_functions(
    [search_tool, weather_info_tool, hub_stats_tool],
    llm=llm
)

# Example query Alfred might receive during the gala
response = await alfred.run("What is Facebook and what's their most popular model?")

print("ğŸ© Alfred's Response:")
print(response)
```
{{< /admonition >}}
{{< admonition note "langgraph" false>}}
```python
from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage
from langgraph.prebulit import ToolNode
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import tools_condition
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace

# Generate the chat interface, including the tools
llm = HuggingFaceEndpoint(
    repo_id="Qwen/Qwen2.5-Coder-32B-Instruct",
    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
)

chat = ChatHuggingFace(llm = llm, verbose = True)
tools = [search_tool, weather_info_tool, hub_stats_tool]
chat_with_tools = chat.bind_tools(tools)

# Generate the AgentState and Agent graph
class AgentState(TypedDict):
  messages: Annotated[list[AnyMessage], add_messages]

def assistant(state: AgentState):
  return {
    "messages": [chat_with_tools.invoke(state["messages"])],
  }

# The graph
builder = StateGraph(AgentState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine how control flow moves
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message requires a tool, route to tools
    # Otherwise, provide a direct response
    tools_condition,
)
builder.add_edge("tools", "assistant")
alfred = builder.compile()

messages = [HumanMessages(content="Who is Facebook and what's their most popular model?")]
response = alfred.invoke({"messages": messages})

print("ğŸ© Alfred's Response:")
print(response['messages'][-1].content)
```
{{< /admonition >}}

## Creating Your Gala Agent  
ç°åœ¨æˆ‘ä»¬å·²ç»ä¸º Alfred æ„å»ºäº†æ‰€æœ‰å¿…è¦çš„ç»„ä»¶ï¼Œç°åœ¨æ˜¯æ—¶å€™å°†æ‰€æœ‰ç»„ä»¶æ•´åˆæˆä¸€ä¸ªå®Œæ•´çš„ä»£ç†ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬ä¸¾åŠå¥¢åçš„ç››ä¼šã€‚  
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠå®¢äººä¿¡æ¯æ£€ç´¢ã€ç½‘ç»œæœç´¢ã€å¤©æ°”ä¿¡æ¯å’Œ Hub ç»Ÿè®¡å·¥å…·ç»„åˆæˆä¸€ä¸ªå¼ºå¤§çš„ä»£ç†ã€‚  
æˆ‘ä»¬åœ¨ä¹‹å‰å·²ç»å®ç°äº†tools.pyå’Œretriever.pyï¼Œæ¥ä¸‹æ¥è¦å¯¼å…¥å®ƒä»¬ã€‚
{{< admonition note "solagents" false>}}
```python
# Import necessary libraries
import random
from smolagents import CodeAgent, InferenceClientModel

# Import our custom tools from their modules
from tools import DuckDuckGoSearchTool, WeatherInfoTool, HubStatsTool
from retriever import load_guest_dataset

# Initialize the Hugging Face model
model = InferenceClientModel()

# Initialize the web search tool
search_tool = DuckDuckGoSearchTool()

# Initialize the weather tool
weather_info_tool = WeatherInfoTool()

# Initialize the Hub stats tool
hub_stats_tool = HubStatsTool()

# Load the guest dataset and initialize the guest info tool
guest_info_tool = load_guest_dataset()

# Create Alfred with all the tools
alfred = CodeAgent(
    tools=[guest_info_tool, weather_info_tool, hub_stats_tool, search_tool], 
    model=model,
    add_base_tools=True,  # Add any additional base tools
    planning_interval=3   # Enable planning every 3 steps
)
```
{{< /admonition >}}
{{< admonition note "llama-index" false>}}
```python
# Import necessary libraries
from llama_index.core.agent.workflow import AgentWorkflow
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

from tools import search_tool, weather_info_tool, hub_stats_tool
from retriever import guest_info_tool

# Initialize the Hugging Face model
llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")

# Create Alfred with all the tools
alfred = AgentWorkflow.from_tools_or_functions(
    [guest_info_tool, search_tool, weather_info_tool, hub_stats_tool],
    llm=llm,
)
```
{{< /admonition >}}
{{< admonition note "langgraph" false>}}
```python
from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage
from langgraph.prebuilt import ToolNode
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import tools_condition
from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace

from tools import DuckDuckGoSearchRun, weather_info_tool, hub_stats_tool
from retriever import guest_info_tool

# Initialize the web search tool
search_tool = DuckDuckGoSearchRun()

# Generate the chat interface, including the tools
llm = HuggingFaceEndpoint(
    repo_id="Qwen/Qwen2.5-Coder-32B-Instruct",
    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
)

chat = ChatHuggingFace(llm=llm, verbose=True)
tools = [guest_info_tool, search_tool, weather_info_tool, hub_stats_tool]
chat_with_tools = chat.bind_tools(tools)

# Generate the AgentState and Agent graph
class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

def assistant(state: AgentState):
    return {
        "messages": [chat_with_tools.invoke(state["messages"])],
    }

## The graph
builder = StateGraph(AgentState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine how the control flow moves
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message requires a tool, route to tools
    # Otherwise, provide a direct response
    tools_condition,
)
builder.add_edge("tools", "assistant")
alfred = builder.compile()
```
{{< /admonition >}}
# åº”ç”¨ç¤ºèŒƒ
æˆ‘ä»¬å°†ä½¿ç”¨åˆšåˆšåˆ›å»ºçš„Alfred Agentå®Œæˆä¸‰ä¸ªå·¥ä½œï¼š
1. æŸ¥æ‰¾å®¢äººä¿¡æ¯
2. æ£€æŸ¥çƒŸèŠ±å¤©æ°”
3. ç»™AI Builder å®¢äººç•™ä¸‹æ·±åˆ»å°è±¡
4. ä½¿ç”¨å¤šä¸ªå·¥å…·ä¸å°¼å¤æ‹‰æ–¯åšå£«è¿›è¡Œå¯¹è¯
{{< admonition note "smolagents" false>}}
```python 
# 1.æŸ¥æ‰¾å®¢äººä¿¡æ¯
query = "Tell me about 'Lady Ada Lovelace'"
response = alfred.run(query)
print("ğŸ© Alfred's Response:")
print(response)
# 2.æ£€æŸ¥çƒŸèŠ±å¤©æ°”
query = "What's the weather like in Paris tonight? Will it be suitable for our fireworks display?"
response = alfred.run(query)

print("ğŸ© Alfred's Response:")
print(response)
# 3. ç»™AI Builder å®¢äººç•™ä¸‹æ·±åˆ»å°è±¡
query = "One of our guests is from Qwen. What can you tell me about their most popular model?"
response = alfred.run(query)

print("ğŸ© Alfred's Response:")
print(response)
# 4. ä½¿ç”¨å¤šä¸ªå·¥å…·ä¸å°¼å¤æ‹‰æ–¯åšå£«è¿›è¡Œå¯¹è¯
query = "I need to speak with Dr. Nikola Tesla about recent advancements in wireless energy. Can you help me prepare for this conversation?"
response = alfred.run(query)

print("ğŸ© Alfred's Response:")
print(response)
```
ALfredçš„å›ç­”ï¼š
```js
ğŸ© Alfred's Response:
I've gathered information to help you prepare for your conversation with Dr. Nikola Tesla.

Guest Information:
Name: Dr. Nikola Tesla
Relation: old friend from university days
Description: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.
Email: nikola.tesla@gmail.com

Recent Advancements in Wireless Energy:
Based on my web search, here are some recent developments in wireless energy transmission:
1. Researchers have made progress in long-range wireless power transmission using focused electromagnetic waves
2. Several companies are developing resonant inductive coupling technologies for consumer electronics
3. There are new applications in electric vehicle charging without physical connections

Conversation Starters:
1. "I'd love to hear about your new patent on wireless energy transmission. How does it compare to your original concepts from our university days?"
2. "Have you seen the recent developments in resonant inductive coupling for consumer electronics? What do you think of their approach?"
3. "How are your pigeons doing? I remember your fascination with them."

This should give you plenty to discuss with Dr. Tesla while demonstrating your knowledge of his interests and recent developments in his field.
```
{{< /admonition >}}

{{< admonition note "llama-index" false>}}
```python 
# 1.æŸ¥æ‰¾å®¢äººä¿¡æ¯
query = "Tell me about 'Lady Ada Lovelace'"
response = await alfred.run(query)
print("ğŸ© Alfred's Response:")
print(response.response.blocks[0].text)
# 2.æ£€æŸ¥çƒŸèŠ±å¤©æ°”
query = "What's the weather like in Paris tonight? Will it be suitable for our fireworks display?"
response = await alfred.run(query)

print("ğŸ© Alfred's Response:")
print(response)
# 3. ç»™AI Builder å®¢äººç•™ä¸‹æ·±åˆ»å°è±¡
query = "One of our guests is from Google. What can you tell me about their most popular model?"
response = await alfred.run(query)

print("ğŸ© Alfred's Response:")
print(response)
# 4. ä½¿ç”¨å¤šä¸ªå·¥å…·ä¸å°¼å¤æ‹‰æ–¯åšå£«è¿›è¡Œå¯¹è¯
query = "I need to speak with Dr. Nikola Tesla about recent advancements in wireless energy. Can you help me prepare for this conversation?"
response = await alfred.run(query)

print("ğŸ© Alfred's Response:")
print(response)
```
ALfredçš„å›ç­”ï¼š
```json
ğŸ© Alfred's Response:
Here are some recent advancements in wireless energy that you might find useful for your conversation with Dr. Nikola Tesla:

1. **Advancements and Challenges in Wireless Power Transfer**: This article discusses the evolution of wireless power transfer (WPT) from conventional wired methods to modern applications, including solar space power stations. It highlights the initial focus on microwave technology and the current demand for WPT due to the rise of electric devices.

2. **Recent Advances in Wireless Energy Transfer Technologies for Body-Interfaced Electronics**: This article explores wireless energy transfer (WET) as a solution for powering body-interfaced electronics without the need for batteries or lead wires. It discusses the advantages and potential applications of WET in this context.

3. **Wireless Power Transfer and Energy Harvesting: Current Status and Future Trends**: This article provides an overview of recent advances in wireless power supply methods, including energy harvesting and wireless power transfer. It presents several promising applications and discusses future trends in the field.

4. **Wireless Power Transfer: Applications, Challenges, Barriers, and the
```
{{< /admonition >}}

{{< admonition note "langgraph" false>}}
```python 
# 1.æŸ¥æ‰¾å®¢äººä¿¡æ¯
response = alfred.invoke({"messages": "Tell me about 'Lady Ada Lovelace'"})

print("ğŸ© Alfred's Response:")
print(response['messages'][-1].content)
# 2.æ£€æŸ¥çƒŸèŠ±å¤©æ°”
response = alfred.invoke({"messages": "What's the weather like in Paris tonight? Will it be suitable for our fireworks display?"})

print("ğŸ© Alfred's Response:")
print(response['messages'][-1].content)
# 3. ç»™AI Builder å®¢äººç•™ä¸‹æ·±åˆ»å°è±¡
response = alfred.invoke({"messages": "One of our guests is from Qwen. What can you tell me about their most popular model?"})

print("ğŸ© Alfred's Response:")
print(response['messages'][-1].content)
# 4. ä½¿ç”¨å¤šä¸ªå·¥å…·ä¸å°¼å¤æ‹‰æ–¯åšå£«è¿›è¡Œå¯¹è¯
response = alfred.invoke({"messages":"I need to speak with 'Dr. Nikola Tesla' about recent advancements in wireless energy. Can you help me prepare for this conversation?"})

print("ğŸ© Alfred's Response:")
print(response['messages'][-1].content)
```
ALfredçš„å›ç­”ï¼š
```json
Based on the provided information, here are key points to prepare for the conversation with 'Dr. Nikola Tesla' about recent advancements in wireless energy:\n1. **Wireless Power Transmission (WPT):** Discuss how WPT revolutionizes energy transfer by eliminating the need for cords and leveraging mechanisms like inductive and resonant coupling.\n2. **Advancements in Wireless Charging:** Highlight improvements in efficiency, faster charging speeds, and the rise of Qi/Qi2 certified wireless charging solutions.\n3. **5G-Advanced Innovations and NearLink Wireless Protocol:** Mention these as developments that enhance speed, security, and efficiency in wireless networks, which can support advanced wireless energy technologies.\n4. **AI and ML at the Edge:** Talk about how AI and machine learning will rely on wireless networks to bring intelligence to the edge, enhancing automation and intelligence in smart homes and buildings.\n5. **Matter, Thread, and Security Advancements:** Discuss these as key innovations that drive connectivity, efficiency, and security in IoT devices and systems.\n6. **Breakthroughs in Wireless Charging Technology:** Include any recent breakthroughs or studies, such as the one from Incheon National University, to substantiate the advancements in wireless charging.
```
{{< /admonition >}}
# é«˜çº§åŠŸèƒ½ï¼šè®°å¿†ï¼
ä¸ºäº†è®© Alfred åœ¨æ™šä¼šä¸Šæä¾›æ›´å¤šå¸®åŠ©ï¼Œæˆ‘ä»¬å¯ä»¥å¯ç”¨å¯¹è¯è®°å¿†åŠŸèƒ½ï¼Œä»¥ä¾¿ä»–è®°ä½ä¹‹å‰çš„äº’åŠ¨ï¼š
{{< admonition note "smolagents" false>}}
```python
# Create Alfred with conversation memory
alfred_with_memory = CodeAgent(
    tools=[guest_info_tool, weather_info_tool, hub_stats_tool, search_tool], 
    model=model,
    add_base_tools=True,
    planning_interval=3 # ä»£ç†æ¯æ‰§è¡Œ 3 ä¸ªå·¥å…·è°ƒç”¨ åï¼Œä¼šåŸºäºå½“å‰è®°å¿†é‡æ–°è§„åˆ’åç»­æ­¥éª¤ã€‚
)

# First interaction
response1 = alfred_with_memory.run("Tell me about Lady Ada Lovelace.")
print("ğŸ© Alfred's First Response:")
print(response1)

# Second interaction (referencing the first)
response2 = alfred_with_memory.run("What projects is she currently working on?", reset=False)
print("ğŸ© Alfred's Second Response:")
print(response2)
```
{{< /admonition >}}

{{< admonition note "llama-index" false>}}
```python
from llama_index.core.workflow import Context

alfred = AgentWorkFlow.from_tools_or_functions(
  [guest_info_tool, search_tool, weather_info_tool, hub_stats_tool],
    llm=llm
)

# Remembering state
ctx = Context(alfred)

# First interaction
response1 = await alfred.run("Tell me about Lady Ada Lovelace.", ctx=ctx)
print("ğŸ© Alfred's First Response:")
print(response1)

# Second interaction (referencing the first)
response2 = await alfred.run("What projects is she currently working on?", ctx=ctx)
print("ğŸ© Alfred's Second Response:")
print(response2)
```
{{< /admonition >}}

{{< admonition note "langgraph" false>}}
æ˜¾å¼ä¼ é€’æ¶ˆæ¯
```python
# First interaction
response = alfred.invoke({"messages": [HumanMessage(content="Tell me about 'Lady Ada Lovelace'. What's her background and how is she related to me?")]})


print("ğŸ© Alfred's Response:")
print(response['messages'][-1].content)
print()

# Second interaction (referencing the first)
response = alfred.invoke({"messages": response["messages"] + [HumanMessage(content="What projects is she currently working on?")]})

print("ğŸ© Alfred's Response:")
print(response['messages'][-1].content)
```
{{< /admonition >}}
æ€»ç»“ä¸€ä¸‹ï¼š
- smolagentsï¼šå†…å­˜ä¸ä¼šåœ¨ä¸åŒçš„æ‰§è¡Œè¿è¡Œä¸­ä¿ç•™ï¼Œæ‚¨å¿…é¡»ä½¿ç”¨ reset=False æ˜ç¡®å£°æ˜å®ƒã€‚
- LlamaIndexï¼šéœ€è¦åœ¨è¿è¡Œä¸­æ˜ç¡®æ·»åŠ ç”¨äºå†…å­˜ç®¡ç†çš„ä¸Šä¸‹æ–‡å¯¹è±¡ã€‚
- LangGraphï¼šæä¾›æ£€ç´¢ä»¥å‰çš„æ¶ˆæ¯æˆ–ä½¿ç”¨ä¸“ç”¨ [MemorySaver ](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/#part-3-adding-memory-to-the-chatbot)ç»„ä»¶çš„é€‰é¡¹ã€‚


å®Œç»“æ’’èŠ±~