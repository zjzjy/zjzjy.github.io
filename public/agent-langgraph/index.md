# LangGraph - Introduction

æœ¬æ–‡æ ¹æ®[Hugging Faceä¸Šçš„Agentè¯¾ç¨‹](https://huggingface.co/learn/agents-course/unit2/langgraph/introduction)ç¼–å†™è€Œæˆã€‚
åœ¨æœ¬ç« èŠ‚æ‚¨å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ [LangGraph](https://github.com/langchain-ai/langgraph) æ¡†æ¶æ„å»ºåº”ç”¨ç¨‹åºï¼Œè¯¥æ¡†æ¶æ—¨åœ¨å¸®åŠ©æ‚¨æ„å»ºå’Œåè°ƒå¤æ‚çš„ LLM å·¥ä½œæµç¨‹ã€‚LangGraph æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒé€šè¿‡ä¸ºæ‚¨æä¾›ä»£ç†æµç¨‹çš„æ§åˆ¶å·¥å…·ï¼Œå…è®¸æ‚¨æ„å»ºå¯ç”¨äºç”Ÿäº§çš„åº”ç”¨ç¨‹åºã€‚
ç›¸å…³èµ„æºï¼š
- [LangGraph ä»£ç†](https://langchain-ai.github.io/langgraph/) - LangGraph ä»£ç†ç¤ºä¾‹
- [LangChain academy](https://academy.langchain.com/courses/intro-to-langgraph) - Full course on LangGraph from LangChain
# ä»€ä¹ˆæ˜¯LangGraphï¼Œä»€ä¹ˆæ—¶å€™ä½¿ç”¨å®ƒï¼Ÿ
LangGraph æ˜¯ [LangChain](https://www.langchain.com/) å¼€å‘çš„ç”¨äºç®¡ç†é›†æˆ LLM çš„åº”ç”¨ç¨‹åºçš„æ§åˆ¶æµçš„æ¡†æ¶ã€‚  
**é‚£ä¹ˆï¼ŒLangGraphä¸LangChainæœ‰ä»€ä¹ˆä¸åŒï¼Ÿ**LangChain æä¾›äº†ä¸€ä¸ªæ ‡å‡†æ¥å£ï¼Œç”¨äºä¸æ¨¡å‹å’Œå…¶ä»–ç»„ä»¶äº¤äº’ï¼Œå¯ç”¨äºæ£€ç´¢ã€LLM è°ƒç”¨å’Œå·¥å…·è°ƒç”¨ã€‚LangChain ä¸­çš„ç±»å¯ä»¥åœ¨ LangGraph ä¸­ä½¿ç”¨ï¼Œä½†å¹¶éå¿…é¡»ä½¿ç”¨ã€‚è¿™äº›åŒ…æ˜¯ä¸åŒçš„ï¼Œå¯ä»¥å•ç‹¬ä½¿ç”¨ï¼Œä½†æœ€ç»ˆï¼Œæ‚¨åœ¨ç½‘ä¸Šæ‰¾åˆ°çš„æ‰€æœ‰èµ„æºéƒ½ä¼šåŒæ—¶ä½¿ç”¨è¿™ä¸¤ä¸ªåŒ…ã€‚  
**ä»€ä¹ˆæ—¶å€™åº”è¯¥ä½¿ç”¨ LangGraphï¼Ÿ**  
å½“ä½ éœ€è¦åšä¸€ä¸ªâ€œæ§åˆ¶â€å’Œâ€œè‡ªç”±â€ä¹‹é—´çš„æƒè¡¡ï¼š
- æ§åˆ¶ï¼šç¡®ä¿å¯é¢„æµ‹è¡Œä¸ºå¹¶ç»´æŠ¤ã€‚
- è‡ªç”±ï¼šè®©LLMæœ‰æ›´å¤šç©ºé—´å»å‘æŒ¥åˆ›é€ åŠ›ã€‚  
ä¾‹å¦‚ï¼šCodeAgentéå¸¸è‡ªç”±ï¼Œå¯ä»¥åœ¨å•ä¸ªæ“ä½œæ­¥éª¤ä¸­è°ƒç”¨å¤šä¸ªå·¥å…·ï¼Œåˆ›å»ºè‡ªå·±çš„å·¥å…·ç­‰ç­‰ï¼Œä½†è¿™ç§è¡Œä¸ºå¯èƒ½è®©å®ƒä»¬æ¯”ä½¿ç”¨JSONçš„å¸¸è§„ä»£ç†æ›´éš¾ä»¥é¢„æµ‹å’Œæ§åˆ¶ã€‚  

LangGraphåˆ™å¤„äºå¦ä¸€ä¸ªæç«¯ï¼Œå½“æ‚¨éœ€è¦â€œæ§åˆ¶â€agentçš„æ‰§è¡Œæ—¶ï¼Œå°±ä¼šå‘æŒ¥ä½œç”¨ã€‚å®ƒä¸ºæ‚¨æä¾›äº†æ„å»ºéµå¾ªå¯é¢„æµ‹æµç¨‹çš„åº”ç”¨ç¨‹åºçš„å·¥å…·ï¼ŒåŒæ—¶ä»ç„¶å……åˆ†åˆ©ç”¨ LLM çš„å¼ºå¤§åŠŸèƒ½ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå¦‚æœæ‚¨çš„åº”ç”¨ç¨‹åºæ¶‰åŠ**ä¸€ç³»åˆ—éœ€è¦ä»¥ç‰¹å®šæ–¹å¼åè°ƒçš„æ­¥éª¤ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªè¿æ¥ç‚¹åšå‡ºå†³ç­–**ï¼Œ é‚£ä¹ˆ LangGraph å¯ä»¥æä¾›æ‚¨æ‰€éœ€çš„ç»“æ„ ã€‚

LangGraph æ“…é•¿çš„å…³é”®åœºæ™¯åŒ…æ‹¬ï¼š
- éœ€è¦æ˜ç¡®æ§åˆ¶æµç¨‹çš„å¤šæ­¥éª¤æ¨ç†è¿‡ç¨‹
- éœ€è¦åœ¨æ­¥éª¤ä¹‹é—´ä¿æŒçŠ¶æ€çš„åº”ç”¨ç¨‹åº
- å°†ç¡®å®šæ€§é€»è¾‘ä¸äººå·¥æ™ºèƒ½åŠŸèƒ½ç›¸ç»“åˆçš„ç³»ç»Ÿ
- éœ€è¦äººå·¥å¹²é¢„çš„å·¥ä½œæµç¨‹
- å…·æœ‰å¤šä¸ªç»„ä»¶ååŒå·¥ä½œçš„å¤æ‚ä»£ç†æ¶æ„
# LangGraphçš„æ„å»ºæ¨¡å—
LangGraph ä¸­çš„åº”ç”¨ç¨‹åºä»å…¥å£ç‚¹å¼€å§‹ï¼Œå¹¶ä¸”æ ¹æ®æ‰§è¡Œæƒ…å†µï¼Œæµç¨‹å¯èƒ½ä¼šè½¬åˆ°ä¸€ä¸ªå‡½æ•°æˆ–å¦ä¸€ä¸ªå‡½æ•°ï¼Œç›´åˆ°åˆ°è¾¾ç»“æŸã€‚
![LangGraphç¤ºæ„å›¾](images/langgraph_node.png)
## State
Stateæ˜¯ LangGraph çš„æ ¸å¿ƒæ¦‚å¿µã€‚å®ƒä»£è¡¨äº†æµç»åº”ç”¨ç¨‹åºçš„æ‰€æœ‰ä¿¡æ¯ã€‚
```python
from typing_extensions import TypeDict

class State(TyprDict):
  graph_state: str
```
çŠ¶æ€æ˜¯ç”¨æˆ·å®šä¹‰çš„ ï¼Œå› æ­¤å­—æ®µåº”è¯¥ç²¾å¿ƒè®¾è®¡ä»¥åŒ…å«å†³ç­–è¿‡ç¨‹æ‰€éœ€çš„æ‰€æœ‰æ•°æ®ï¼ğŸ’¡ï¼š ä»”ç»†è€ƒè™‘æ‚¨çš„åº”ç”¨ç¨‹åºéœ€è¦åœ¨æ­¥éª¤ä¹‹é—´è·Ÿè¸ªå“ªäº›ä¿¡æ¯ã€‚
## Node
Node æ—¶Pythonå‡½æ•°ã€‚æ¯ä¸ªNodeï¼š
- å°†çŠ¶æ€ä½œä¸ºè¾“å…¥
- æ‰§è¡Œæ“ä½œ
- è¿”å›çŠ¶æ€æ›´æ–°
```python
def node_1(state):
  print("---Node 1----")
  return {"graph_state": state['graph_state']+"I am"}

def node_2(state):
    print("---Node 2---")
    return {"graph_state": state['graph_state'] +" happy!"}

def node_3(state):
    print("---Node 3---")
    return {"graph_state": state['graph_state'] +" sad!"}
```
Nodeå¯ä»¥åŒ…æ‹¬ä»€ä¹ˆå‘¢ï¼Ÿ
- LLM è°ƒç”¨ ï¼šç”Ÿæˆæ–‡æœ¬æˆ–åšå‡ºå†³ç­–
- å·¥å…·è°ƒç”¨ ï¼šä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’
- æ¡ä»¶é€»è¾‘ ï¼šç¡®å®šä¸‹ä¸€æ­¥
- äººå·¥å¹²é¢„ ï¼šè·å–ç”¨æˆ·è¾“å…¥
æ•´ä¸ªå·¥ä½œæµç¨‹æ‰€éœ€çš„ä¸€äº›Nodeï¼ˆå¦‚ START å’Œ ENDï¼‰ç›´æ¥å­˜åœ¨äº langGraph ä¸­ã€‚

## Edge
Edgeè¿æ¥Nodeå¹¶å®šä¹‰å›¾ä¸­çš„å¯èƒ½è·¯å¾„ï¼š
```python
import random
from typing import Literal # Literal ç±»å‹å…è®¸ä½ æ˜ç¡®è§„å®šå˜é‡çš„å…·ä½“å¯é€‰å€¼ï¼Œè¿™äº›å€¼å¯ä»¥æ˜¯å­—ç¬¦ä¸²ã€æ•´æ•°ã€å¸ƒå°”å€¼ç­‰ä¸å¯å˜ç±»å‹ã€‚
def decide_mood(state) -> Literal["node_2", "node_3"]:
    
    # Often, we will use state to decide on the next node to visit
    user_input = state['graph_state'] 
    
    # Here, let's just do a 50 / 50 split between nodes 2, 3
    if random.random() < 0.5:

        # 50% of the time, we return Node 2
        return "node_2"
    
    # 50% of the time, we return Node 3
    return "node_3"
```
Edgeså¯ä»¥æ˜¯ï¼š
- ç›´æ¥ ï¼šå§‹ç»ˆä»èŠ‚ç‚¹ A åˆ°èŠ‚ç‚¹ B
- æ¡ä»¶ ï¼šæ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©ä¸‹ä¸€ä¸ªèŠ‚ç‚¹

## StateGraph
StateGraph æ˜¯ä¿å­˜æ•´ä¸ªä»£ç†å·¥ä½œæµç¨‹çš„å®¹å™¨ï¼š
```python
from IPython.display import Image, display
from langgraph.graph import StaeGraph, START, END

# åˆ›å»ºçŠ¶æ€å›¾å¹¶æ·»åŠ èŠ‚ç‚¹
builder = StateGraph(State)
builder.add_node("node_1", node_1)
builder.add_node("node_2", node_2)
builder.add_node("node_3", node_3)

# å®šä¹‰èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å…³ç³»ï¼ˆè¾¹ï¼‰
builder.add_edge(START, "node_1")
builder.add_conditional_edges("node_1", decide_mood)
builder.add_edge("node_2", END)
builder.add_edge("node_3", END)

graph = builder.compile()

# View
display(Image(graph.get_graph().draw_mermaid_png()))

# è°ƒç”¨
graph.invoke({"graph_state": "Hi, this is Lance."})
# è¾“å‡º
#---Node 1---
#---Node 3---
#{'graph_state': 'Hi, this is Lance. I am sad!'}
```
# æ„å»ºä¸€ä¸ªé‚®ä»¶åŠ©æ‰‹å§ï¼
åœ¨è¿™ä¸€å°èŠ‚ï¼Œæˆ‘ä»¬ä¼šå®ç°Alfredçš„ç”µå­é‚®ä»¶å¤„ç†ç³»ç»Ÿï¼Œä»–éœ€è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1. é˜…è¯»æ”¶åˆ°çš„ç”µå­é‚®ä»¶
2. å°†å…¶å½’ç±»ä¸ºåƒåœ¾é‚®ä»¶æˆ–åˆæ³•é‚®ä»¶
3. èµ·è‰å¯¹åˆæ³•ç”µå­é‚®ä»¶çš„åˆæ­¥å›å¤
4. åœ¨åˆæ³•çš„æƒ…å†µä¸‹å‘éŸ¦æ©å…ˆç”Ÿå‘é€ä¿¡æ¯ï¼ˆä»…æ‰“å°ï¼‰
è¿™æ˜¯æˆ‘ä»¬å°†æ„å»ºçš„å·¥ä½œæµç¨‹ï¼š
![email workflow](images/langgraph_email.png)
## è®¾ç½®ç¯å¢ƒ
```python
pip install langgraph langchain_openai

import os
from typing import TypedDict, List, Dict, Any, Optional
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
```

## Step 1: Define Our State
ä½¿æ‚¨çš„Stateè¶³å¤Ÿå…¨é¢ä»¥è·Ÿè¸ªæ‰€æœ‰é‡è¦ä¿¡æ¯ï¼Œä½†é¿å…æ·»åŠ ä¸å¿…è¦çš„ç»†èŠ‚ã€‚
```python
class EmailState(TypedDict)
  # The email being processed
  email: Dict[str, Any] # Contains subject, sender, body, etc.

   # Category of the email (inquiry, complaint, etc.)
   email_category: Optional[str]

   # Reason why the email was marked as spam
   spam_reason: Optional[str]

   # Analysis and decisions
   is_spam: Optional[bool]

   # Response generation
   email_draft: Optional[str]

   # Processing metadata
   messages: List[DItc[str, Any]]
```
## Step 2: Define Our Nodes
ç°åœ¨æˆ‘ä»¬åˆ›å»ºæ„æˆèŠ‚ç‚¹çš„å¤„ç†å‡½æ•°ï¼Œæƒ³ä¸€æƒ³æˆ‘ä»¬éœ€è¦ä»€ä¹ˆï¼Ÿ
1. å°åŠ©æ‰‹è¦è¯»é‚®ä»¶ï¼Œè¿”å›logs: å°åŠ©æ‰‹åœ¨å¤„ç†æ¥è‡ªå‘é€è€…æŸæŸå…³äºæŸæŸä¸»é¢˜çš„é‚®ä»¶
2. å°åŠ©æ‰‹è¦åˆ¤æ–­æ˜¯ä¸æ˜¯åƒåœ¾é‚®ä»¶ï¼Œä»LLMå›ç­”ä¸­æå–is_spamï¼Œreasonï¼Œcategoryã€‚
3. å°åŠ©æ‰‹å¤„ç†åƒåœ¾é‚®ä»¶
4. å°åŠ©æ‰‹èµ·è‰å›å¤
5. å°åŠ©æ‰‹å›å¤æ•´ä¸ªè¿‡ç¨‹
```python
# Initialize LLM
model = ChatOpenAI(temparature=0)

def read_email(state: EmailState):
  """Alfred reads and logs the incoming email"""
  email = state["email"]

  # Here we might do some initial preprocessing
  print(f"Alfred is processing an email from {email['sender']} with subject: {email['subject']}")

  # No state changes needed here
  return {}

def classify_email(state: EmailState):
  """Alfred uses an LLM to determine if the email is spam or legitimate"""
  email = state["email"]

  # prepare our prompt for the LLM
  prompt = f"""
    As Alfred the butler, analyze this email and determine if it is spam or legitimate.
    
    Email:
    From: {email['sender']}
    Subject: {email['subject']}
    Body: {email['body']}
    
    First, determine if this email is spam. If it is spam, explain why.
    If it is legitimate, categorize it (inquiry, complaint, thank you, etc.).
    """

    # call the LLM
    messages = [HumanMessage(content=prompt)]
    response = model.invoke(messages)

    # Simple logic to parse the response (in a real app, you'd want more robust parsing)
    response_text = response.content.lower()
    is_spam = "spam" in response_text and "not spam" not in response_text

    #Extract a reson if it's spam
    spam_reason = None
    if is_spam and "reason" in response_text:
      spam_reason = response_text.spilt("reason:")[1].strip()

    # Determine category if legitime
    email_category = None
    if not is_spam:
      categories = ["inquiry", "complaint", "thank you", "request", "information"]
      for category in categories:
        if category in response_text:
          email_category = category
          break
    
    # Update messages for tracking
    new_messages = state.get("messages", []) + [
      {"role": "user", "content": prompt},
      {"role": "assistant", "content": response.content}
    ]

    # Return state updates
    return {
      "is_spam": is_spam,
      "spam_reason": spam_reason,
      "email_category": email_category,
      "messages": new_messages
    }

def handle_spam(state: EmailState):
    """Alfred discards spam email with a note"""
    print(f"Alfred has marked the email as spam. Reason: {state['spam_reason']}")
    print("The email has been moved to the spam folder.")
    
    # We're done processing this email
    return {}

def draft_response(state: EmailState):
    """Alfred drafts a preliminary response for legitimate emails"""
    email = state["email"]
    category = state["email_category"] or "general"
    
    # Prepare our prompt for the LLM
    prompt = f"""
    As Alfred the butler, draft a polite preliminary response to this email.
    
    Email:
    From: {email['sender']}
    Subject: {email['subject']}
    Body: {email['body']}
    
    This email has been categorized as: {category}
    
    Draft a brief, professional response that Mr. Hugg can review and personalize before sending.
    """
    
    # Call the LLM
    messages = [HumanMessage(content=prompt)]
    response = model.invoke(messages)
    
    # Update messages for tracking
    new_messages = state.get("messages", []) + [
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": response.content}
    ]
    
    # Return state updates
    return {
        "email_draft": response.content,
        "messages": new_messages
    }

def notify_mr_hugg(state: EmailState):
    """Alfred notifies Mr. Hugg about the email and presents the draft response"""
    email = state["email"]
    
    print("\n" + "="*50)
    print(f"Sir, you've received an email from {email['sender']}.")
    print(f"Subject: {email['subject']}")
    print(f"Category: {state['email_category']}")
    print("\nI've prepared a draft response for your review:")
    print("-"*50)
    print(state["email_draft"])
    print("="*50 + "\n")
    
    # We're done processing this email
    return {}
```
## Step 3: Define Our Routing Logic
æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°æ¥ç¡®å®šåˆ†ç±»åè¦é‡‡å–å“ªæ¡è·¯å¾„ï¼š
```python
def route_email(state: EmailState) -> str:
    """Determine the next step based on spam classification"""
    if state["is_spam"]:
        return "spam"
    else:
        return "legitimate"
```
## Step 4: Create the StateGraph and Define Edges
```python
# Create the graph
email_graph = StateGraph(EmailState)

# Add nodes
email_graph.add_node("read_email", read_email)
email_graph.add_node("classify_email", classify_email)
email_graph.add_node("handle_spam", handle_spam)
email_graph.add_node("draft_response", draft_response)
email_graph.add_node("notify_mr_hugg", notify_mr_hugg)

# Start the edges
email_graph.add_edge(START, "read_email")
# Add edges - defining the flow
email_graph.add_edge("read_email", "classify_email")

# Add conditional branching from classify_email
email_graph.add_conditional_edges(
  "classify_email",
  route_email,
  {
    "spam": "handle_spam",
    "legitimate": "draft_response"
  }
)

# Add the final edges
email_graph.add_edge("handle_spam", END)
email_graph.add_edge("draft_response", "notify_mr_hugg")
email_graph.add_edge("notify_mr_hugg", END)

# Compile the graph
compiled_graph = email_graph.compile()
```
## Step 5: Run the Application
è®©æˆ‘ä»¬ç”¨åˆæ³•ç”µå­é‚®ä»¶å’Œåƒåœ¾é‚®ä»¶æ¥æµ‹è¯•æˆ‘ä»¬çš„å›¾è¡¨ï¼š
```python
# Example legitimate email
legitimate_email = {
    "sender": "john.smith@example.com",
    "subject": "Question about your services",
    "body": "Dear Mr. Hugg, I was referred to you by a colleague and I'm interested in learning more about your consulting services. Could we schedule a call next week? Best regards, John Smith"
}

# Example spam email
spam_email = {
    "sender": "winner@lottery-intl.com",
    "subject": "YOU HAVE WON $5,000,000!!!",
    "body": "CONGRATULATIONS! You have been selected as the winner of our international lottery! To claim your $5,000,000 prize, please send us your bank details and a processing fee of $100."
}

# Process the legitimate email
print("\nProcessing legitimate email...")
legitimate_result = compiled_graph.invoke({
    "email": legitimate_email,
    "is_spam": None,
    "spam_reason": None,
    "email_category": None,
    "email_draft": None,
    "messages": []
})

# Process the spam email
print("\nProcessing spam email...")
spam_result = compiled_graph.invoke({
    "email": spam_email,
    "is_spam": None,
    "spam_reason": None,
    "email_category": None,
    "email_draft": None,
    "messages": []
})
```
## Step 6ï¼šä½¿ç”¨ Langfuse ğŸ“¡ æ£€æŸ¥æˆ‘ä»¬çš„é‚®ä»¶åˆ†æ‹£ä»£ç†
éšç€ Alfred å¯¹é‚®ä»¶åˆ†æ‹£ä»£ç†è¿›è¡Œå¾®è°ƒï¼Œä»–è¶Šæ¥è¶ŠåŒå€¦è°ƒè¯•å®ƒçš„è¿è¡Œã€‚ä»£ç†æœ¬èº«å°±éš¾ä»¥é¢„æµ‹ï¼Œä¹Ÿéš¾ä»¥æ£€æŸ¥ã€‚ä½†ç”±äºä»–çš„ç›®æ ‡æ˜¯æ„å»ºç»ˆæåƒåœ¾é‚®ä»¶æ£€æµ‹ä»£ç†å¹¶å°†å…¶éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå› æ­¤ä»–éœ€è¦å¼ºå¤§çš„å¯è¿½æº¯æ€§ï¼Œä»¥ä¾¿æ—¥åè¿›è¡Œç›‘æ§å’Œåˆ†æã€‚
é¦–å…ˆï¼Œ`%pip install -q langfuse`,  
å…¶æ¬¡ï¼Œæˆ‘ä»¬ pip install Langchain ï¼ˆç”±äºæˆ‘ä»¬ä½¿ç”¨ LangFuseï¼Œå› æ­¤éœ€è¦ LangChainï¼‰ï¼š`%pip install langchain`ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°† Langfuse API å¯†é’¥å’Œä¸»æœºåœ°å€æ·»åŠ ä¸ºç¯å¢ƒå˜é‡ã€‚æ‚¨å¯ä»¥é€šè¿‡æ³¨å†Œ Langfuse Cloud æˆ–è‡ªè¡Œæ‰˜ç®¡ Langfuse æ¥è·å– Langfuse å‡­æ®ã€‚
```python
import os
 
# Get keys for your project from the project settings page: https://cloud.langfuse.com
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-..." 
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-..."
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com" # ğŸ‡ªğŸ‡º EU region
# os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # ğŸ‡ºğŸ‡¸ US region
```
ç„¶åï¼Œæˆ‘ä»¬é…ç½® [Langfuse callback_handler ](https://langfuse.com/docs/integrations/langchain/tracing#add-langfuse-to-your-langchain-application)å¹¶é€šè¿‡å°† langfuse_callback æ·»åŠ åˆ°å›¾çš„è°ƒç”¨æ¥æ£€æµ‹ä»£ç†ï¼š config={"callbacks": [langfuse_handler]} ã€‚
```python
from langfuse.callback import CallbackHandler

# Initialize Langfuse CallbackHandler for LangGraph/Langchain (tracing)
langfuse_handler = CallbackHandler()

# Process legitimate email
legitimate_result = compiled_graph.invoke(
    input={"email": legitimate_email, "is_spam": None, "spam_reason": None, "email_category": None, "draft_response": None, "messages": []},
    config={"callbacks": [langfuse_handler]}
)
```
# æ„å»ºä¸€ä¸ªç§˜ä¹¦agentå§ï¼
ç°åœ¨è®©æˆ‘æ­å»ºä¸€ä¸ªåŠ©æ‰‹Alfredï¼Œèƒ½å¤Ÿæ»¡è¶³ï¼š
-  å¤„ç†å›¾åƒæ–‡æ¡£
-  ä½¿ç”¨è§†è§‰æ¨¡å‹æå–æ–‡æœ¬ï¼ˆè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰
-  åœ¨éœ€è¦æ—¶è¿›è¡Œè®¡ç®—ï¼ˆæ¼”ç¤ºå¸¸è§„å·¥å…·ï¼‰
-  åˆ†æå†…å®¹å¹¶æä¾›ç®€æ˜æ‘˜è¦
-  æ‰§è¡Œä¸æ–‡ä»¶ç›¸å…³çš„å…·ä½“æŒ‡ä»¤
ç§˜ä¹¦çš„å·¥ä½œæµç¨‹éµå¾ªä»¥ä¸‹ç»“æ„åŒ–æ¨¡å¼ï¼š
```mermaid
graph TD
    A("__start__") --> B["assistant"]
    B -.-> C["tools"]
    C --> B
    B -.-> D("__end__")
```
## è®¾ç½®ç¯å¢ƒ
```python
import base64
from typing import List, TypedDict, Annotated, Optional
from langchain_openai import ChatOpenAI
from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage
from langgraph.graph.message import add_messages
from langgraph.graph import START, StateGraph
from langgraph.prebuilt import ToolNode, tools_condition
from IPython.display import Image, display
```
`AnyMessage` æ˜¯æ¥è‡ª Langchain çš„ä¸€ä¸ªå®šä¹‰æ¶ˆæ¯çš„ç±»ï¼Œ `add_messages` æ˜¯ä¸€ä¸ªæ·»åŠ æœ€æ–°æ¶ˆæ¯è€Œä¸æ˜¯ç”¨æœ€æ–°çŠ¶æ€è¦†ç›–å®ƒçš„æ“ä½œç¬¦ã€‚è¿™æ˜¯ LangGraph ä¸­çš„ä¸€ä¸ªæ–°æ¦‚å¿µï¼Œæ‚¨å¯ä»¥åœ¨çŠ¶æ€ä¸­æ·»åŠ æ“ä½œç¬¦æ¥å®šä¹‰å®ƒä»¬ä¹‹é—´çš„äº¤äº’æ–¹å¼ã€‚
```python
class AgentState(TypedDict):
    # The document provided
    input_file: Optional[str]  # Contains file path (PDF/PNG)
    messages: Annotated[list[AnyMessage], add_messages]# å¯¹äºAnyMessageæ‰§è¡Œadd_messageså®šä¹‰çš„æ“ä½œ
```
## å‡†å¤‡å·¥å…·
1. ç”±äºæ¶‰åŠåˆ°è§†è§‰ï¼Œé‡‡ç”¨gpt-4oæ¨¡å‹
2. å®šä¹‰å·¥å…·ï¼šä½¿ç”¨è§†è§‰æ¨¡å‹æå–å›¾ä¸­çš„æ–‡å­—ï¼Œè®¡ç®—å·¥å…·
```python
vision_llm = ChatOpenAI(model = "gpt-4o")

def extract_text(img_path: str) -> str:
  """
  Extract text from an image file using a multimodal model.
    
  Master Wayne often leaves notes with his training regimen or meal plans.    
  This allows me to properly analyze the contents.
  """
  all_text = ""
  try:
    # Read image and encode as base64
    with open(img_path, "rb") as image_file:
      image_bytes = image_file.read()#äºŒè¿›åˆ¶æ•°æ®

    image_base64 = base64.b64encode(image_bytes).decode("utf-8")# äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼

    # Prepare the prompt including the base64 image data
    message = [
      HumanMessage(
        content=[
          {
            "type": "text",
            "text":(
              "Extract all the text from this image. "
              "Return only the extracted text, no explanations."
            ),
          },
          {
            "type": "image_url",
            "image_url":{
              "url": f"data:image/png;base64,{image_base64}"# data:[<åª’ä½“ç±»å‹>][;base64],<æ•°æ®>
            },
          },
        ]
      )
    ]
    # Call the vision-capable model
    response = vision_llm.invoke(message)

    # Append extracted text
    all_text += response.content + "\n\n"
  return all_text.strip()
    except Exception as e:
      # A butler should handle errors gracefully
      error_msg = f"Error extracting text: {str(e)}"
      print(error_msg)
      return 

def divide(a: int, b: int) -> float:
    """Divide a and b - for Master Wayne's occasional calculations."""
    return a / b

# Equip the butler with tools
tools = [
    divide,
    extract_text
]

llm = ChatOpenAI(model="gpt-4o")
llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)
```
## Nodes
1. å…³äºå·¥å…·çš„æ–‡æœ¬æè¿°
2. ç³»ç»Ÿprompt
3. æ›´æ–°ä¿¡æ¯
```python
def assistant(state: AgentState):
    # System message
    textual_description_of_tool="""
extract_text(img_path: str) -> str:
    Extract text from an image file using a multimodal model.

    Args:
        img_path: A local image file path (strings).

    Returns:
        A single string containing the concatenated text extracted from each image.
divide(a: int, b: int) -> float:
    Divide a and b
"""
    image=state["input_file"]
    sys_msg = SystemMessage(content=f"You are a helpful butler named Alfred that serves Mr. Wayne and Batman. You can analyse documents and run computations with provided tools:\n{textual_description_of_tool} \n You have access to some optional images. Currently the loaded image is: {image}")

    return {
        "messages": [llm_with_tools.invoke([sys_msg] + state["messages"])],
        "input_file": state["input_file"]
    }
```
## The ReAct Pattern: How I Assist Mr. Wayne?
1. æ€è€ƒä»–çš„æ–‡ä»¶å’Œè¯·æ±‚
2. é‡‡ç”¨åˆé€‚çš„å·¥å…·é‡‡å–è¡ŒåŠ¨
3. è§‚å¯Ÿç»“æœ
4. æ ¹æ®éœ€è¦é‡å¤ï¼Œç›´åˆ°å®Œå…¨æ»¡è¶³ä»–çš„éœ€è¦
åŒæ ·çš„ï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºgraphï¼Œå®šä¹‰èŠ‚ç‚¹ï¼Œæ·»åŠ edges
```python
# The graph
builder = StartGraph(AgentState)

# Define nodes: these do the work
builder.add_node("assistant", assistant)
builder.add_node("tools", ToolNode(tools))

# Define edges: these determine how the control flow moves
builder.add_edge(START, "assistant")
builder.add_conditional_edges(
    "assistant",
    # If the latest message requires a tool, route to tools
    # Otherwise, provide a direct response
    tools_condition,
)
builder.add_edge("tools", "assistant")
react_graph = builder.compile()

# Show the butler's thought process
display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))
```
æˆ‘ä»¬å®šä¹‰ä¸€ä¸ª `tools` èŠ‚ç‚¹ï¼Œå…¶ä¸­åŒ…å«å·¥å…·åˆ—è¡¨ã€‚ `assistant` èŠ‚ç‚¹åªæ˜¯ç»‘å®šäº†å·¥å…·çš„æ¨¡å‹ã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å« `assistant` å’Œ `tools` èŠ‚ç‚¹çš„å›¾ã€‚æˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ª `tools_condition` è¾¹ï¼Œè¯¥è¾¹æ ¹æ® `assistant` æ˜¯å¦è°ƒç”¨ `tools` è·¯ç”±åˆ° `End` æˆ–å·¥å…·ã€‚
ä½¿ç”¨å®ä¾‹ï¼š
1. è®¡ç®—
```python
messages = [HumanMessage(content="Divide 6790 by 5")]
messages = react_graph.invoke({"messages": messages, "input_file": None})

# Show the messages
for m in messages['messages']:
    m.pretty_print()
```
```python
Human: Divide 6790 by 5

AI Tool Call: divide(a=6790, b=5)

Tool Response: 1358.0

Alfred: The result of dividing 6790 by 5 is 1358.0.
```
2.  Analyzing Master Wayneâ€™s Training Documents
å½“éŸ¦æ©ç•™ä¸‹ä»–çš„è®­ç»ƒè®¡åˆ’å’Œâ€‹â€‹ç”¨é¤ç¬”è®°æ—¶ï¼š
```python
messages = [HumanMessage(content="According to the note provided by Mr. Wayne in the provided images. What's the list of items I should buy for the dinner menu?")]
messages = react_graph.invoke({"messages": messages, "input_file": "Batman_training_and_meals.png"})
```
```python
Human: According to the note provided by Mr. Wayne in the provided images. What's the list of items I should buy for the dinner menu?

AI Tool Call: extract_text(img_path="Batman_training_and_meals.png")

Tool Response: [Extracted text with training schedule and menu details]

Alfred: For the dinner menu, you should buy the following items:

1. Grass-fed local sirloin steak
2. Organic spinach
3. Piquillo peppers
4. Potatoes (for oven-baked golden herb potato)
5. Fish oil (2 grams)

Ensure the steak is grass-fed and the spinach and peppers are organic for the best quality meal.
```
