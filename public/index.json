[{"categories":["GenRec"],"content":"Related Work Sequential Recommenders：序列推荐系统旨在根据用户过去的交互行为序列来预测用户接下来可能与之交互的项目。早期方法常依赖于马尔可夫链技术来基于历史交互建模用户行为。近年来，基于Transformer的模型被广泛应用于序列推荐系统中，这些模型能够捕捉用户交互序列中的长距离依赖关系。在使用时，首先需要收集用户与项目的交互数据，并按照时间顺序构建用户的行为序列。然后，选择合适的序列模型（如GRU、LSTM、Transformer等）来学习用户的行为模式。在训练过程中，模型会根据用户的历史行为序列来预测用户接下来可能感兴趣的项目。最后，在实际应用中，根据模型的预测结果为用户生成推荐列表。 GRU4REC：本文提出的TIGER框架在序列推荐任务中，借鉴了GRU4REC的思想，但采用了更先进的Transformer架构来建模用户的行为序列，从而提高了推荐性能。 SASRec：TIGER框架在建模用户行为序列时，借鉴了SASRec中自注意力机制的思想，但进一步通过生成式检索方法直接预测项目的语义ID，从而实现了更高效的检索和推荐。 BERT4Rec：TIGER框架在建模用户行为序列时，借鉴了BERT4Rec中Transformer架构的思想，但通过生成式检索方法直接预测项目的语义ID，从而实现了更高效的检索和推荐。 Semantic IDs：语义ID是一种用于表示项目的序列，它由一系列离散的标记组成，能够捕捉项目的语义信息。生成语义ID的过程通常包括两个步骤：首先，使用预训练的文本编码器（如BERT、Sentence-T5等）将项目的文本描述编码为嵌入向量；然后，通过自编码器框架（如RQ-VAE）对嵌入向量进行量化，生成语义ID。在推荐系统中使用语义ID时，首先需要为数据集中的每个项目生成语义ID。然后，在训练推荐模型时，将项目的语义ID作为输入，而不是传统的项目ID。这样，模型能够学习到项目之间的语义相似性，从而提高推荐的准确性和多样性。此外，语义ID的层次化特性还可以用于解决推荐系统中的冷启动问题。 VQ-Rec：TIGER框架在生成语义ID时，借鉴了VQ-Rec中矢量量化技术的思想，但采用了更先进的RQ-VAE方法来生成层次化的语义ID，从而提高了推荐的准确性和多样性。 RQ-VAE：TIGER框架在生成语义ID时，采用了RQ-VAE方法来量化项目的嵌入向量，从而生成层次化的语义ID。这种方法不仅提高了推荐的准确性和多样性，还减少了存储需求。 Generative Retrieval：生成式检索是一种信息检索方法，它利用生成模型直接预测目标项目的索引。与传统的基于嵌入的检索方法不同，生成式检索不需要为每个项目生成和存储单独的嵌入向量。在生成式检索中，项目通过语义ID来表示，模型通过自回归的方式预测用户接下来可能交互的项目的语义ID。在推荐系统中实现生成式检索时，首先需要训练一个生成模型（如基于Transformer的序列到序列模型），该模型能够根据用户的历史行为序列生成目标项目的语义ID。在检索阶段，模型会根据用户的交互历史自回归地解码目标项目的语义ID，然后通过查找表将语义ID映射回实际的项目。此外，通过调整生成过程中的温度参数，可以控制推荐结果的多样性。 DSI：DSI是第一个将端到端Transformer用于检索应用的系统。它通过为每个文档分配结构化的语义DocID，并在给定查询时自回归地返回DocID，实现了高效的检索。TIGER框架在生成式检索方面，借鉴了DSI中Transformer用于检索的思想，但进一步通过生成项目的语义ID来实现更高效的检索和推荐。 概念理解：什么叫直接预测目标项目的索引？什么叫“不需要为每个项目生成和存储单独的嵌入向量？ 在传统的基于嵌入的检索方法中，推荐系统通常会将查询（用户的意图）和候选项目都映射到同一个高维空间中。然后，通过计算查询嵌入和候选项目嵌入之间的相似度（如余弦相似度或内积），来找到最相关的候选项目。这种方法需要为每个项目生成一个嵌入向量，并将这些嵌入向量存储在一个索引结构中，以便快速检索。 而在生成式检索中，模型的目标是直接生成目标项目的索引（或标识符）。这里的“索引”可以理解为项目的唯一标识符，例如项目的ID。模型通过学习用户行为序列的模式，直接预测用户接下来可能交互的项目的ID，而不是先生成嵌入向量再进行相似度计算。这种方法避免了为每个项目生成和存储嵌入向量的需要，从而减少了存储需求。 概念理解：什么叫以自回归方式预测？ 自回归（Autoregressive）是一种生成模型的常见方法，它通过逐步生成序列中的下一个元素来构建输出。在生成式检索中，模型会根据用户的历史行为序列，逐步生成目标项目的索引。 ","date":"262622-22-20","objectID":"/genrec-paper-recommender-systems-with-generative-retrieval/:1:0","tags":["GenRec"],"title":"GenRec - Recommender Systems with Generative Retrieval","uri":"/genrec-paper-recommender-systems-with-generative-retrieval/"},{"categories":["GenRec"],"content":"框架介绍 生成语义ID：这一步涉及将项目的内容特征（例如标题、描述或图片）编码成嵌入向量，然后通过量化方案将这些嵌入向量转换成codewords元组，具体来说： 使用预训练的内容编码器（如 Sentence-T5 或 BERT）来生成项目的语义嵌入。 应用量化方法（例如 RQ-VAE）来生成语义ID，这通常涉及将连续的嵌入向量映射到离散的代码字空间。 训练生成式推荐系统：这一步涉及使用基于 Transformer 的序列到序列模型，并在语义ID上进行训练，以预测用户接下来可能交互的项目。 ","date":"262622-22-20","objectID":"/genrec-paper-recommender-systems-with-generative-retrieval/:2:0","tags":["GenRec"],"title":"GenRec - Recommender Systems with Generative Retrieval","uri":"/genrec-paper-recommender-systems-with-generative-retrieval/"},{"categories":["GenRec"],"content":"Semantic ID 怎么生成？ RQ-VAE：通过更新codebook和DNN encoder-decoder参数共同训练自动编码器。 RQ-VAE首先通过编码器$\\mathcal{E}$编码输入x，以学习潜在表示z：= $\\mathcal{E}$（x）。在d = 0时，初始残差简单地定义为r0：= z。在每个level的d中，我们都有一个CodeBook $\\mathcal{C}d$：= ${e_k}{k=1}^{K}$ = 1，其中k是codebook的大小。然后，通过将r0映射到该level的codebook中closest的嵌入来量化r0。 d = 0时，$e_{c_d}$的索引，即$c_0 = arg min_{i}∥r0-e_{k}∥$。对于下一个d = 1，残差定义为r1：= r0 - $e_{c_0}$。然后，类似于level 0，通过找到最接近r1的嵌入方式来计算第一级的代码。该过程是递归的M次重复，以获取代表语义ID的M codeWord元组。这种递归方法近似于从粗到细粒度的输入。请注意，我们选择为每个M级别使用大小K的单独代码簿，而不是使用单个MK大小的代码簿。之所以这样做，是因为residue的范数倾向于随着水平的增加而降低，因此允许不同水平的不同粒度。 我们有语义ID$(c_0,…,c_{m-1})$，z的量化表示为$\\hat{z}:=\\Sigma^{m-1}{d=0}e{c_i}$，再把$\\hat{z}$传递给decoder。RQ-VAE的loss定义为$\\mathcal{L}(x):=\\mathcal{L}{recon}+\\mathcal{L}{rqvae},\\text{ } \\mathcal{L}{recon}:=∥x-\\hat{x}∥^2,\\mathcal{L}{rqvae}:=\\Sigma^{m-1}{d=0}∥sg[r_i]-e{c_i}∥^2+\\beta∥r_i-sg[e_{c_i}]∥^2$。sg是stop-gradient，以做到分部分不打扰地参数更新。基于K-Means聚集的初始化为codebook。 ","date":"262622-22-20","objectID":"/genrec-paper-recommender-systems-with-generative-retrieval/:3:0","tags":["GenRec"],"title":"GenRec - Recommender Systems with Generative Retrieval","uri":"/genrec-paper-recommender-systems-with-generative-retrieval/"},{"categories":["GenRec"],"content":"做了哪些实验？ ","date":"262622-22-20","objectID":"/genrec-paper-recommender-systems-with-generative-retrieval/:4:0","tags":["GenRec"],"title":"GenRec - Recommender Systems with Generative Retrieval","uri":"/genrec-paper-recommender-systems-with-generative-retrieval/"},{"categories":["GenRec"],"content":"解决什么问题？怎么解决的？ ","date":"262622-22-20","objectID":"/genrec-paper-recommender-systems-with-generative-retrieval/:5:0","tags":["GenRec"],"title":"GenRec - Recommender Systems with Generative Retrieval","uri":"/genrec-paper-recommender-systems-with-generative-retrieval/"},{"categories":["GenRec"],"content":"有什么未来改进方向？ ","date":"262622-22-20","objectID":"/genrec-paper-recommender-systems-with-generative-retrieval/:6:0","tags":["GenRec"],"title":"GenRec - Recommender Systems with Generative Retrieval","uri":"/genrec-paper-recommender-systems-with-generative-retrieval/"},{"categories":["Agent"],"content":"本文根据Hugging Face上的Agent课程编写而成。 在本章节，我们将使用 Agentic RAG 创建一个工具来帮助主持晚会的友好经纪人 Alfred，该工具可用于回答有关晚会嘉宾的问题。 难忘的盛会 你决定举办一场本世纪最奢华、最奢华的派对。 这意味着丰盛的宴席、迷人的舞者、知名 DJ、精致的饮品、令人叹为观止的烟火表演等等。我们委托管家Alfred来全权举办这个盛会。为此，他需要掌握派对的所有信息，包括菜单、宾客、日程安排、天气预报等等！不仅如此，他还需要确保聚会取得成功，因此他需要能够在聚会期间回答有关聚会的任何问题 ，同时处理可能出现的意外情况。他无法独自完成这项工作，所以我们需要确保阿尔弗雷德能够获得他所需的所有信息和工具。 首先，我们给他列一份联欢晚会的硬性要求清单：在文艺复兴时期，受过良好教育的人需要具备三个主要特质：对体育、文化和科学知识的深厚造诣。因此，我们需要确保用我们的知识给宾客留下深刻印象，为他们打造一场真正难忘的盛会。然而，为了避免冲突， 在盛会上应该避免讨论政治和宗教等话题。 盛会需要充满乐趣，避免与信仰和理想相关的冲突。按照礼仪， 一位好的主人应该了解宾客的背景 ，包括他们的兴趣和事业。一位好的主人也会与宾客们闲聊八卦，分享他们的故事。最后，我们需要确保自己掌握一些天气常识 ，以便能够持续获得实时更新，确保在最佳时机燃放烟花，并以一声巨响结束庆典！🎆 创建工具 首先，我们将创建一个 RAG 工具，用于检索受邀者的最新详细信息。接下来，我们将开发用于网页搜索、天气更新和 Hugging Face Hub 模型下载统计的工具。 ","date":"222224-24-120","objectID":"/agent-agentic-rag/:0:0","tags":["Agent","Agentic RAG"],"title":"Agentic RAG - Usecase","uri":"/agent-agentic-rag/"},{"categories":["Agent"],"content":"为来宾创建 RAG 工具 我们将在HF Space开发我们的Agent。 tools.py：为Agent提供辅助工具。 retriever.py：实现检索功能，支持知识访问。 app.py：将所有组件集成到功能齐全的agent中。 使用的dataset，每个访客包含以下字段： Name: 客人的全名 Relation: 客人与主人的关系 Description：关于客人的简短传记或有趣的事实 Email Address：发送邀请或后续活动的联系信息 ","date":"222224-24-120","objectID":"/agent-agentic-rag/:1:0","tags":["Agent","Agentic RAG"],"title":"Agentic RAG - Usecase","uri":"/agent-agentic-rag/"},{"categories":["Agent"],"content":"Step1: 加载并准备数据集 我们提供了三种不同 Agent 库的实现方式，你可以展开下面的折叠框查看各自的代码。 smolagents 我们将使用 Hugging Face datasets 集库来加载数据集并将其转换为来自 langchain.docstore.document 模块的 Document 对象列表。 import datasets from langchain.docstore.document import Document # Load the dataset guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\") # Convert dataset entries into document objects docs = [ Document( page_content = \"\\n\".join([ f\"Name: {guest['name']}\", f\"Relation: {guest['relation']}\", f\"Description: {guest['description']}\", f\"Email: {guest['email']}\" ]), metadata={\"name\": guest[\"name\"]} ) for guest in guest_dataset ] llama-index 我们将使用 Hugging Face datasets 集库来加载数据集并将其转换为来自 llama_index.core.schema 模块的 Document 对象列表。 import datasets from llama_index.core.schema import Document # Load the dataset guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\") # Convert dataset entries into Document objects docs = [ Document( text=\"\\n\".join([ f\"Name: {guest_dataset['name'][i]}\", f\"Relation: {guest_dataset['relation'][i]}\", f\"Description: {guest_dataset['description'][i]}\", f\"Email: {guest_dataset['email'][i]}\" ]), metadata={\"name\": guest_dataset['name'][i]} ) for i in range(len(guest_dataset)) ] langgraph 我们将使用 Hugging Face datasets 集库来加载数据集并将其转换为来自 langchain.docstore.document 模块的 Document 对象列表。 import datasets from langchain.docstore.document import Document # Load the dataset guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\") # Convert dataset entries into document objects docs = [ Document( page_content = \"\\n\".join([ f\"Name: {guest['name']}\", f\"Relation: {guest['relation']}\", f\"Description: {guest['description']}\", f\"Email: {guest['email']}\" ]), metadata={\"name\": guest[\"name\"]} ) for guest in guest_dataset ] 在上面的代码中，我们：加载数据集，将每个客人条目转换为具有格式化内容的 Document 对象，将 Document 对象存储在列表中。 ","date":"222224-24-120","objectID":"/agent-agentic-rag/:1:1","tags":["Agent","Agentic RAG"],"title":"Agentic RAG - Usecase","uri":"/agent-agentic-rag/"},{"categories":["Agent"],"content":"Step2: 创建检索工具 smolagents 我们将使用 langchain_community.retrievers 模块中的 BM25Retriever 来创建检索工具。BM25是相关性搜索，如果要更高级的语义搜索，可以考虑embedding检索器，例如sentence-transformers 。 from solagents import Tool from langchain_community.retrievers import BM25Retriever class GuestInfoRetrieverTool(Tool): # 工具的元数据描述 name = \"guest_info_retriever\" description = \"Retrieves detailed information about gala guests based on their name or relation.\" inputs = { \"query\": { \"type\": \"string\", \"description\": \"The name or relation of the guest you want information about.\" } } output_type = \"string\" def __init__(self, docs): self.is_initialized = False self.retriever = BM25Retriever.from_document(docs) def forward(self, query: str): results = self.retriever.get_relevant_documents(query) if results: return \"\\n\\n\".join([doc.page_content for doc in results[:3]]) else: return \"No matching guest information found.\" # Initialize the tool guest_info_tool = GuestInfoRetrieverTool(docs) llama-index from llama_index.core.tools import FunctionTool from llama_index.retrievers.bm25 import BM25Retriever bm25_retriever = BM25Retriever.from_defaults(nodes = docs) def get_guest_info_retriever(query: str) -\u003e str: \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\" results = bm25_retriever(query) if results: return \"\\n\\n\".join([doc.text for doc in results[:3]]) else: return \"No matching guest information found.\" # Initialize the tool guest_info_tool = FunctionTool.from_defaults(get_guest_info_retriever) langgraph from langchain_community.retrievers import BM25Retriever from langchain.tools import Tool bm25_retriever = BM25Retriever.from_documents(docs) def extract_text(query: str) -\u003e str: \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\" results = bm25_retriever.invoke(query) if results: return \"\\n\\n\".join([doc.page_content for doc in results[:3]]) else: return \"No matching guest information found.\" guest_info_tool = Tool( name=\"guest_info_retriever\", func=extract_text, description=\"Retrieves detailed information about gala guests based on their name or relation.\" ) ","date":"222224-24-120","objectID":"/agent-agentic-rag/:1:2","tags":["Agent","Agentic RAG"],"title":"Agentic RAG - Usecase","uri":"/agent-agentic-rag/"},{"categories":["Agent"],"content":"Step3：将工具与Alfred集成 最后，让我们通过创建代理并为其配备自定义工具来将所有内容整合在一起： smolagents from smolagents import CodeAgent, InferenceClientModel # Initialize the Hugging Face model model = InferenceClientModel() # Create Alfred, our gala agent, with the guest info tool alfred = CodeAgent(tools=[guest_info_tool], model=model) # Example query Alfred might receive during the gala response = alfred.run(\"Tell me about our guest named 'Lady Ada Lovelace'.\") print(\"🎩 Alfred's Response:\") print(response) #🎩 Alfred's Response: #Based on the information I retrieved, Lady Ada Lovelace is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine. Her email address is ada.lovelace@example.com. llama-index from llama_index.core.agent.workflow import AgentWorkflow from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI # Initialize the Hugging Face model llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") # Create Alfred, our gala agent, with the guest info tool alfred = AgentWorkflow.from_tools_or_functions( [guest_info_tool], llm=llm, ) # Example query Alfred might receive during the gala response = await alfred.run(\"Tell me about our guest named 'Lady Ada Lovelace'.\") print(\"🎩 Alfred's Response:\") print(response) #🎩 Alfred's Response: #Lady Ada Lovelace is an esteemed mathematician and friend, renowned for her pioneering work in mathematics and computing. She is celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine. Her email is ada.lovelace@example.com. langgraph from typing import TypedDict, Annotated from langgraph.graph.message import add_messages from langchain_core.messages import AnyMessage, HumanMessage, AIMessage from langgraph.prebuilt import ToolNode from langgraph.graph import START, StateGraph from langgraph.prebuilt import tools_condition from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace # Generate the chat interface, including the tools llm = HuggingFaceEndpoint( repo_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\", huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN, ) chat = ChatHuggingFace(llm=llm, verbose=True) tools = [guest_info_tool] chat_with_tools = chat.bind_tools(tools) # Generate the AgentState and Agent graph class AgentState(TypedDict): messages: Annotated[list[AnyMessage], add_messages] def assistant(state: AgentState): return { \"messages\": [chat_with_tools.invoke(state[\"messages\"])], } ## The graph builder = StateGraph(AgentState) # Define nodes: these do the work builder.add_node(\"assistant\", assistant) builder.add_node(\"tools\", ToolNode(tools)) # Define edges: these determine how the control flow moves builder.add_edge(START, \"assistant\") builder.add_conditional_edges( \"assistant\", # If the latest message requires a tool, route to tools # Otherwise, provide a direct response tools_condition, ) builder.add_edge(\"tools\", \"assistant\") alfred = builder.compile() messages = [HumanMessage(content=\"Tell me about our guest named 'Lady Ada Lovelace'.\")] response = alfred.invoke({\"messages\": messages}) print(\"🎩 Alfred's Response:\") print(response['messages'][-1].content) #🎩 Alfred's Response: #Lady Ada Lovelace is an esteemed mathematician and pioneer in computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine. 最后一步发生了什么： 我们使用 HuggingFaceEndpoint 类初始化 HuggingFace 模型。我们还生成了一个聊天界面并附加了一些工具。 我们将代理（Alfred）创建为 StateGraph ，它使用边组合 2 个节点（ assistant 、 tools ） 我们要求阿尔弗雷德检索有关一位名叫“Lady Ada Lovelace”的客人的信息。 现在 Alfred 可以检索客人信息，请考虑如何增强此系统： 改进检索器以使用更复杂的算法，例如句子转换器 实现对话记忆 ，以便 Alfred 记住之前的互动 结合网络搜索获取陌生客人的最新信息 整合多个索引 ，从经过验证的来源获取更完整的信息。 ","date":"222224-24-120","objectID":"/agent-agentic-rag/:1:3","tags":["Agent","Agentic RAG"],"title":"Agentic RAG - Usecase","uri":"/agent-agentic-rag/"},{"categories":["Agent"],"content":"小结 关于solagents,llama-index, langgraph 小TIPs: 数据加载这块：smolagent和langgraph共用from langchain.docstore.document import Document，lamma-index用from llama_index.core.schema import Document加载document模块转化为Document object。 创建检索工具： 检索工具导入：Smolagent和langgraph使用langchain的BM25检索工具；llama-index使用from llama_index.retrievers.bm25 import BM25Retriever Smolagent的Tool库，使用方法：定义一个工具类继承自Tool，添加工具的元数据描述(name, description, inputs)，定义forward方法。 llama-index的from llama_index.core.tools import FunctionTool，直接定义python函数即可（注意要添加函数描述），最后FunctionTool.from_defaults(get_guest_info_retriever) 初始化工具 langgraph的from langchain.tools import Tool，定义python函数（注意要添加函数描述），初始化工具步骤需要手动描述(name, func,description), like guest_info_tool = Tool( name=\"guest_info_retriever\", func=extract_text, description=\"Retrieves detailed information about gala guests based on their name or relation.\") 工具集成： Smolagent：from smolagents import CodeAgent, InferenceClientModel，初始化model，直接使用CodeAgent即可，alfred = CodeAgent(tools=[guest_info_tool], model=model)。 llama-index：from llama_index.core.agent.workflow import AgentWorkflow，from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI，初始化定义好llm，直接用Agentflow即可alfred = AgentWorkflow.from_tools_or_functions([guest_info_tool],llm=llm,)。 langgraph：是个大工程。需要先初始化llm，工具列表，将llm与工具表绑定。初始化graph，定义node，定义edge。 ","date":"222224-24-120","objectID":"/agent-agentic-rag/:1:4","tags":["Agent","Agentic RAG"],"title":"Agentic RAG - Usecase","uri":"/agent-agentic-rag/"},{"categories":["Agent"],"content":"为您的Agent构建和集成工具 在本节中，我们将授予 Alfred 访问网络的权限，使他能够查找最新新闻和全球动态。此外，他还将能够访问天气数据和 Hugging Face 中心模型的下载统计数据，以便他就热门话题进行相关对话。 ","date":"222224-24-120","objectID":"/agent-agentic-rag/:2:0","tags":["Agent","Agentic RAG"],"title":"Agentic RAG - Usecase","uri":"/agent-agentic-rag/"},{"categories":["Agent"],"content":"Give Your Agent Access to the Web 我们需要确保Alfred德能够获取有关世界的最新新闻和信息。 让我们从为 Alfred 创建一个网络搜索工具开始吧！ solagents from smolagents import DuckDuckGoSearchTool search_tool = DuckDuckGoSearchTool() # Example usage results = search_tool(\"Who's the current President of France?\") print(resultts) # The current President of France in Emmanuel Macron. llama-index from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec from llama_index.core.tools import FunctionTool # Initialize the DuckDuckGo search tool tool_spec = DuckDuckGoSearchToolSpec() search_tool = FunctionTool.from_defaults(tool_spec.duckduckgo_full_search) # Example usage results = search_tool(\"Who's the current President of France?\") print(resultts.raw_output[-1]['body']) # The President of the French Republic is the head of state of France. The current President is Emmanuel Macron since 14 May 2017 defeating Marine Le Pen in the second round of the presidential election on 7 May 2017. List of French presidents (Fifth Republic) N° Portrait Name ... langgraph from langchain.community.tools import DuckDuckGoSearchRun search_tool = DuckDuckGoSearchRun() # Example usage results = search_tool.invoke(\"Who's the current President of France?\") print(resultts) # Emmanuel Macron (born December 21, 1977, Amiens, France) is a French banker and politician who was elected president of France in 2017... ","date":"222224-24-120","objectID":"/agent-agentic-rag/:2:1","tags":["Agent","Agentic RAG"],"title":"Agentic RAG - Usecase","uri":"/agent-agentic-rag/"},{"categories":["Agent"],"content":"创建自定义工具来获取天气信息以安排烟花表演 完美的庆典应该是在晴朗的天空下燃放烟花，我们需要确保烟花不会因为恶劣的天气而取消。让我们创建一个自定义工具，可用于调用外部天气 API 并获取给定位置的天气信息。为了简单起见，我们在本例中使用了一一个虚拟天气API，如果您想用真实的天气API，可以使用OpenWeatherMap API等。 smolagents from smolagents import Tool import random class WeatherInfoTool(Tool): name = \"weather_info\" description = \"Fetches dummy weather information for a given location.\" inputs = { \"location\" : { \"type\" : \"string\", \"description\": \"The location to get weather information for.\" } } output_type = \"string\" def forward(self, location:str): # Dummy weather data weather_conditions = [ {\"condition\": \"Rainy\", \"temp_c\": 15}, {\"condition\": \"Clear\", \"temp_c\": 25}, {\"condition\": \"Windy\", \"temp_c\": 20} ] # Randomly select a weather condition data = random.choice(weather_conditions) return f\"Weather in {location}: {data['condition']}, {data['temp_c']}°C\" # Initialize the tool weather_info_tool = WeatherInfoTool() llama-index import random from llama_index.core.tools import FunctionTool def get_weather_info(location: str) -\u003e str: \"\"\"Fetches dummy weather information for a given location.\"\"\" # Dummy weather data weather_conditions = [ {\"condition\": \"Rainy\", \"temp_c\": 15}, {\"condition\": \"Clear\", \"temp_c\": 25}, {\"condition\": \"Windy\", \"temp_c\": 20} ] # Randomly select a weather condition data = random.choice(weather_conditions) return f\"Weather in {location}: {data['condition']}, {data['temp_c']}°C\" # Initialize the tool weather_info_tool = FunctionTool.from_defaults(get_weather_info) langgraph from langchain.tools import Tool import random def get_weather_info(location: str) -\u003e str: \"\"\"Fetches dummy weather information for a given location.\"\"\" # Dummy weather data weather_conditions = [ {\"condition\": \"Rainy\", \"temp_c\": 15}, {\"condition\": \"Clear\", \"temp_c\": 25}, {\"condition\": \"Windy\", \"temp_c\": 20} ] # Randomly select a weather condition data = random.choice(weather_conditions) return f\"Weather in {location}: {data['condition']}, {data['temp_c']}°C\" # Initialize the tool weather_info_tool = Tool( name=\"get_weather_info\", func=get_weather_info, description=\"Fetches dummy weather information for a given location.\" ) ","date":"222224-24-120","objectID":"/agent-agentic-rag/:2:2","tags":["Agent","Agentic RAG"],"title":"Agentic RAG - Usecase","uri":"/agent-agentic-rag/"},{"categories":["Agent"],"content":"为AI Builders创建 Hub Stats Tool 出席此次盛会的都是 AI 开发者的精英。Alfred 希望通过讨论他们最受欢迎的模型、数据集和空间来给他们留下深刻印象。我们将创建一个工具，根据用户名从 Hugging Face Hub 获取模型统计数据。 smolagents from solagents import Tool from huggingface_hub import list_models class HubStatsTool(Tool): name = \"hub_stats\" description = \"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\" inputs = { \"author\":{ \"type\": \"string\", \"description\": \"The username of the model author/organization to find models from.\" } } output_type = \"string\" def forward(self, author: str): try: # List Models from the specified author, sorted by downloads models = list(list_models(author = author, sort = \"sdownloads\", direction=-1, limit=1)) if models: model = models[0] return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\" else: return f\"Nomodels found for author {author}\" except Exception as e: return f\"Error fetching models for {author}: {str(e)}\" # Initialize the tool hub_stats_tool = HubStatsTool() # Example usage print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook # The most downloaded model by facebook is facebook/esmfold_v1 with 12,544,550 downloads. llama-index import random from llama_index.core.tools import FunctionTool from huggingface_hub import list_models def get_hub_stats(author: str) -\u003e str: \"\"\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\"\" try: # List models from the specified author, sorted by downloads models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1)) if models: model = models[0] return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\" else: return f\"No models found for author {author}.\" except Exception as e: return f\"Error fetching models for {author}: {str(e)}\" # Initialize the tool hub_stats_tool = FunctionTool.from_defaults(get_hub_stats) # Example usage print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook langgraph from langchain.tools import Tool from huggingface_hub import list_models def get_hub_stats(author: str) -\u003e str: \"\"\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\"\" try: # List models from the specified author, sorted by downloads models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1)) if models: model = models[0] return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\" else: return f\"No models found for author {author}.\" except Exception as e: return f\"Error fetching models for {author}: {str(e)}\" # Initialize the tool hub_stats_tool = Tool( name=\"get_hub_stats\", func=get_hub_stats, description=\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\" ) # Example usage print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook ","date":"222224-24-120","objectID":"/agent-agentic-rag/:2:3","tags":["Agent","Agentic RAG"],"title":"Agentic RAG - Usecase","uri":"/agent-agentic-rag/"},{"categories":["Agent"],"content":"工具集成 现在我们已经拥有了所有的工具，让我们将它们集成到 Alfred 的代理中： smolagents from smolagents import CodeAgent, InferenceClientModel model = InferenceModel() alfred = CodeAgent( tools = [search_tool, weather_info_tool, hub_stats_tool], model = model ) # Example query Alfred might receive during the gala response = alfred.run(\"What is Facebook and what's their most popular model?\") print(\"🎩 Alfred's Response:\") print(response) llama-index from llama_index.core.agent.workflow import AgentWorkflow from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI # Initialize the Hugging Face model llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") # Create Alfred with all the tools alfred = AgentWorkflow.from_tools_or_functions( [search_tool, weather_info_tool, hub_stats_tool], llm=llm ) # Example query Alfred might receive during the gala response = await alfred.run(\"What is Facebook and what's their most popular model?\") print(\"🎩 Alfred's Response:\") print(response) langgraph from typing import TypedDict, Annotated from langgraph.graph.message import add_messages from langchain_core.messages import AnyMessage, HumanMessage, AIMessage from langgraph.prebulit import ToolNode from langgraph.graph import START, StateGraph from langgraph.prebuilt import tools_condition from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace # Generate the chat interface, including the tools llm = HuggingFaceEndpoint( repo_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\", huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN, ) chat = ChatHuggingFace(llm = llm, verbose = True) tools = [search_tool, weather_info_tool, hub_stats_tool] chat_with_tools = chat.bind_tools(tools) # Generate the AgentState and Agent graph class AgentState(TypedDict): messages: Annotated[list[AnyMessage], add_messages] def assistant(state: AgentState): return { \"messages\": [chat_with_tools.invoke(state[\"messages\"])], } # The graph builder = StateGraph(AgentState) # Define nodes: these do the work builder.add_node(\"assistant\", assistant) builder.add_node(\"tools\", ToolNode(tools)) # Define edges: these determine how control flow moves builder.add_edge(START, \"assistant\") builder.add_conditional_edges( \"assistant\", # If the latest message requires a tool, route to tools # Otherwise, provide a direct response tools_condition, ) builder.add_edge(\"tools\", \"assistant\") alfred = builder.compile() messages = [HumanMessages(content=\"Who is Facebook and what's their most popular model?\")] response = alfred.invoke({\"messages\": messages}) print(\"🎩 Alfred's Response:\") print(response['messages'][-1].content) ","date":"222224-24-120","objectID":"/agent-agentic-rag/:2:4","tags":["Agent","Agentic RAG"],"title":"Agentic RAG - Usecase","uri":"/agent-agentic-rag/"},{"categories":["Agent"],"content":"Creating Your Gala Agent 现在我们已经为 Alfred 构建了所有必要的组件，现在是时候将所有组件整合成一个完整的代理，以帮助我们举办奢华的盛会。 在本节中，我们将把客人信息检索、网络搜索、天气信息和 Hub 统计工具组合成一个强大的代理。 我们在之前已经实现了tools.py和retriever.py，接下来要导入它们。 solagents # Import necessary libraries import random from smolagents import CodeAgent, InferenceClientModel # Import our custom tools from their modules from tools import DuckDuckGoSearchTool, WeatherInfoTool, HubStatsTool from retriever import load_guest_dataset # Initialize the Hugging Face model model = InferenceClientModel() # Initialize the web search tool search_tool = DuckDuckGoSearchTool() # Initialize the weather tool weather_info_tool = WeatherInfoTool() # Initialize the Hub stats tool hub_stats_tool = HubStatsTool() # Load the guest dataset and initialize the guest info tool guest_info_tool = load_guest_dataset() # Create Alfred with all the tools alfred = CodeAgent( tools=[guest_info_tool, weather_info_tool, hub_stats_tool, search_tool], model=model, add_base_tools=True, # Add any additional base tools planning_interval=3 # Enable planning every 3 steps ) llama-index # Import necessary libraries from llama_index.core.agent.workflow import AgentWorkflow from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI from tools import search_tool, weather_info_tool, hub_stats_tool from retriever import guest_info_tool # Initialize the Hugging Face model llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") # Create Alfred with all the tools alfred = AgentWorkflow.from_tools_or_functions( [guest_info_tool, search_tool, weather_info_tool, hub_stats_tool], llm=llm, ) langgraph from typing import TypedDict, Annotated from langgraph.graph.message import add_messages from langchain_core.messages import AnyMessage, HumanMessage, AIMessage from langgraph.prebuilt import ToolNode from langgraph.graph import START, StateGraph from langgraph.prebuilt import tools_condition from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace from tools import DuckDuckGoSearchRun, weather_info_tool, hub_stats_tool from retriever import guest_info_tool # Initialize the web search tool search_tool = DuckDuckGoSearchRun() # Generate the chat interface, including the tools llm = HuggingFaceEndpoint( repo_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\", huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN, ) chat = ChatHuggingFace(llm=llm, verbose=True) tools = [guest_info_tool, search_tool, weather_info_tool, hub_stats_tool] chat_with_tools = chat.bind_tools(tools) # Generate the AgentState and Agent graph class AgentState(TypedDict): messages: Annotated[list[AnyMessage], add_messages] def assistant(state: AgentState): return { \"messages\": [chat_with_tools.invoke(state[\"messages\"])], } ## The graph builder = StateGraph(AgentState) # Define nodes: these do the work builder.add_node(\"assistant\", assistant) builder.add_node(\"tools\", ToolNode(tools)) # Define edges: these determine how the control flow moves builder.add_edge(START, \"assistant\") builder.add_conditional_edges( \"assistant\", # If the latest message requires a tool, route to tools # Otherwise, provide a direct response tools_condition, ) builder.add_edge(\"tools\", \"assistant\") alfred = builder.compile() 应用示范 我们将使用刚刚创建的Alfred Agent完成三个工作： 查找客人信息 检查烟花天气 给AI Builder 客人留下深刻印象 使用多个工具与尼古拉斯博士进行对话 smolagents # 1.查找客人信息 query = \"Tell me about 'Lady Ada Lovelace'\" response = alfred.run(query) print(\"🎩 Alfred's Response:\") print(response) # 2.检查烟花天气 query = \"What's the weather like in Paris tonight? Will it be suitable for our fireworks display?\" response = alfred.run(query) print(\"🎩 Alfred's Response:\") print(response) # 3. 给AI Builder 客人留下深刻印象 query = \"One of our guests is from Qwen. What can you tell me about their most popular model?\" response = alfred.run(query) print(\"🎩 Alfred's Response:\") print(response) # 4. 使用多个工具与尼古拉斯博士进行对话 query = \"I need to speak with Dr. Nikola Tesla about recent advancements in wireless energy. Can you help me prepare for this conversation?\" response = alfred.run(query)","date":"222224-24-120","objectID":"/agent-agentic-rag/:3:0","tags":["Agent","Agentic RAG"],"title":"Agentic RAG - Usecase","uri":"/agent-agentic-rag/"},{"categories":["Agent"],"content":"本文根据Hugging Face上的Agent课程编写而成。 在本章节您将学习如何使用 LangGraph 框架构建应用程序，该框架旨在帮助您构建和协调复杂的 LLM 工作流程。LangGraph 是一个框架，它通过为您提供代理流程的控制工具，允许您构建可用于生产的应用程序。 相关资源： LangGraph 代理 - LangGraph 代理示例 LangChain academy - Full course on LangGraph from LangChain 什么是LangGraph，什么时候使用它？ LangGraph 是 LangChain 开发的用于管理集成 LLM 的应用程序的控制流的框架。 **那么，LangGraph与LangChain有什么不同？**LangChain 提供了一个标准接口，用于与模型和其他组件交互，可用于检索、LLM 调用和工具调用。LangChain 中的类可以在 LangGraph 中使用，但并非必须使用。这些包是不同的，可以单独使用，但最终，您在网上找到的所有资源都会同时使用这两个包。 什么时候应该使用 LangGraph？ 当你需要做一个“控制”和“自由”之间的权衡： 控制：确保可预测行为并维护。 自由：让LLM有更多空间去发挥创造力。 例如：CodeAgent非常自由，可以在单个操作步骤中调用多个工具，创建自己的工具等等，但这种行为可能让它们比使用JSON的常规代理更难以预测和控制。 LangGraph则处于另一个极端，当您需要“控制”agent的执行时，就会发挥作用。它为您提供了构建遵循可预测流程的应用程序的工具，同时仍然充分利用 LLM 的强大功能。简而言之，如果您的应用程序涉及一系列需要以特定方式协调的步骤，并且在每个连接点做出决策， 那么 LangGraph 可以提供您所需的结构 。 LangGraph 擅长的关键场景包括： 需要明确控制流程的多步骤推理过程 需要在步骤之间保持状态的应用程序 将确定性逻辑与人工智能功能相结合的系统 需要人工干预的工作流程 具有多个组件协同工作的复杂代理架构 LangGraph的构建模块 LangGraph 中的应用程序从入口点开始，并且根据执行情况，流程可能会转到一个函数或另一个函数，直到到达结束。 ","date":"212111-11-100","objectID":"/agent-langgraph/:0:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"State State是 LangGraph 的核心概念。它代表了流经应用程序的所有信息。 from typing_extensions import TypeDict class State(TyprDict): graph_state: str 状态是用户定义的 ，因此字段应该精心设计以包含决策过程所需的所有数据！💡： 仔细考虑您的应用程序需要在步骤之间跟踪哪些信息。 ","date":"212111-11-100","objectID":"/agent-langgraph/:1:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"Node Node 时Python函数。每个Node： 将状态作为输入 执行操作 返回状态更新 def node_1(state): print(\"---Node 1----\") return {\"graph_state\": state['graph_state']+\"I am\"} def node_2(state): print(\"---Node 2---\") return {\"graph_state\": state['graph_state'] +\" happy!\"} def node_3(state): print(\"---Node 3---\") return {\"graph_state\": state['graph_state'] +\" sad!\"} Node可以包括什么呢？ LLM 调用 ：生成文本或做出决策 工具调用 ：与外部系统交互 条件逻辑 ：确定下一步 人工干预 ：获取用户输入 整个工作流程所需的一些Node（如 START 和 END）直接存在于 langGraph 中。 ","date":"212111-11-100","objectID":"/agent-langgraph/:2:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"Edge Edge连接Node并定义图中的可能路径： import random from typing import Literal # Literal 类型允许你明确规定变量的具体可选值，这些值可以是字符串、整数、布尔值等不可变类型。 def decide_mood(state) -\u003e Literal[\"node_2\", \"node_3\"]: # Often, we will use state to decide on the next node to visit user_input = state['graph_state'] # Here, let's just do a 50 / 50 split between nodes 2, 3 if random.random() \u003c 0.5: # 50% of the time, we return Node 2 return \"node_2\" # 50% of the time, we return Node 3 return \"node_3\" Edges可以是： 直接 ：始终从节点 A 到节点 B 条件 ：根据当前状态选择下一个节点 ","date":"212111-11-100","objectID":"/agent-langgraph/:3:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"StateGraph StateGraph 是保存整个代理工作流程的容器： from IPython.display import Image, display from langgraph.graph import StaeGraph, START, END # 创建状态图并添加节点 builder = StateGraph(State) builder.add_node(\"node_1\", node_1) builder.add_node(\"node_2\", node_2) builder.add_node(\"node_3\", node_3) # 定义节点之间的连接关系（边） builder.add_edge(START, \"node_1\") builder.add_conditional_edges(\"node_1\", decide_mood) builder.add_edge(\"node_2\", END) builder.add_edge(\"node_3\", END) graph = builder.compile() # View display(Image(graph.get_graph().draw_mermaid_png())) # 调用 graph.invoke({\"graph_state\": \"Hi, this is Lance.\"}) # 输出 #---Node 1--- #---Node 3--- #{'graph_state': 'Hi, this is Lance. I am sad!'} 构建一个邮件助手吧！ 在这一小节，我们会实现Alfred的电子邮件处理系统，他需要执行以下操作： 阅读收到的电子邮件 将其归类为垃圾邮件或合法邮件 起草对合法电子邮件的初步回复 在合法的情况下向韦恩先生发送信息（仅打印） 这是我们将构建的工作流程： ","date":"212111-11-100","objectID":"/agent-langgraph/:4:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"设置环境 pip install langgraph langchain_openai import os from typing import TypedDict, List, Dict, Any, Optional from langgraph.graph import StateGraph, START, END from langchain_openai import ChatOpenAI from langchain_core.messages import HumanMessage ","date":"212111-11-100","objectID":"/agent-langgraph/:5:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"Step 1: Define Our State 使您的State足够全面以跟踪所有重要信息，但避免添加不必要的细节。 class EmailState(TypedDict) # The email being processed email: Dict[str, Any] # Contains subject, sender, body, etc. # Category of the email (inquiry, complaint, etc.) email_category: Optional[str] # Reason why the email was marked as spam spam_reason: Optional[str] # Analysis and decisions is_spam: Optional[bool] # Response generation email_draft: Optional[str] # Processing metadata messages: List[DItc[str, Any]] ","date":"212111-11-100","objectID":"/agent-langgraph/:6:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"Step 2: Define Our Nodes 现在我们创建构成节点的处理函数，想一想我们需要什么？ 小助手要读邮件，返回logs: 小助手在处理来自发送者某某关于某某主题的邮件 小助手要判断是不是垃圾邮件，从LLM回答中提取is_spam，reason，category。 小助手处理垃圾邮件 小助手起草回复 小助手回复整个过程 # Initialize LLM model = ChatOpenAI(temparature=0) def read_email(state: EmailState): \"\"\"Alfred reads and logs the incoming email\"\"\" email = state[\"email\"] # Here we might do some initial preprocessing print(f\"Alfred is processing an email from {email['sender']} with subject: {email['subject']}\") # No state changes needed here return {} def classify_email(state: EmailState): \"\"\"Alfred uses an LLM to determine if the email is spam or legitimate\"\"\" email = state[\"email\"] # prepare our prompt for the LLM prompt = f\"\"\" As Alfred the butler, analyze this email and determine if it is spam or legitimate. Email: From: {email['sender']} Subject: {email['subject']} Body: {email['body']} First, determine if this email is spam. If it is spam, explain why. If it is legitimate, categorize it (inquiry, complaint, thank you, etc.). \"\"\" # call the LLM messages = [HumanMessage(content=prompt)] response = model.invoke(messages) # Simple logic to parse the response (in a real app, you'd want more robust parsing) response_text = response.content.lower() is_spam = \"spam\" in response_text and \"not spam\" not in response_text #Extract a reson if it's spam spam_reason = None if is_spam and \"reason\" in response_text: spam_reason = response_text.spilt(\"reason:\")[1].strip() # Determine category if legitime email_category = None if not is_spam: categories = [\"inquiry\", \"complaint\", \"thank you\", \"request\", \"information\"] for category in categories: if category in response_text: email_category = category break # Update messages for tracking new_messages = state.get(\"messages\", []) + [ {\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response.content} ] # Return state updates return { \"is_spam\": is_spam, \"spam_reason\": spam_reason, \"email_category\": email_category, \"messages\": new_messages } def handle_spam(state: EmailState): \"\"\"Alfred discards spam email with a note\"\"\" print(f\"Alfred has marked the email as spam. Reason: {state['spam_reason']}\") print(\"The email has been moved to the spam folder.\") # We're done processing this email return {} def draft_response(state: EmailState): \"\"\"Alfred drafts a preliminary response for legitimate emails\"\"\" email = state[\"email\"] category = state[\"email_category\"] or \"general\" # Prepare our prompt for the LLM prompt = f\"\"\" As Alfred the butler, draft a polite preliminary response to this email. Email: From: {email['sender']} Subject: {email['subject']} Body: {email['body']} This email has been categorized as: {category} Draft a brief, professional response that Mr. Hugg can review and personalize before sending. \"\"\" # Call the LLM messages = [HumanMessage(content=prompt)] response = model.invoke(messages) # Update messages for tracking new_messages = state.get(\"messages\", []) + [ {\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": response.content} ] # Return state updates return { \"email_draft\": response.content, \"messages\": new_messages } def notify_mr_hugg(state: EmailState): \"\"\"Alfred notifies Mr. Hugg about the email and presents the draft response\"\"\" email = state[\"email\"] print(\"\\n\" + \"=\"*50) print(f\"Sir, you've received an email from {email['sender']}.\") print(f\"Subject: {email['subject']}\") print(f\"Category: {state['email_category']}\") print(\"\\nI've prepared a draft response for your review:\") print(\"-\"*50) print(state[\"email_draft\"]) print(\"=\"*50 + \"\\n\") # We're done processing this email return {} ","date":"212111-11-100","objectID":"/agent-langgraph/:7:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"Step 3: Define Our Routing Logic 我们需要一个函数来确定分类后要采取哪条路径： def route_email(state: EmailState) -\u003e str: \"\"\"Determine the next step based on spam classification\"\"\" if state[\"is_spam\"]: return \"spam\" else: return \"legitimate\" ","date":"212111-11-100","objectID":"/agent-langgraph/:8:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"Step 4: Create the StateGraph and Define Edges # Create the graph email_graph = StateGraph(EmailState) # Add nodes email_graph.add_node(\"read_email\", read_email) email_graph.add_node(\"classify_email\", classify_email) email_graph.add_node(\"handle_spam\", handle_spam) email_graph.add_node(\"draft_response\", draft_response) email_graph.add_node(\"notify_mr_hugg\", notify_mr_hugg) # Start the edges email_graph.add_edge(START, \"read_email\") # Add edges - defining the flow email_graph.add_edge(\"read_email\", \"classify_email\") # Add conditional branching from classify_email email_graph.add_conditional_edges( \"classify_email\", route_email, { \"spam\": \"handle_spam\", \"legitimate\": \"draft_response\" } ) # Add the final edges email_graph.add_edge(\"handle_spam\", END) email_graph.add_edge(\"draft_response\", \"notify_mr_hugg\") email_graph.add_edge(\"notify_mr_hugg\", END) # Compile the graph compiled_graph = email_graph.compile() ","date":"212111-11-100","objectID":"/agent-langgraph/:9:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"Step 5: Run the Application 让我们用合法电子邮件和垃圾邮件来测试我们的图表： # Example legitimate email legitimate_email = { \"sender\": \"john.smith@example.com\", \"subject\": \"Question about your services\", \"body\": \"Dear Mr. Hugg, I was referred to you by a colleague and I'm interested in learning more about your consulting services. Could we schedule a call next week? Best regards, John Smith\" } # Example spam email spam_email = { \"sender\": \"winner@lottery-intl.com\", \"subject\": \"YOU HAVE WON $5,000,000!!!\", \"body\": \"CONGRATULATIONS! You have been selected as the winner of our international lottery! To claim your $5,000,000 prize, please send us your bank details and a processing fee of $100.\" } # Process the legitimate email print(\"\\nProcessing legitimate email...\") legitimate_result = compiled_graph.invoke({ \"email\": legitimate_email, \"is_spam\": None, \"spam_reason\": None, \"email_category\": None, \"email_draft\": None, \"messages\": [] }) # Process the spam email print(\"\\nProcessing spam email...\") spam_result = compiled_graph.invoke({ \"email\": spam_email, \"is_spam\": None, \"spam_reason\": None, \"email_category\": None, \"email_draft\": None, \"messages\": [] }) ","date":"212111-11-100","objectID":"/agent-langgraph/:10:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"Step 6：使用 Langfuse 📡 检查我们的邮件分拣代理 随着 Alfred 对邮件分拣代理进行微调，他越来越厌倦调试它的运行。代理本身就难以预测，也难以检查。但由于他的目标是构建终极垃圾邮件检测代理并将其部署到生产环境中，因此他需要强大的可追溯性，以便日后进行监控和分析。 首先，%pip install -q langfuse, 其次，我们 pip install Langchain （由于我们使用 LangFuse，因此需要 LangChain）：%pip install langchain。接下来，我们将 Langfuse API 密钥和主机地址添加为环境变量。您可以通过注册 Langfuse Cloud 或自行托管 Langfuse 来获取 Langfuse 凭据。 import os # Get keys for your project from the project settings page: https://cloud.langfuse.com os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\" os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\" os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # 🇪🇺 EU region # os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # 🇺🇸 US region 然后，我们配置 Langfuse callback_handler 并通过将 langfuse_callback 添加到图的调用来检测代理： config={“callbacks”: [langfuse_handler]} 。 from langfuse.callback import CallbackHandler # Initialize Langfuse CallbackHandler for LangGraph/Langchain (tracing) langfuse_handler = CallbackHandler() # Process legitimate email legitimate_result = compiled_graph.invoke( input={\"email\": legitimate_email, \"is_spam\": None, \"spam_reason\": None, \"email_category\": None, \"draft_response\": None, \"messages\": []}, config={\"callbacks\": [langfuse_handler]} ) 构建一个秘书agent吧！ 现在让我搭建一个助手Alfred，能够满足： 处理图像文档 使用视觉模型提取文本（视觉语言模型） 在需要时进行计算（演示常规工具） 分析内容并提供简明摘要 执行与文件相关的具体指令 秘书的工作流程遵循以下结构化模式： graph TD A(\"__start__\") --\u003e B[\"assistant\"] B -.-\u003e C[\"tools\"] C --\u003e B B -.-\u003e D(\"__end__\") ","date":"212111-11-100","objectID":"/agent-langgraph/:11:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"设置环境 import base64 from typing import List, TypedDict, Annotated, Optional from langchain_openai import ChatOpenAI from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage from langgraph.graph.message import add_messages from langgraph.graph import START, StateGraph from langgraph.prebuilt import ToolNode, tools_condition from IPython.display import Image, display AnyMessage 是来自 Langchain 的一个定义消息的类， add_messages 是一个添加最新消息而不是用最新状态覆盖它的操作符。这是 LangGraph 中的一个新概念，您可以在状态中添加操作符来定义它们之间的交互方式。 class AgentState(TypedDict): # The document provided input_file: Optional[str] # Contains file path (PDF/PNG) messages: Annotated[list[AnyMessage], add_messages]# 对于AnyMessage执行add_messages定义的操作 ","date":"212111-11-100","objectID":"/agent-langgraph/:12:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"准备工具 由于涉及到视觉，采用gpt-4o模型 定义工具：使用视觉模型提取图中的文字，计算工具 vision_llm = ChatOpenAI(model = \"gpt-4o\") def extract_text(img_path: str) -\u003e str: \"\"\" Extract text from an image file using a multimodal model. Master Wayne often leaves notes with his training regimen or meal plans. This allows me to properly analyze the contents. \"\"\" all_text = \"\" try: # Read image and encode as base64 with open(img_path, \"rb\") as image_file: image_bytes = image_file.read()#二进制数据 image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")# 二进制数据转换为文本格式 # Prepare the prompt including the base64 image data message = [ HumanMessage( content=[ { \"type\": \"text\", \"text\":( \"Extract all the text from this image. \" \"Return only the extracted text, no explanations.\" ), }, { \"type\": \"image_url\", \"image_url\":{ \"url\": f\"data:image/png;base64,{image_base64}\"# data:[\u003c媒体类型\u003e][;base64],\u003c数据\u003e }, }, ] ) ] # Call the vision-capable model response = vision_llm.invoke(message) # Append extracted text all_text += response.content + \"\\n\\n\" return all_text.strip() except Exception as e: # A butler should handle errors gracefully error_msg = f\"Error extracting text: {str(e)}\" print(error_msg) return def divide(a: int, b: int) -\u003e float: \"\"\"Divide a and b - for Master Wayne's occasional calculations.\"\"\" return a / b # Equip the butler with tools tools = [ divide, extract_text ] llm = ChatOpenAI(model=\"gpt-4o\") llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False) ","date":"212111-11-100","objectID":"/agent-langgraph/:13:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"Nodes 关于工具的文本描述 系统prompt 更新信息 def assistant(state: AgentState): # System message textual_description_of_tool=\"\"\" extract_text(img_path: str) -\u003e str: Extract text from an image file using a multimodal model. Args: img_path: A local image file path (strings). Returns: A single string containing the concatenated text extracted from each image. divide(a: int, b: int) -\u003e float: Divide a and b \"\"\" image=state[\"input_file\"] sys_msg = SystemMessage(content=f\"You are a helpful butler named Alfred that serves Mr. Wayne and Batman. You can analyse documents and run computations with provided tools:\\n{textual_description_of_tool} \\n You have access to some optional images. Currently the loaded image is: {image}\") return { \"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])], \"input_file\": state[\"input_file\"] } ","date":"212111-11-100","objectID":"/agent-langgraph/:14:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"The ReAct Pattern: How I Assist Mr. Wayne? 思考他的文件和请求 采用合适的工具采取行动 观察结果 根据需要重复，直到完全满足他的需要 同样的，我们需要构建graph，定义节点，添加edges # The graph builder = StartGraph(AgentState) # Define nodes: these do the work builder.add_node(\"assistant\", assistant) builder.add_node(\"tools\", ToolNode(tools)) # Define edges: these determine how the control flow moves builder.add_edge(START, \"assistant\") builder.add_conditional_edges( \"assistant\", # If the latest message requires a tool, route to tools # Otherwise, provide a direct response tools_condition, ) builder.add_edge(\"tools\", \"assistant\") react_graph = builder.compile() # Show the butler's thought process display(Image(react_graph.get_graph(xray=True).draw_mermaid_png())) 我们定义一个 tools 节点，其中包含工具列表。 assistant 节点只是绑定了工具的模型。我们创建一个包含 assistant 和 tools 节点的图。我们添加了一个 tools_condition 边，该边根据 assistant 是否调用 tools 路由到 End 或工具。 使用实例： 计算 messages = [HumanMessage(content=\"Divide 6790 by 5\")] messages = react_graph.invoke({\"messages\": messages, \"input_file\": None}) # Show the messages for m in messages['messages']: m.pretty_print() Human: Divide 6790 by 5 AI Tool Call: divide(a=6790, b=5) Tool Response: 1358.0 Alfred: The result of dividing 6790 by 5 is 1358.0. Analyzing Master Wayne’s Training Documents 当韦恩留下他的训练计划和​​用餐笔记时： messages = [HumanMessage(content=\"According to the note provided by Mr. Wayne in the provided images. What's the list of items I should buy for the dinner menu?\")] messages = react_graph.invoke({\"messages\": messages, \"input_file\": \"Batman_training_and_meals.png\"}) Human: According to the note provided by Mr. Wayne in the provided images. What's the list of items I should buy for the dinner menu? AI Tool Call: extract_text(img_path=\"Batman_training_and_meals.png\") Tool Response: [Extracted text with training schedule and menu details] Alfred: For the dinner menu, you should buy the following items: 1. Grass-fed local sirloin steak 2. Organic spinach 3. Piquillo peppers 4. Potatoes (for oven-baked golden herb potato) 5. Fish oil (2 grams) Ensure the steak is grass-fed and the spinach and peppers are organic for the best quality meal. ","date":"212111-11-100","objectID":"/agent-langgraph/:15:0","tags":["Agent","LangGraph"],"title":"LangGraph - Introduction","uri":"/agent-langgraph/"},{"categories":["Agent"],"content":"本文根据Hugging Face上的Agent课程编写而成。 什么是LlamaIndex？ LlamaIndex 是一个完整的工具包，用于使用索引和工作流创建基于LLM的Agent。 ","date":"191953-53-20","objectID":"/agent-llamaindex/:0:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"LlamaIndex的关键部分以及它们如何帮助代理？ Components：在 LlamaIndex 中使用的基本构建块。 These include things like prompts, models, and databases.组件通常用于将 LlamaIndex 与其他工具和库连接起来。 Tools: 工具是提供特定功能（例如搜索、计算或访问外部服务）的组件。 Agents：能够使用工具并做出决策的自主组件。它们协调工具的使用，以实现复杂的目标。 Workflows：是将逻辑整合在一起的逐步流程。工作流或代理工作流是一种无需明确使用代理即可构建代理行为的方法。 ","date":"191953-53-20","objectID":"/agent-llamaindex/:1:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"LlamaIndex的关键优势 清晰的工作流系统 ：工作流使用事件驱动和异步优先的语法，逐步分解代理的决策流程。这有助于您清晰地编写和组织逻辑。 使用 LlamaParse 进行高级文档解析 ：LlamaParse 是专为 LlamaIndex 制作的，因此集成是无缝的，尽管它是一项付费功能。 众多即用型组件 ：LlamaIndex 已经推出一段时间了，因此可以与许多其他框架兼容。这意味着它拥有许多经过测试且可靠的组件，例如 LLM、检索器、索引等等 LlamaHub ：是数百个此类组件、代理和工具的注册表，您可以在 LlamaIndex 中使用它们。 LlamaIndex的使用 ","date":"191953-53-20","objectID":"/agent-llamaindex/:2:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"LlamaHub 简介 LlamaHub 是一个包含数百个集成、代理和工具的注册表，您可以在 LlamaIndex 中使用它们。 那么应该怎么使用呢？ LlamaIndex 的安装说明在 LlamaHub 上提供了结构清晰的概述 。乍一看可能有点难以理解，但大多数安装命令通常都遵循一种易于记忆的格式 ： pip install llama-index-{component-type}-{framework-name} 让我们来尝试使用Hugging Face inference API integration安装 LLM 和嵌入组件的依赖项。 pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface 使用刚刚下载好的组件的示例： from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI import os from dotenv import load_dotenv # Load the .env file load_dotenv() # Retrieve HF_TOKEN from the environment variables hf_token = os.getenv(\"HF_TOKEN\") llm = HuggingFaceInferenceAPI( model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\", temperature=0.7, max_tokens=100, token=hf_token, ) response = llm.complete(\"Hello, how are you?\") print(response) # I am good, how can I help you today? ","date":"191953-53-20","objectID":"/agent-llamaindex/:3:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"QueryEngine 组件 QueryEngine 组件可以用作代理的检索增强生成 (RAG) 工具。 现在，想想 Alfred 是如何工作的： 你请阿尔弗雷德帮忙策划一场晚宴 Alfred 需要检查你的日历、饮食偏好和过去成功的菜单 QueryEngine 帮助 Alfred 找到这些信息并使用它来计划晚宴 现在，让我们更深入地了解组件，看看如何组合组件来创建 RAG 管道。 RAG包含五个关键步骤： Loading：指的是将数据从其所在位置（无论是文本文件、PDF、其他网站、数据库还是 API）加载到您的工作流程中。LlamaHub 提供数百种集成方案供您选择。 Indexing：这意味着创建一个允许查询数据的数据结构。对于LLM来说，这几乎总是意味着创建向量嵌入。向量嵌入是数据含义的数值表示。索引还可以指许多其他元数据策略，以便于根据属性准确地找到上下文相关的数据。 Storing：一旦您的数据被索引，您将需要存储您的索引以及其他元数据，以避免重新索引它。 Querying：对于任何给定的索引策略，您可以通过多种方式利用 LLM 和 LlamaIndex 数据结构进行查询，包括子查询、多步骤查询和混合策略。 Evaluation：任何流程中的关键步骤是检查其相对于其他策略的有效性，或检查何时进行更改。评估可以客观衡量您对查询的响应的准确性、可靠性和速度。 ","date":"191953-53-20","objectID":"/agent-llamaindex/:4:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Loading and embedding documents LlamaIndex 可以在您自己的数据上工作，但是， 在访问数据之前，我们需要加载它。 将数据加载到 LlamaIndex 主要有三种方法： SimpleDirectoryReader ：用于从本地目录加载各种文件类型的内置加载器。 from llama_index.core import SimpleDirectoryReader reader = SimpleDirectoryReader(input_dir = \"path/to/directory\") documents = reader.load_data() 这个多功能组件可以从文件夹中加载各种文件类型，并将它们转换为 LlamaIndex 可以使用的 Document 对象。 加载文档后，我们需要将它们分解成更小的部分，称为 Node 对象。Node 只是原始文档中的一段文本，方便AI处理，同时仍然保留对原始 Document 对象的引用。 IngestionPipeline 通过两个关键转换帮助我们创建这些节点。 SentenceSplitter 按照自然句子边界将文档拆分为可管理的块。 HuggingFaceEmbedding 将每个块转换为数字嵌入 - 以 AI 可以有效处理的方式捕捉语义含义的矢量表示。 from llama_index.core import Document from llama_index.embeddings.huggingface import HuggingFaceEmbedding from llama_index.core.node_parser import SentenceSplitter from llama_index.core.ingestion import IngestionPipeline # create the pipeline with transformations pipeline = IngestionPipeline( transformations = [ SentenceSplitter(chunk_overlap=0), HuggingFaceEmbedding(model_name = \"BAAI/bge-small-en-v1.5\"), ] ) nodes = await pipeline.arun(documents=[Document.example()]) LlamaParse ：LlamaParse，LlamaIndex 用于 PDF 解析的官方工具，可作为托管 API 使用。 LlamaHub ：数百个数据加载库的注册表，用于从任何来源提取数据。 ","date":"191953-53-20","objectID":"/agent-llamaindex/:4:1","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Storing and indexing documents 创建 Node 对象后，我们需要对它们进行索引以使它们可搜索，但在执行此操作之前，我们需要一个地方来存储我们的数据。 由于我们使用的是提取管道，因此可以直接将向量存储附加到管道来填充数据。在本例中，我们将使用 Chroma 来存储文档。 可以在 LlamaIndex 文档中找到不同向量存储的概述。 import chromadb from llama_index.vector_stores.chroma import ChromaVectorStore db = chromadb.PersistentClient(path=\"./alfred_chroma_db\") # 创建持久化客户端，将数据存储在本地文件系统（而非内存）。 chroma_collection = db.get_or_create_collection('alfred') # 获取或创建向量集合 vector_store = ChromaVectorStore(chroma_collection = chroma_collection) # 将 Chroma 集合包装为 LlamaIndex 兼容的向量存储接口。 pipeline = IngestionPipeline( transformations=[ SentenceSplitter(chunk_size = 25, chunk_overlap = 0), HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"), ], vector_store = vector_store, ) 通过将查询和节点嵌入 VectorStoreIndex 同一个向量空间中，我们可以找到相关的匹配项。 让我们看看如何从向量存储和嵌入中创建这个索引。 from llama_index.core import VectorStoreIndex from llama_index.embeddings.huggingface import HuggingFaceEmbedding embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\") index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model) 所有信息都会自动保存在 ChromaVectorStore 对象和传递的目录路径中。 ","date":"191953-53-20","objectID":"/agent-llamaindex/:4:2","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Querying a VectorStoreIndex with prompts and LLMs 在查询索引之前，我们需要将其转换为查询接口。最常见的转换选项是： as_retriever ：用于基本文档检索，返回具有相似度分数的 NodeWithScore 对象列表 as_query_engine ：对于单个问答交互，返回书面答复 from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") query_engine = index.as_query_engine( llm = llm, response_mode = \"tree_summarize\"#先从检索到的多个文档片段中生成局部摘要。再将这些摘要递归合并为最终答案，确保回答全面且连贯。 ) query_engine.query(\"What is the meaning of life?\") as_chat_engine ：对于在多条消息中保持记忆的对话交互，使用聊天历史记录和索引上下文返回书面回复。 ","date":"191953-53-20","objectID":"/agent-llamaindex/:4:3","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Response Processing 在底层，查询引擎不仅使用 LLM 来回答问题，还使用 ResponseSynthesizer 作为处理响应的策略。同样，这是完全可定制的，但主要有三种开箱即用的策略： refine ：通过按顺序遍历每个检索到的文本块来创建并优化答案。这会为每个节点/检索到的文本块单独调用一次 LLM。 compact （默认）：类似于细化，但事先连接块，从而减少 LLM 调用。 tree_summarize ：通过遍历每个检索到的文本块并创建答案的树形结构来创建详细的答案。 ","date":"191953-53-20","objectID":"/agent-llamaindex/:4:4","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Evaluation and observability LlamaIndex 提供内置评估工具来评估响应质量。 这些评估人员利用 LLM 来分析不同维度的响应。 让我们看一下可用的三个主要评估器： FaithfulnessEvaluator ：通过检查答案是否得到上下文支持来评估答案的真实性。 AnswerRelevancyEvaluator ：通过检查答案是否与问题相关来评估答案的相关性。 CorrectnessEvaluator ：通过检查答案是否正确来评估答案的正确性。 from llama_index.core.evaluation import FaithfulnessEvaluator query_engine = # from the previous section llm = # from the previous section # query index evaluator = FaithfulnessEvaluator(llm=llm) response = query_engine.query( \"What battles took place in New York City in the American Revolution?\" ) eval_result = evaluator.evaluate_response(response=response) eval_result.passing 评估流程： 分解回答：将回答拆分为多个独立的陈述（如 “Long Island 战役发生在纽约市” 和 “Fort Washington 战役发生在纽约市”）。 检查依据：针对每个陈述，验证是否存在于检索到的上下文中。 生成评估结果： passing：布尔值，表示回答是否完全忠实于上下文。 score：分数（0-1），表示忠实程度。 feedback：详细反馈，指出不忠实的陈述及原因。 安装 LlamaTrace 正如 LlamaHub 部分介绍的那样，我们可以使用以下命令从 Arize Phoenix 安装 LlamaTrace 回调： pip install -U llama-index-callbacks-arize-phoenix import llama_index import os PHOENIX_API_KEY = \"\u003cPHOENIX_API_KEY\u003e\" os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\" llama_index.core.set_global_handler( \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\" ) ","date":"191953-53-20","objectID":"/agent-llamaindex/:4:5","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"使用LlamaIndex中的工具 LlamaIndex 中有四种主要类型的工具 ： FunctionTool ：将任何 Python 函数转换为代理可以使用的工具。它会自动理解函数的工作原理。 QueryEngineTool ：允许代理使用查询引擎的工具。由于代理构建于查询引擎之上，因此它们也可以使用其他代理作为工具。 Toolspecs ：社区创建的工具集，通常包括用于特定服务（如 Gmail）的工具。 Utility Tools ：帮助处理来自其他工具的大量数据的特殊工具。 ","date":"191953-53-20","objectID":"/agent-llamaindex/:5:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Creating a FunctionTool FunctionTool 提供了一种简单的方法来包装任何 Python 函数并将其提供给代理。您可以将同步或异步函数以及可选的 name 和 description 参数传递给该工具。 from llama_index.core.tools import FunctionTool def get_weather(location: str) -\u003e str: \"\"\"Useful for getting the weather for a given location.\"\"\" print(f\"Getting weather for {location}\") return f\"The weather in {location} is sunny\" tool = FuntionTool.from_defaults( get_weather, name = \"my_weather_tool\", description=\"Useful for getting the weather for a given location.\", ) tool.call(\"New York\") ","date":"191953-53-20","objectID":"/agent-llamaindex/:5:1","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Creating a QueryEngineTool 使用 QueryEngineTool 类，我们可以轻松地将上一单元中定义的 QueryEngine 转换为工具。让我们在下面的示例中看看如何从 QueryEngine 创建 QueryEngineTool 。 from llama_index.core import VectorStoreIndex from llama_index.core.tools import QueryEngineTool from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI from llama_index.embeddings.huggingface import HuggingFaceEmbedding from llama_index.vector_stores.chroma import ChromaVectorStore embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\") db = chromadb.PersistentClient(path=\"./alfred_chroma_db\") chroma_collection = db.get_or_create_collection(\"alfred\") vector_store = ChromaVectorStore(chroma_collection=chroma_collection) index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model) llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") query_engine = index.as_query_engine(llm=llm) tool = QueryEngineTool.from_defaults(query_engine, name=\"some useful name\", description=\"some useful description\") ","date":"191953-53-20","objectID":"/agent-llamaindex/:5:2","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Creating Toolspecs 可以将 ToolSpecs 视为协同工作的工具集合，就像一个井然有序的专业工具包。正如机械师的工具包包含用于车辆维修的互补工具一样， ToolSpec 可以将相关工具组合起来用于特定用途。例如，会计代理的 ToolSpec 可以巧妙地集成电子表格功能、电子邮件功能和计算工具，从而精准高效地处理财务任务。 pip install llama-index-tools-google 加载工具规范并将其转换为工具列表。 from llama_index.tools.google import GmailToolSpec tool_spec = GmailToolSpec() tool_spec_list = tool_spec.to_tool_list() [(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]# 查看每个工具的 metadata ","date":"191953-53-20","objectID":"/agent-llamaindex/:5:3","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Model Context Protocol (MCP) in LlamaIndex LlamaIndex 还允许通过 LlamaHub 上的 ToolSpec 使用 MCP 工具。您可以简单地运行一个 MCP 服务器，并通过以下实现开始使用它。 pip install llama-index-tools-mcp from llama_index.tools.mcp import BasicMCPClient, McpToolSpec from llama_index.tools.mcp import BasicMCPClient, McpToolSpec # We consider there is a mcp server running on 127.0.0.1:8000, or you can use the mcp client to connect to your own mcp server. mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\")#建立与 MCP 服务器的连接 mcp_tool = McpToolSpec(client=mcp_client)#将 MCP 客户端包装为 LlamaIndex 工具 # get the agent agent = await get_agent(mcp_tool) # create the agent context agent_context = Context(agent)#创建代理上下文 ","date":"191953-53-20","objectID":"/agent-llamaindex/:5:4","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Utility Tools 通常，直接查询 API 可能会返回过多的数据 ，其中一些数据可能不相关，溢出 LLM 的上下文窗口，或者不必要地增加您正在使用的令牌数量。下面让我们来介绍一下我们的两个主要实用工具。You can find toolspecs and utility tools on the LlamaHub. OnDemandToolLoader ：此工具可将任何现有的 LlamaIndex 数据加载器（BaseReader 类）转换为代理可以使用的工具。调用此工具时，可以使用触发数据加载器 load_data 所需的所有参数以及自然语言查询字符串。在执行过程中，我们首先从数据加载器加载数据，对其进行索引（例如使用向量存储），然后“按需”查询。所有这三个步骤都可在一次工具调用中完成。 LoadAndSearchToolSpec ：LoadAndSearchToolSpec 接受任何现有工具作为输入。作为工具规范，它实现了 to_tool_list ，当调用该函数时，会返回两个工具：一个加载工具和一个搜索工具。加载工具的执行会调用底层工具，然后对输出进行索引（默认使用向量索引）。搜索工具的执行会接受查询字符串作为输入，并调用底层索引。 ","date":"191953-53-20","objectID":"/agent-llamaindex/:5:5","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"在LlamaIndex中使用Agent LlamaIndex 支持三种主要类型的推理代理： Function Calling Agents ——它们与可以调用特定函数的 AI 模型一起工作。 ReAct Agents - 它们可以与任何进行聊天或文本端点的 AI 一起工作并处理复杂的推理任务。 Advanced Custom Agents - 这些代理使用更复杂的方法来处理更复杂的任务和工作流程。 ","date":"191953-53-20","objectID":"/agent-llamaindex/:6:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"初始化Agents 要创建代理，我们首先要为其提供一组定义其功能的函数/工具 。 from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI from llama_index.core.agent.workflow import AgentWorkflow from llama_index.core.tools import FunctionTool # define sample Tool -- type annotations, function names, and docstrings, are all included in parsed schemas! def multiply(a: int, b: int) -\u003e int: \"\"\"Multiplies two integers and returns the resulting integer\"\"\" return a * b # initialize llm llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") # initialize agent agent = AgentWorkflow.from_tools_or_functions(#将工具（Tools）或函数（Functions）注册到代理中 [FunctionTool.from_defaults(multiply)], llm=llm ) 代理默认是无状态的 ，使用 Context 对象可以选择记住过去的交互，如果您想使用需要记住以前交互的代理，这可能会很有用，例如在多个消息中维护上下文的聊天机器人或需要跟踪进度的任务管理器。很棒的异步指南 。 # stateless response = await agent.run(\"What is 2 times 2?\") # remembering state from llama_index.core.workflow import Context ctx = Context(agent) response = await agent.run(\"My name is Bob.\", ctx=ctx)#将上下文对象传递给每次调用，保持状态连续性。 response = await agent.run(\"What was my name again?\", ctx=ctx) ","date":"191953-53-20","objectID":"/agent-llamaindex/:6:1","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"使用 QueryEngineTools 创建 RAG 代理 Agentic RAG 是一种强大的工具，它能够利用代理来解答数据相关问题。 我们可以将各种工具传递给 Alfred，帮助他解答问题。不过，Alfred 可以选择使用任何其他工具或流程来解答问题，而不是自动在文档上进行解答。 将 QueryEngine 包装为代理工具很容易。包装时，我们需要定义名称和描述 。LLM 将使用这些信息来正确使用该工具。让我们看看如何使用我们在组件部分创建的 QueryEngine 加载 QueryEngineTool 。 from llama_index.core.tools import QueryEngineTool query_engine = index.as_query_engine(llm=llm, similarity_top_k=3) # as shown in the Components in LlamaIndex section query_engine_tool = QueryEngineTool.from_defaults( query_engine=query_engine, name=\"name\", description=\"a specific description\", return_direct=False, ) query_engine_agent = AgentWorkflow.from_tools_or_functions( [query_engine_tool], llm=llm, system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions. \" ) ","date":"191953-53-20","objectID":"/agent-llamaindex/:6:2","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Creating Multi-agent systems AgentWorkflow 类还直接支持多代理系统。通过为每个代理赋予名称和描述，系统可以维护单个活跃的发言者，并且每个代理都可以将发言权移交给另一个代理。 LlamaIndex 中的代理也可以直接用作其他代理的工具 ，用于更复杂和自定义的场景。 from llama_index.core.agent.workflow import ( AgentWorkflow, FunctionAgent, ReActAgent, ) # Define some tools def add(a: int, b: int) -\u003e int: \"\"\"Add two numbers.\"\"\" return a + b def subtract(a: int, b: int) -\u003e int: \"\"\"Subtract two numbers.\"\"\" return a - b # Create agent configs # NOTE: we can use FunctionAgent or ReActAgent here. # FunctionAgent works for LLMs with a function calling API. # ReActAgent works for any LLM. calculator_agent = ReActAgebt( name = 'calculator', description=\"Performs basic arithmetic operations\", system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\", tools = [add, substract], llm = llm, ) query_agent = ReActAgent( name=\"info_lookup\", description=\"Looks up information about XYZ\", system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\", tools=[query_engine_tool], llm=llm ) agent = AgentWorkflow( agents=[calculator_agent, query_agent], root_agent=\"calculator\" ) # Run the system response = await agent.run(user_msg=\"Can you add 5 and 3?\") 在 LlamaIndex 中创建代理工作流 LlamaIndex 中的工作流提供了一种结构化的方法，将您的代码组织成连续且可管理的步骤。 这样的工作流是通过定义由 Events 触发的 Steps 来创建的，这些步骤本身会发出 Events 来触发后续步骤。让我们来看看 Alfred 如何演示 RAG 任务的 LlamaIndex 工作流。 工作流程在代理的自主性与保持对整个工作流程的控制之间取得了很好的平衡。 ","date":"191953-53-20","objectID":"/agent-llamaindex/:6:3","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"创建工作流 首先安装需要的包 pip install llama-index-utils-workflow 我们可以通过定义一个继承自 Workflow 的类，并用 @step 装饰函数来创建单步工作流。我们还需要添加 StartEvent 和 StopEvent ，它们是用于指示工作流开始和结束的特殊事件。 from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step class MyWorkflow(Workflow): @step async def my_step(self, ev: StartEvent) -\u003e StopEvent: # do something here return StopEvent(result=\"Hello, world!\") w = MyWorkflow(timeout=10, verbose=False) result = await w.run() ","date":"191953-53-20","objectID":"/agent-llamaindex/:7:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"连接多个步骤 为了连接多个步骤，我们创建在步骤之间传递数据的自定义事件。 为此，我们需要添加一个在步骤之间传递的 Event ，并将第一步的输出传输到第二步。 from llama_index.core.workflow import Event class ProcessingEvent(Event) intermediate_result: str class MultiStepWorkflow(Workflow): @step async def step_one(self, ev: StartEvent) -\u003e ProcessingEvent # Process initial data return ProcessingEvent(intermediate_result='Step 1 complete') @step async def step_two(self, ev: ProcessingEvent) -\u003e StopEvent: # Use the intermediate result final_result = f\"Finished processing: {ev.intermediate_result}\" return StopEvent(result=final_result) w = MultiStepWorkflow(timeout=10, verbose=False) result = await w.run() result ","date":"191953-53-20","objectID":"/agent-llamaindex/:8:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"循环和分支 类型提示是工作流中最强大的部分，因为它允许我们创建分支、循环和连接以促进更复杂的工作流。 让我们展示一个使用联合运算符 | 创建循环的示例。在下面的示例中，我们看到 LoopEvent 被作为步骤的输入，也可以作为输出返回。 step_one 输入： 首次执行：接收StartEvent（工作流启动）。 循环执行：接收LoopEvent（携带重试信息）。 输出： 50% 概率返回LoopEvent，触发循环。 50% 概率返回ProcessingEvent，进入下一步。 from llama_index.core.workflow import Event import random class ProcessingEvent(Event): intermediate_result: str class LoopEvent(Event): loop_output: str class MultiStepWorkflow(Workflow): @step async def step_one(self, ev: StartEvent | LoopEvent) -\u003e ProcessingEvent | LoopEvent: if random.randint(0, 1) == 0: print(\"Bad thing happened\") return LoopEvent(loop_output=\"Back to step one.\") else: print(\"Good thing happened\") return ProcessingEvent(intermediate_result=\"First step complete.\") @step async def step_two(self, ev: ProcessingEvent) -\u003e StopEvent: # Use the intermediate result final_result = f\"Finished processing: {ev.intermediate_result}\" return StopEvent(result=final_result) w = MultiStepWorkflow(verbose=False) result = await w.run() result ","date":"191953-53-20","objectID":"/agent-llamaindex/:9:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Drawing Workflows 我们还可以绘制工作流程。让我们使用 draw_all_possible_flows 函数来绘制工作流程。该函数将工作流程存储在 HTML 文件中。 from llama_index.utils.workflow import draw_all_possible_flows w = ... # as defined in the previous section draw_all_possible_flows(w, \"flow.html\") ","date":"191953-53-20","objectID":"/agent-llamaindex/:10:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"状态管理 当您想要跟踪工作流的状态，以便每个步骤都能访问相同的状态时，状态管理非常有用。我们可以在步骤函数的参数上使用 Context 类型提示来实现这一点。 from llama_index.core.workflow import Context, StartEvent, StopEvent @step async def query(self, ctx: Context, ev: StartEvent) -\u003e StopEvent: #存储查询到上下文 await ctx.set(\"query\", \"What is the capital of France?\") # do something with context and event val = ... # 从上下文检索查询 query = await ctx.get(\"query\") return StopEvent(result=val) ","date":"191953-53-20","objectID":"/agent-llamaindex/:11:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"Automating workflows with Multi-Agent Workflows 我们可以使用 AgentWorkflow 类来创建多代理工作流，而无需手动创建工作流 。 AgentWorkflow 使用工作 AgentWorkflow 代理，允许您创建一个由一个或多个代理组成的系统，这些代理可以根据各自的特定功能进行协作并相互交接任务。这使我们能够构建复杂的代理系统，其中不同的代理负责处理任务的不同方面。我们不会从 llama_index.core.agent 导入类，而是从 llama_index.core.agent.workflow 导入代理类。必须在 AgentWorkflow 构造函数中指定一个代理作为根代理。当用户消息传入时，它会首先路由到根代理。 然后每个代理可以： 使用他们的工具直接处理请求 移交给另一个更适合该任务的代理 向用户返回响应 from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI # Define some tools def add(a: int, b: int) -\u003e int: \"\"\"Add two numbers.\"\"\" return a + b def multiply(a: int, b: int) -\u003e int: \"\"\"Multiply two numbers.\"\"\" return a * b llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\") # we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description multiply_agent = ReActAgent( name=\"multiply_agent\", description=\"Is able to multiply two integers\", system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\", tools=[multiply], llm=llm, ) addition_agent = ReActAgent( name=\"add_agent\", description=\"Is able to add two integers\", system_prompt=\"A helpful assistant that can use a tool to add numbers.\", tools=[add], llm=llm, ) # Create the workflow workflow = AgentWorkflow( agents=[multiply_agent, addition_agent], root_agent=\"multiply_agent\", ) # Run the system response = await workflow.run(user_msg=\"Can you add 5 and 3?\") 代理工具还可以修改我们之前提到的工作流状态。在启动工作流之前，我们可以提供一个初始状态字典，供所有代理使用。该状态存储在工作流上下文的 state 键中。它将被注入到 state_prompt 中，用于增强每条新的用户消息。Let’s inject a counter to count function calls by modifying the previous example: from llama_index.core.workflow import Context # Define some tools async def add(ctx: Context, a: int, b: int) -\u003e int: \"\"\"Add two numbers.\"\"\" # update our count cur_state = await ctx.get(\"state\") cur_state[\"num_fn_calls\"] += 1 await ctx.set(\"state\", cur_state) return a + b async def multiply(ctx: Context, a: int, b: int) -\u003e int: \"\"\"Multiply two numbers.\"\"\" # update our count cur_state = await ctx.get(\"state\") cur_state[\"num_fn_calls\"] += 1 await ctx.set(\"state\", cur_state) return a * b ... workflow = AgentWorkflow( agents=[multiply_agent, addition_agent], root_agent=\"multiply_agent\" initial_state={\"num_fn_calls\": 0}, state_prompt=\"Current state: {state}. User message: {msg}\", ) # run the workflow with context ctx = Context(workflow) response = await workflow.run(user_msg=\"Can you add 5 and 3?\", ctx=ctx) # pull out and inspect the state state = await ctx.get(\"state\") print(state[\"num_fn_calls\"]) ","date":"191953-53-20","objectID":"/agent-llamaindex/:12:0","tags":["Agent","LlamaIndex"],"title":"LlamaIndex - Introduction","uri":"/agent-llamaindex/"},{"categories":["Agent"],"content":"本文根据Hugging Face上的Agent课程编写而成。 相关资源： smolagents Documentation - Official docs for the smolagents library Building Effective Agents - Research paper on agent architectures Agent Guidelines - Best practices for building reliable agents LangGraph Agents - Additional examples of agent implementations Function Calling Guide - Understanding function calling in LLMs RAG Best Practices - Guide to implementing effective RAG smolagents Blog - Introduction to smolagents and code interactions 什么是smolagents，为什么要使用smolagents？ smolagents 是一个 Hugging Face 库。smolagents 是一个简单但功能强大的 AI 代理构建框架。它为LLM提供了与现实世界交互的能力，例如搜索或生成图像。 ","date":"171719-19-30","objectID":"/agent-smolagents/:0:0","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Agent"],"content":"主要优势 简单性： 最小的代码复杂性和抽象性，使框架易于理解、采用和扩展。 灵活的 LLM 支持： 通过与 Hugging Face 工具和外部 API 集成，可与任何 LLM 配合使用 代码优先方法： 对代码代理提供一流的支持，这些代理直接在代码中编写其操作，无需解析并简化工具调用 HF Hub 集成： 与 Hugging Face Hub 无缝集成，允许使用 Gradio Spaces 作为工具 ","date":"171719-19-30","objectID":"/agent-smolagents/:1:0","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Agent"],"content":"何时适合使用smolagents? 您需要一个轻量级且最小的解决方案。 您希望快速进行实验而无需进行复杂的配置。 您的应用程序逻辑很简单。 以上的情况适合使用smolagents。 ","date":"171719-19-30","objectID":"/agent-smolagents/:2:0","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Agent"],"content":"smolagents中的模型集成 smolagents 支持灵活的 LLM 集成，允许您使用任何符合特定条件的可调用模型。该框架提供了几个预定义的类来简化模型连接： TransformersModel： 实现本地 transformers 管道，实现无缝集成。 from smolagents import TransformersModel model = TransformersModel(model_id=\"HuggingFaceTB/SmolLM-135M-Instruct\") print(model([{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Ok!\"}]}], stop_sequences=[\"great\"])) InferenceClientModel ： 支持通过 Hugging Face 的基础设施或通过越来越多的第三方推理提供商进行无服务器推理调用。 HfApiModel 封装了 huggingface_hub 的 InferenceClient ，用于执行 LLM。它支持 Hub 上所有可用的推理提供程序 ：Cerebras、Cohere、Fal、Fireworks、HF-Inference、Hyperbolic、Nebius、Novita、Replicate、SambaNova、Together 等。 from smolagents import InferenceClientModel message = [ {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, how are you?\"}]} ] model = InferenceClientModel(provider = \"novita\") print(model(messages)) LiteLLMModel ： 利用 LiteLLM 实现轻量级模型交互。 from solagents import LiteLLMModel messages = [ {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, how are you?\"}]} ] model = LiteLLMModel(model_id=\"anthropic/claude-3-5-sonnet-latest\", temperature=0.2, max_tokens=10) print(model(messages)) OpenAIServerModel ： 连接到任何提供 OpenAI API 接口的服务。 import os from smolagents import OpenAIServerModel model = OpenAIServerModel( model_id = \"gpt-4o\", api_base = \"https://api.openai.com/v1\", api_key = os.environ[\"OPENAI_API_KEY\"], ) AzureOpenAIServerModel ： 支持与任何 Azure OpenAI 部署集成。 import os from smolagents import AzureOpenAIServerModel model = AzureOpenAIServerModel( model_id = os.environ.get(\"AZURE_OPENAI_MODEL\"), azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"), api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"), api_version=os.environ.get(\"OPENAI_API_VERSION\") ) Agent类型 ","date":"171719-19-30","objectID":"/agent-smolagents/:3:0","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Agent"],"content":"CodeAgents 使用代码而不是 JSON 编写操作有几个主要优势： 可组合性 ：轻松组合和重用操作 对象管理 ：直接处理图像等复杂结构 通用性 ：表达任何计算上可能的任务 这对 LLM 来说很自然 ：LLM 训练数据中已经存在高质量的代码。 它是核心构建块。CodeAgent 是一种特殊的 MultiStepAgent ， CodeAgent 将在下面的示例中看到。 ","date":"171719-19-30","objectID":"/agent-smolagents/:4:0","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Agent"],"content":"CodeAgent逻辑流程 CodeAgent 通过一系列步骤执行操作，将现有变量和知识纳入代理的上下文中，并保存在执行日志中： 系统提示存储在 SystemPromptStep 中，用户查询记录在 TaskStep 中。 然后，执行以下 while 循环： agent.write_memory_to_messages() 将代理的日志写入 LLM 可读的chat messages中。 这些消息被发送到一个 Model ，该模型生成一个完成信息。 解析完成以提取操作，在我们的例子中，它应该是一个代码片段，因为我们正在使用 CodeAgent 。 动作执行。 将结果记录到 ActionStep 的内存中。 在每个步骤结束时，如果agent包含任何函数调用（在 agent.step_callback 中），则会执行它们。 ","date":"171719-19-30","objectID":"/agent-smolagents/:4:1","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Agent"],"content":"实践时间！ 以下我会展示两个示例，一个是huggingface 官方课程的例子，一个是自己设计的旅游助手。 派对管家 Alfred要为Wayen家族筹办一场派对。需要做到以下几点： 选择派对上的音乐 为访客整理菜单 计算准备时间 在社区共享 使用 OpenTelemetry 和 Langfuse 📡 检查我们的派对管家 旅游助手 ","date":"171719-19-30","objectID":"/agent-smolagents/:4:2","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Agent"],"content":"ToolCallingAgents ","date":"171719-19-30","objectID":"/agent-smolagents/:4:3","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Agent"],"content":"Tools ","date":"171719-19-30","objectID":"/agent-smolagents/:4:4","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Agent"],"content":"让我们来实现一些Agents吧 ","date":"171719-19-30","objectID":"/agent-smolagents/:5:0","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Agent"],"content":"Retrieval Agents ","date":"171719-19-30","objectID":"/agent-smolagents/:5:1","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Agent"],"content":"Multi-Agent System ","date":"171719-19-30","objectID":"/agent-smolagents/:5:2","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Agent"],"content":"Vision and Browser agents ","date":"171719-19-30","objectID":"/agent-smolagents/:5:3","tags":["Agent","Smolagents"],"title":"Smolagents - Introduction","uri":"/agent-smolagents/"},{"categories":["Application"],"content":"在当今飞速发展的技术浪潮中，人工智能（AI）正以前所未有的速度渗透到各个领域，软件开发亦不例外。从智能代码补全到自动化测试，AI 工具的出现极大地提升了开发效率，并逐渐改变了开发者们的工作方式。特别是在前端开发领域，AI 的应用潜力更是巨大，它有望简化繁琐的UI构建过程，让开发者能够更专注于业务逻辑和用户体验。 正是在这样的背景下，Vercel 推出了 v0.dev，一个开创性的 AI 驱动的用户界面生成系统。v0.dev 的核心理念在于，通过简单的文本描述，即可快速生成高质量、可复用的前端 UI 代码，极大地加速了原型设计和开发迭代的进程。 本文旨在分享我对 v0.dev 的实际体验和深入探索。我将结合自己创建前端项目的经历，详细阐述 v0.dev 如何助力我从零开始构建项目，并展示其在实际应用中的强大能力和独特魅力。 ","date":"313144-44-30","objectID":"/application-v0/:0:0","tags":["Application","v0"],"title":"v0.dev：AI 驱动前端开发的探索与实践","uri":"/application-v0/"},{"categories":["Application"],"content":"深入了解 v0.dev v0.dev 是由知名前端工具和平台提供商 Vercel 推出的一款创新性 AI 产品。它不仅仅是一个代码生成器，更是一个能够将自然语言描述转化为实际用户界面的智能系统。v0.dev 的核心在于其强大的 AI 模型，能够理解开发者的意图，并快速生成基于 React、shadcn/ui 和 Tailwind CSS 的高质量、可复用前端代码。这意味着开发者可以通过简单的文本提示，例如\"创建一个带有搜索框和筛选功能的电商产品列表\"，v0.dev 就能立即呈现出相应的 UI 结构和样式。Community里面也有很多优秀作品，包括：APPs, Games, Sites, Components, Blocks, Starters。下图是Crypto Dashboard Site示例。 v0.dev 的工作流程直观且高效。用户只需在输入框中键入对所需 UI 的描述，AI 模型便会迅速处理这些信息。一旦选择了合适的 UI，用户不仅可以直接复制其生成的代码，还可以对 UI 的特定部分进行进一步的精细调整。这种迭代和优化的能力，让 v0.dev 不仅能够快速生成初始代码，还能确保最终产出符合高质量标准。 v0.dev 的出现为前端开发带来了多重显著优势： 首先，它极大地提高了开发效率。传统的前端开发流程中，UI 的构建往往耗时耗力，需要开发者手动编写大量的 HTML、CSS 和 JavaScript 代码。而 v0.dev 通过自动化生成，将这一过程缩短至几秒钟，使得开发者可以将更多精力投入到核心业务逻辑的实现上。 其次，v0.dev 降低了前端开发的门槛。即使是不具备深厚前端背景的设计师或后端开发者，也能通过简单的文本描述快速创建出功能完备的 UI，从而加速了团队协作和项目推进。 最后，v0.dev 是一个快速原型设计的利器。在项目初期，产品经理和设计师可以利用 v0.dev 迅速生成多种 UI 方案，进行快速验证和迭代，有效缩短了产品从概念到落地的周期。 尽管 v0.dev 带来了诸多便利，但在其 Beta 阶段，也存在一些值得注意的局限性。目前，v0.dev 主要专注于前端 UI 代码的生成，对于更复杂的业务逻辑、数据绑定或后端交互等功能，仍需要开发者手动完成。然而，Vercel 已经明确表示，未来将逐步增加对其他 UI 库（如 Svelte, Vue 或纯 HTML）的支持，并探索集成数据获取代码生成的能力。随着 v0.dev 的不断发展和完善，我们有理由相信它将在前端开发领域扮演越来越重要的角色。 ","date":"313144-44-30","objectID":"/application-v0/:1:0","tags":["Application","v0"],"title":"v0.dev：AI 驱动前端开发的探索与实践","uri":"/application-v0/"},{"categories":["Application"],"content":"我的前端项目创建经历 项目背景与需求：我需要做一个网站展示市面上大模型的某种能力的对比结果，风格活泼自然。尽可能用多种不同的方式呈现结果，并且支持选择对比功能。 如何输入提示词: v0支持图片理解，在素材网上找两张你喜欢的风格以及示例网站参考图，描述你想要的网站功能和布局。这里提供两个网站：promptcoder，可以将心仪的网站截图转化为专业的prompt；https://dribbble.com, 可以找到很多好看的网站原型，然后截图给v0，让它分析相关的风格和组件，如下图。 如何生成UI: 输入提示词后，点击生成按钮，等待生成完成。 v0.dev 生成的 UI 示例 不断调整风格、添加组件内容直至你满意为止。 部署到vercel 项目成果展示 ","date":"313144-44-30","objectID":"/application-v0/:2:0","tags":["Application","v0"],"title":"v0.dev：AI 驱动前端开发的探索与实践","uri":"/application-v0/"},{"categories":[],"content":"听了Tako的推荐，决定读一读这本书。果然带给我极大的收获。可以这么说，这本书一定程度上修正了我的世界观和价值观，重塑了我对待事物的态度。花了五个半小时读完中了中译版，我想再过一段时间经历更多的事情之后，我会再读一遍英文原版，看看是否会有观念的变化。 ","date":"313140-40-110","objectID":"/reading-factfulness/:0:0","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"故事的开始 故事的一开始，让我们从十三个关于事实的问题开始（注意到这本书是2019年出版的，这是基于2019年的事实，但是对结果的影响很小），写下你的答案： 在全世界所有的低收入国家里面，有多少百分比的女孩能够上完小学？ □ A.20% □ B.40% □ C.60% 全世界最多的人口生活在什么样的国家？ □ A．低收入国家 □ B．中等收入国家 □ C．高收入国家 在过去的20年里，全世界生活在极度贫困状态下的人口是如何变化的？ □ A．几乎翻倍 □ B．保持不变 □ C．几乎减半 全世界人口的预期寿命现在是多少岁？ □ A.50岁 □ B.60岁 □ C.70岁 今天全世界有20亿儿童，他们的年龄从0到15岁，那么根据联合国的预测，到2100年，全世界会有多少儿童？ □ A.40亿 □ B.30亿 □ C.20亿 联合国预测，到2100年，世界人口将增加40亿，那么请问主要原因是什么？ □ A．将会有更多的儿童（15岁以下） □ B．将会有更多的成年人（15到74岁） □ C．将会有更多的老年人（75岁以上） 在过去的100年间，死于自然灾害的人数是如何变化的？ □ A．几乎翻倍 □ B．保持不变 □ C．几乎减半 当今世界上的人口数量接近70亿，下面哪张地图最佳地表示了人口的分布情况？每一个人形图案代表了10亿人。 现在全世界有多少一岁儿童接种过疫苗？ □ A.20% □ B.50% □ C.80% 在全世界范围内，30岁的男人平均接受教育的时间超过10年。请问30岁的女性，平均在学校接受教育的时间是多少年？ □ A.9年 □ B.6年 □ C.3年 在1996年，老虎、大熊猫和黑犀牛被列为濒危动物，那么请问到今天这三种动物中的哪些还是濒危动物？ □ A．全部都是 □ B．其中的一种 □ C．全部都不是 全世界有多少人能够使用电？ □ A.20% □ B.50% □ C.80% 全球气候专家预测，在接下来的100年中，全球的平均温度将______________。 □ A．升高 □ B．保持不变 □ C．降低 在公布正确答案之前，我翻出当时的答案：A A A C C C C C B C B C A， 38%的正确率。读完这本书之后，再回顾这些回答，感觉当时的我像珊瑚礁在潜意识里缓慢钙化，我以为自己在海水中自由游弋，却早已被亿万枚贝壳包裹成孤岛；像被倒扣的玻璃钟罩住，听见的风声是失真的，看见的阳光带着棱镜的折射，我却以为是真实的世界。这也是作者们创立Gapminder的目的所在，消除全球误解。 正确的答案是C B C C C B C A C A C C A！ 当我发现我大错特错的时候，极度的震撼带来极致的好奇，特别是这段话的出现貌似绝大多数人对这个世界的理解都是错误的。并且不仅仅是错误，而是系统性的错误。人类的答案却是毫无例外地偏向一个方向。相对于事实，每组人群都普遍相信这个世界是更加可怕、更加暴力，而且更加没有希望的。简单来说，人们想象中的世界比真实的世界更加夸张。这是为什么？如果是随机的分布，倒显得正常，偏向性的回答预示了一个根源性的回答：我们情绪化的本能和过分情绪化的世界观！ 作者用整整一本书的经历和数据阐述了各个方面的错误认知，读着读着时常有‘啊，是的，我有这样子想过，原来根本原因是这个，原来应该这样子思考’的想法。自诩接受过高等教育，却发现很多时候的思考是由本能占据的，简单粗暴地直接归因，那些看似成熟的归因，本质是大脑为节省能量的 “认知吝啬”，逐年的积累导致看待世界出现了偏差。那么怎么解决这个问题呢？实事求是！ ","date":"313140-40-110","objectID":"/reading-factfulness/:1:0","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"实事求是！ 我想这四个字在中国人心中应该是再熟悉不过了，但是当我阅读书中的事件时，常常发现我却没有贯彻它。我会把作者关于实事求是的部分节选出来，作为日后复习的提醒。 ","date":"313140-40-110","objectID":"/reading-factfulness/:2:0","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"一分为二 人们似乎总是喜欢把事物一分为二为两个类别，而这两个类别又是互相对立、互相矛盾的，并且这两个类别之间存在着一道巨大的鸿沟。我们也会探讨，正是这种一分为二的本能，促使我们把世界和人群都分成两个不同的类别：富有的和贫穷的。 我的学生谈论“他们”和“我们”​，其他人经常谈论“发展中国家”和“发达国家”​。你通常也会使用各种类似的标签，那么这有什么问题呢？记者、政治家、社会活动家、教师和学者，都习惯于使用这些标签。当人们说“发展中国家”和“发达国家”​，他们通常真正想表达的意思是，​“贫穷的国家”和“富裕的国家”​。 在当今的世界，更多的人生活在中间地带，不再有一道鸿沟能够简单地划分西方国家与其他国家、发达国家和发展中国家、富国和穷国。所以我们不应当再继续使用那些一分为二的简单标签了。 人生活在中间地带，不再有一道鸿沟能够简单地划分西方国家与其他国家、发达国家和发展中国家、富国和穷国。所以我们不应当再继续使用那些一分为二的简单标签了。 要想有效地控制我们一分为二的错误本能，我们就要坚持寻找绝大多数。 我们要注意只比较平均数的做法。平均数之外，我们还要注意数据的实际分布。如果两组数据的分布出现了重叠，那么有可能两组之间的鸿沟并不存在。 我们要注意只比较极端情况的做法。在所有的群体、国家或者国民中，总会有极端情况的存在，总会有顶层和底层。而顶层和底层之间的差别，有时候是极端不公平的。即便如此，大多数仍然分布在中间状态，而在中间并不存在鸿沟。 我们要注意只俯视不仰视的做法。记住俯视会带来错觉，一切看起来都一样矮，但是事实并非如此。 ","date":"313140-40-110","objectID":"/reading-factfulness/:2:1","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"负面思维 世界正在变得更坏，是一个重大的误解。 我们对坏事总会比好事更加关注。这种负面思维的本能就是我们重大误解背后的第二个原因。我们总是很容易就可以注意到世界上发生的所有坏事情，而非常难以发现好事情正在发生。数以10亿次的进步和提高都得不到报道。请不要误解我的意思，我不是说应该用那些微不足道的正面消息来平衡那些负面消息。我指的是那些改变世界的根本性进步。然而这些进步都是以很缓慢、很分散、聚沙成塔、积少成多的方式发生的，所以并不太具备新闻价值。而这些无声无息发生着的人类的进步，成就了人类的奇迹。 20年前你多大？请闭上你的双眼几秒，然后回忆一下20年前的自己，这20年间，你自己的世界变化了多少？是很多还是很少？在过去的20年间，这个世界变化很大。20年前，这个世界上29%的人口仍然生活在极度贫困状态中，而现在这个数字是9%。然而我们却很悲观。我们这些生活在第四级的人，仍然不停地在电视上看到极度贫困的人们，似乎一切都没有改变。 我们不应当无视当今世界上仍然在发生的各种悲剧，但是回顾历史，我们也应该清晰地认识到，这个世界正在变得越来越透明，困难的地区也越来越容易得到帮助。事实上，几乎所有的国家都经历了全方位的进步。 之所以大多数人会认为这个世界正在变得更坏，很大程度上是因为我们的负面思维的本能。这种负面思维的本能体现在三个方面：第一，我们对过去错误的记忆；第二，媒体和社会活动家对于负面新闻的选择性报道；第三，我们总是觉得，只要有坏的事情发生，就不应该认为世界是在变好。 事情，可以是不好的，但同时也在变得更好。不好和更好可以是同时存在的。另外一种可以帮助我们控制负面思维的方法就是对坏消息有思想准备。不要过分美化历史。要做到实事求是，我们就要做到在听到负面消息时能够认识到我们原本就更容易获得负面新闻，而很难听到关于事情在进步的消息。这种现实情况，使得我们系统性地对世界产生了负面的印象，从而产生了焦虑。 要想控制我们的负面情绪，我们就要做到对坏消息要有思想准备。 更好和不好。我们要学会区分状态和趋势，要认识到事情可以同时是不好的，但也是在变得更好的。 好消息不是新闻。好消息是很少得到报道的，因此我们总是听到坏消息。所以当你听到坏消息的时候，可以问一下自己是否我们没有听到好的消息。 循序渐进的进步不是新闻。当一件事情在持续变好，但当中产生了一些小的低谷的时候，通常你只会注意到低谷，而不是整体的趋势。 更多的坏消息并不意味着更多的坏事情。我们能够听到更多的坏消息，有时仅仅是因为我们对坏事情的关注度和监控能力提高了，并不意味着这个世界在变得更坏。 警惕过分美化的历史。人们经常会刻意地美化自己的历史，而国家也经常会刻意地美化自己的历史。 ","date":"313140-40-110","objectID":"/reading-factfulness/:2:2","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"直线思维 我们都有一种直线思维的本能，这种本能使得我们假设，所有的事情都是按照直线的规律来发展的。 在1948年，当我出生的时候，平均每位妇女的生育数量是5个孩子。但是到1965年之后，这个数字快速下降。在过去的50年间，这个数字大幅度下降到了现在的世界平均水平，平均每位妇女生育2.5个孩子。 在几十亿人口脱离贫困的过程中，他们决定生育更少的孩子。他们不需要那么多孩子来作为儿童劳动力在家庭农场工作，他们也不再需要生那么多孩子来对冲儿童夭折的风险。 为什么人口停止增长？未来人口的增长并不是由于更多的新生儿童，也不是因为人类寿命的延长。而是由于今天的儿童成长为成年人，在上面的图表中，​“填充”成为新增的30亿成年人。这种“填充效应”就会在下面的45年中发生，并且在45年之后停止。我们已经达到了新的平衡：每一代人养育的儿童数量不再增长了。 一旦人们不再需要儿童作为劳动力，一旦妇女们得到了更好的教育并且人们获得了避孕的手段，无论他们的文化背景和宗教信仰有怎样的不同，他们都会毫无例外地选择生育更少的孩子，并且让孩子得到更好的教育。 当我们讨论儿童死亡率的问题的时候，我们不需要在未来和现在之间、在理智和良心之间做出选择。因为本质是一样的，只要我们降低儿童的死亡率，我们就能使全人类受益，使今天和未来的人们全都受益。 如何控制直线思维？控制直线思维本能的最佳方式，就是每当我们看到一条直线的时候，我们就应当想到事物的演变有多种方式，不一定是按照直线发展的。并且我们要记得，直线发展的事物在现实中是很少见的。 不要做直线假设。有很多事物的发展并不遵循直线规律，而是遵循S形曲线、滑梯曲线、驼峰曲线或者倍增曲线的规律。没有一个孩子是按照直线的规律长高的，而且也没有父母会认为孩子的身高会无限增长。 ","date":"313140-40-110","objectID":"/reading-factfulness/:2:3","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"恐惧本能 1975 年 10 月 7 日午餐时间，作为瑞典海边小县城急诊室助理医师工作第五天的作者，在资深医师都去吃午餐时，独自面对一架坠毁飞机送来的受伤飞行员。因飞行员穿着特殊连体服，作者找不到拉链开口，又见地板有 “血迹”，听到疑似俄语的话语，误以为是苏联空军飞行员，以为第三次世界大战爆发。后来护士长回来解释，那是瑞典空军飞行员，“血迹” 是被踩的救生颜料，飞行员因在冷海浸泡而发抖抽筋。理性思考永远是困难的，尤其当我们恐惧的时候。当我们的思想被恐惧填满的时候，我们的大脑就没有空间来思索事实了。 我们可以想象在外部世界和我们的大脑之间，有一张护盾，或者一种注意力过滤器。这种过滤器可以保护我们免受外界噪声的干扰。然后让我们想象这个过滤器上面有十个洞，对应十种本能，一分为二、负面思维、直线思维等等。绝大多数信息都不能通过这个过滤器，但是这十个洞会允许那些符合我们十种基本本能的信息通过，而忽略掉那些不符合这些本能的信息。所有的媒体才不会浪费时间去编造那些不符合我们基本本能的故事。**由于我们情绪化的思维本能，以及媒体必须利用我们的情绪化本能来捕获我们的注意力，我们一直拥有一种过度情绪化的世界观。在所有情绪化的本能中，恐惧本能最能影响媒体对于传递给大众的新闻的选择。**然而这里出现了一个悖论：当现实世界变得前所未有的和平和安全的时候，我们看到的却是铺天盖地的关于各种危险的报道。 在正常的情况下，恐惧本能对我们人类是有用的。但是恐惧本能往往对于我们理解这个世界起到反面作用。它误导我们的注意力去关注那些我们最害怕的事情，而不是那些真正危险的事情。本章介绍了一些可怕的事件：自然灾害（1‰的致死原因）​、飞机坠毁（1/10万的致死原因）​、谋杀（7‰的致死原因）​、核泄漏（0%致死原因）和恐怖主义（5‱的致死原因）​。所有这些无一能够构成百分之一的致死原因，然而它们却得到了媒体的大量关注。当然我们应当致力于减少这些死亡的案例。但是我们也必须清醒地认识到，恐惧本能在多大程度上扭曲了我们的关注点。要想理解真正的生命威胁所在，并且有效地保护我们的家人，我们应该克制自己的恐惧本能，并实实在在地分析死亡原因。 因为恐惧和危险是两个不同概念。可怕的事情，仅仅给了我们一种危险的感觉，但是另外一些真正危险的事情则会威胁我们的生命。过度关注可怕的而不是危险的事情，就意味着我们把自己宝贵的注意力放在了错误的方向。恐惧曾经使得我在本应该给飞行员治疗低温综合征的时候误以为第三次世界大战爆发了，也使得人们在本应当关注正在荒漠化的海床和数百万人死于痢疾的时候，却去关注地震、飞机坠毁以及化学物质污染。我希望我的恐惧能够集中在今天真正的威胁上，而不是我们在进化过程中形成的本能。 要做到实事求是，就是当我们感到恐惧的时候，我们能够认识到我们害怕的事情不一定是真正危险的。我们对于暴力、受困以及污染的天然恐惧，会使我们习惯性地过度高估这些风险。的。我们对于暴力、受困以及污染的天然恐惧，会使我们习惯性地过度高估这些风险。 可怕的世界：恐惧vs．现实。我们感受到的世界，比真实的世界更可怕，这是因为我们注意到的信息都是被媒体精心选择过滤过的，而媒体刻意选择那些吓人的信息来吸引我们的注意力。 风险=危险程度×发生的可能性。你面临的真实风险，并不取决于它看起来多么吓人，而在于两个因素：危险的程度和发生的概率。 在采取行动之前，先让自己冷静下来。当你在恐惧中的时候，你会看到一个完全不同的世界。所以不要在恐惧中做决定。 ","date":"313140-40-110","objectID":"/reading-factfulness/:2:4","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"规模错觉 人们总是容易注意局部而忽略整体。这是我们的本能之一。我们总是会注意到一个单一的数字而误判它的重要性。正如在纳卡拉的医院中发生的一样，我们总是对单一事件或者看得见的受害者的重要性产生误判。这两者是规模错觉的最重要的两个方面。 规模错觉的两个方面和负面思维的本能结合在一起，使得我们系统性地低估了这个世界发生的进步。媒体和慈善组织总是习惯于宣传一些看起来很大的数字，并且给我们看一些受苦难的人的照片，使得我们在印象中系统性地低估了真正的比例和世界上发生的进步。 数据表明，在全世界范围内，几乎一半以上的儿童生存率的提高是来自母亲获得了读书和写字的能力。今天更多的儿童能够生存下来，是因为预防性的措施使他们更少得病。受过培训的乡村医生可以给怀孕的母亲们照顾和接生。乡村护士们帮助他们做好免疫工作。父母们可以让儿童吃得饱、穿得暖。周围的人有良好的洗手的卫生习惯。母亲们可以读得懂药瓶上的使用说明。所以当你要投资来提高收入水平在第一级和第二级的人们的健康水平的话，你应该把你有限的资金投入小学、护士教育和疫苗注射上面，而不是急于建造宏伟的医院。 要想控制规模错觉，我们就要关注比例： 对比。大的数字总是看起来很大，而单一数字很容易误导我们。当我们看到一个单一数字的时候，一定要记得做对比，或者做除法，得到某种比例。 二八原则。如果你得到了一个长长的清单，就应该先排序，然后找到最大的几项并且做深入分析。通常这几项的重要性要远大于其他所有项目加在一起的重要性。 比例。数字和比例有可能代表着完全不同的含义。尤其当我们在不同大小的组别之间做对比的时候，比例总是更有意义。具体来讲，我们在对国家和地区进行比较的时候，应该更加关注人均数字。 ","date":"313140-40-110","objectID":"/reading-factfulness/:2:5","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"以偏概全 每个人都会在头脑中自动地进行演绎和归纳。这是下意识的行为，这也不涉及偏见或者受教育程度。 归纳法有时候会使我们错误地把非常不同的人、事物或者国家划分到同一个组，而忽视它们的不同。我们会自动假设我们归到一类的事物是非常相似的。这样我们就经常会犯以偏概全的错误，以我们看到的很少数的非正常案例来给整个群体下结论。 在这个方面，媒体再一次充当了这种本能的朋友。误导性的以偏概全，以及极端典型，都是媒体善用的手段，因为这样他们可以很轻松、很快速地沟通。错误的归纳分类就会导致我们脑海中形成错误典型。 一分为二的本能促使我们把世界分为“我们”和“他们”​，而以偏概全的本能使得“我们”认为“他们”是完全一样的。 要做到实事求是，就是要意识到当我们讨论一个群体的时候，我们的分类可能是错误的。要想控制住我们的以偏概全的本能，我们要经常质疑自己的分类方法。 在同一类别中寻找不同。特别是当一个类别非常巨大的时候，我们应该试图找到有效的办法来将其分得更小、更准确。 在不同类别中寻找相同。如果你发现不同的类别之间存在着巨大的相似性，那么要考虑，你的分类方法有可能是不正确的。 在不同类别中寻找不同。不要假设在一个类别中适用的规则可以在其他类别中同样适用。比如收入水平第四级的人不要假设其他级别的人也适用同样的生活规则。再比如失去意识的士兵和沉睡中的婴儿是不同的。 注意大多数。大多数仅仅意味着超过一半，我们应当具体区分，大多数究竟意味着51%还是99%。 注意极端案例。活灵活现的图片往往会给我们留下深刻的印象，但是它们有可能只代表着极端案例，而不是普遍现象。 不要假设别人是傻瓜。当你发现一些奇怪现象的时候，请保持好奇心和谦卑之心，去探究这现象背后的道理。 ","date":"313140-40-110","objectID":"/reading-factfulness/:2:6","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"命中注定 所谓命中注定本能，就是我们认为一些事物内在的属性将决定其命运，无论是人民、国家、宗教还是文化。这种思想认为所有的落后都是他们的内在本质造成的，而这一点是永远不会改变的。同时为你所处的特定群体宣称一种所谓的命运，也将有利于将这个群体团结在一起，并且产生一种优越感。所以这种命中注定的本能对于强权部落和独裁统治都是很重要的。 社会和文化并不像岩石一样不可改变。它们是在持续变化中的。在非洲五个最大的国家，突尼斯、阿尔及利亚、摩洛哥、利比亚和埃及，当我小的时候，人们普遍认为这些国家是典型的非洲国家。当这几个国家获得了巨大的进步之后，它们却成了非洲国家的例外，而不再被认为是典型的非洲国家了。 我认为，最后脱离贫困的人将是那些生活在偏远、极其贫瘠的土地上，同时又受到战争困扰的农民。他们想要脱贫，当然会面临非常大的困难，但并不是因为他们无法改变的文化，而是因为土壤和战争。 要做到实事求是，就是要认识到很多事情（比如人民、国家、宗教和文化）看起来似乎保持不变，仅仅是因为改变发生得非常缓慢，并且要记住聚沙成塔。要想控制命中注定的本能，就要记住缓慢的改变也仍然是改变。 注意追踪持续的提高。每年小的改变可以在几十年后积累成巨大的改变。 更新你的知识。有些知识很快就会变得过时，技术、国家、社会文化和宗教都在持续的改变当中。 与老年人对话。如果你想弄清楚价值观是如何改变的，请想一想你的祖父母们的价值观和你的价值观有什么不同。 收集文化改变的案例。找到反面的案例来挑战那种认为文化一成不变的说法。 ","date":"313140-40-110","objectID":"/reading-factfulness/:2:7","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"单一视角 我们非常喜欢简单的想法。我们欣赏洞见真相的时刻。我们也享受真正理解到真相的喜悦。当我们拥有了一个简单的想法，并且发现它可以解释很多事情的时候，我们会非常开心，觉得这个世界变得简单了。要多和拥有不同意见的人交流，把这些不同意见看作帮助你真正理解这个世界的有用的资源，而不是仅仅和那些和你有共同观点的人一起交流。 为什么人们总是习惯于用单一视角去理解这个世界呢？我发现了两个主要的原因：其一是政治的意识形态；其二是专业局限性。 我喜欢专家，但是他们都有自己的局限性。首先非常明显的是，专家们都只对自己熟悉的特定领域拥有专业知识，尽管他们往往不承认这一点。几乎我遇到的所有社会活动家，都会有意或无意地夸大他们所致力于解决的问题。致力于保护濒危动物以及动物栖息地的社会活动家，总是会犯我刚刚描述过的那种错误：他们拼命地想让人们关注这些濒危动物，而他们自己却忘记了发生的进步。 人们知道了所取得的进步，就会受到很大的激励，而反复地强调问题的存在，却不会给人们带来任何激励。 你也许听过这样一句谚语：你给你的孩子一把锤子，他就会把所有的东西都看作钉子。知识有时候会成为专家的障碍，使他们看不到真正的解决方案。所有的解决方案，对于解决特定的问题都是很管用的，但是没有任何一种方案可以解决所有的问题。最好的方法就是以多视角来观察这个世界。 数据是有它自己的局限性的。仅仅在数据能够帮助我理解数字背后的现实的时候，我才会喜欢使用数据。莫桑比克的总理都是帕斯库亚尔·莫昆比，他回答说：​“我会看这些数据，但是这些数据并不是太准确，所以我自己会观察每年5月1日参加游行的人。这个游行是我们国家的传统。我就观察这些参加游行的人的脚，看他们穿什么样的鞋。 要想控制单一视角的本能，必须有一个工具箱，而不仅仅是一把锤子。 检查你的想法。不要仅仅专注于那些能够证明你的想法的正确案例，而要多与那些持有不同意见的人讨论。发现自己想法的不足之处。 有限的经验。不要认为你在自己的专业领域之外有什么真知灼见。对自己未知的领域要保持谦逊。同时也要注意到专家也有他们的局限性。 锤子和钉子。当你会熟练地使用某一种工具的时候，你总会尽可能多地使用它。等你花了太多的时间专注于分析某一个问题的时候，有可能会夸大这个问题以及解决方案的重要性。请牢记，没有任何一个工具是万能的。如果你总是习惯于使用锤子的话，那么请多和那些习惯使用改锥、扳手和卷尺的人打交道。多听听来自不同领域的人的意见。 关注数字，但不仅仅关注数字。没有数字，我们无法理解世界，但是仅有数字，我们仍然无法理解世界。请专注于发现数字背后的真实世界。 当心简单的想法和简单的解决方案。人类历史上从来就不缺乏充满了乌托邦式的简单想法的空想家，而最终他们都带来了可怕的结果。我们应当认识到事物的复杂性，学会兼收并蓄以及妥协。我们应当在具体情况具体分析的基础上来解决问题。 ","date":"313140-40-110","objectID":"/reading-factfulness/:2:8","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"归咎他人 当坏事情发生的时候，人们总是试图找到一个清晰而简单的理由去责怪其他人，这就是我们所说的归咎于人的本能。 当有坏事情发生的时候，我们似乎总是很自然想到，一定是有其他人故意做坏事。我们总是倾向于相信有人利用权力或者手段，才能够使得事情发生，否则的话，这个世界就会让人感到不可预测、令人困惑和非常可怕。当我们的注意力集中在思考究竟该揍谁的时候，我们就会停止思考问题产生的真正原因。这就大大地降低了我们真正解决问题或者预防问题的能力。对世界上的绝大多数问题而言，我们不能停止于找到替罪羊，而应该观察理解产生问题的整个系统。 对他人的指责往往能够揭示我们自己的思维模式。当我们在寻找替罪羊的时候，其实反映的是我们内心早已存在的思维模式。 与其简单地指责记者不说实话，我们还不如仔细地反思一下，为什么媒体一定要给我们反映一个扭曲的世界？是记者们刻意为之吗？还是有其他更深层次的原因？因为我们并不是刚刚开始对世界产生错误的认识，而是一直以来我们对世界的认识都是错误的。​ 要做到实事求是，就是当人们开始找替罪羊的时候，你应该认识到这是错误的，并且能够记起，简单地归咎他人只会使你把握不住问题的真正要点，并且无法集中注意力防止类似事件再次发生。要想控制归咎他人的本能，你应该停止寻找替罪羊。 寻找原因，而不是寻找坏人。当坏事情发生的时候不要试图去责怪任何个人或群体。首先接受没有人刻意为之这个事实。然后努力去理解这一事情发生背后的系统性原因。 寻找系统，而不是寻找英雄。当有人号称自己做了什么伟大的业绩的时候，问问自己，如果没有这个人，是否这件事情仍然可以发生？通常是整个系统的有效运行使得好的事情发生了。 ","date":"313140-40-110","objectID":"/reading-factfulness/:2:9","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"情急生乱 当我们陷入恐惧中，并且在紧迫的时间压力下，我们就会过分地思考最坏情景，于是做出非常愚蠢的决定。在事出紧急的压力下，我们的分析能力就会丧失。 请放轻松，这永远都不是真的，事情永远都没有那么紧急，而且事情永远不是非黑即白的选择。情急生乱的本能会驱使我们做出错误的决定。它使得我们感到很大的压力，放大了我们的其他本能，并且阻止了我们分析思考，使得我们仓促地做出决定，在没有深思熟虑的情况下贸然采取行动。 我们天天喊狼来了，结果就是最终人们变得麻木而再不相信这些警报。我们都清楚那个故事的结尾，最终狼真的来了的时候没有人快速地反应，结果所有的羊都被吃光了。 要想控制情急生乱的本能，你需要做到循序渐进。 深呼吸。当你情急生乱的本能被唤醒的时候，你的其他本能也会被激活，而你大脑的分析能力则停止工作了。请给你自己一点时间和更多的信息。绝大多数情况下，你并不需要立即采取行动，以后仍然会有机会。事实也通常不是非黑即白的。 坚持了解基础数据。如果一件事是紧急且很重要的，那么我们必须对它进行持续观测。请警惕那些虽然相关但并不准确的数据，或者那些虽然准确但实际并不相关的数据。只有相关且准确的数据才真正有用。 警惕那些带有偏见的预言家。任何关于未来的预测都是具有不确定性的。所有的预测都必须考虑到未来的不确定性。你应当坚持对预测有一个全面的、包含多种情形分析的了解。永远不要只看最佳或最差情形。并且要用这种预测和历史上发生的事实相对比，来检查这种预测方法的准确度。 小心过激的行动。尽可能了解激烈行动的后果和副作用。了解这一行动的理论依据。应当稳扎稳打地取得现实的进步，并且在过程中持续观测实施效果。通常循序渐进的方案，总会优于大刀阔斧的行动。 ","date":"313140-40-110","objectID":"/reading-factfulness/:2:10","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":[],"content":"致敬汉斯·罗斯林：用数据点亮真相的行者 他以动态气泡图打破“世界崩塌”的迷思，用数据证明全球贫困率下降超50%的进步轨迹；作为无国界医生，在非洲疫区用流行病学调查连接生命与希望；即便身患癌症，仍在化疗期间整理非洲农村电力数据，将《事实》写成对抗认知偏见的宣言。 汉斯·罗斯林用实证精神解构绝望，以国际视野超越文明对立——当他的气泡图在TED舞台绽放，世人终于看见：真相不在非黑即白的叙事里，而在每个被认真丈量的事实中。他留下的，是用理性守护希望的生命范本。直到生命的终点，他仍然保持着对世界的好奇和热情。他勇敢，有创意，严肃认真，永远相信奇迹。 同时致谢安娜·罗斯林·罗朗德，欧拉·罗斯林，献给所有为这本书做贡献的人们，感谢你们写出了如此震撼人心的作品。 ","date":"313140-40-110","objectID":"/reading-factfulness/:3:0","tags":[],"title":"Factfulness","uri":"/reading-factfulness/"},{"categories":null,"content":"关于 LoveIt","date":"20249-49-110","objectID":"/about/","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"  LoveIt 是一个由  Dillon 开发的简洁、优雅且高效的 Hugo 博客主题。 它的原型基于 LeaveIt 主题 和 KeepIt 主题。 Hugo 主题 LoveIt ","date":"20249-49-110","objectID":"/about/:0:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"特性 ","date":"20249-49-110","objectID":"/about/:1:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"性能和 SEO  性能优化：在 Google PageSpeed Insights 中， 99/100 的移动设备得分和 100/100 的桌面设备得分  使用基于 JSON-LD 格式 的 SEO SCHEMA 文件进行 SEO 优化  支持 Google Analytics  支持 Fathom Analytics  支持 Plausible Analytics  支持 Yandex Metrica  支持搜索引擎的网站验证 (Google, Bind, Yandex 和 Baidu)  支持所有第三方库的 CDN  基于 lazysizes 自动转换图片为懒加载 ","date":"20249-49-110","objectID":"/about/:1:1","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"外观和布局  桌面端/移动端 响应式布局  浅色/深色 主题模式  全局一致的设计语言  支持分页  易用和自动展开的文章目录  支持多语言和国际化  美观的 CSS 动画 社交和评论系统  支持 Gravatar 头像  支持本地头像  支持多达 76 种社交链接  支持多达 24 种网站分享  支持 Disqus 评论系统  支持 Gitalk 评论系统  支持 Valine 评论系统  支持 Facebook comments 评论系统  支持 Telegram comments 评论系统  支持 Commento 评论系统  支持 utterances 评论系统  支持 giscus 评论系统 ","date":"20249-49-110","objectID":"/about/:1:2","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"扩展功能  支持基于 Lunr.js 或 algolia 的搜索  支持 Twemoji  支持代码高亮  一键复制代码到剪贴板  支持基于 lightGallery 的图片画廊  支持 Font Awesome 图标的扩展 Markdown 语法  支持上标注释的扩展 Markdown 语法  支持分数的扩展 Markdown 语法  支持基于 $\\KaTeX$ 的数学公式  支持基于 mermaid 的图表 shortcode  支持基于 ECharts 的交互式数据可视化 shortcode  支持基于 Mapbox GL JS 的 Mapbox shortcode  支持基于 APlayer 和 MetingJS 的音乐播放器 shortcode  支持 Bilibili 视频 shortcode  支持多种注释的 shortcode  支持自定义样式的 shortcode  支持自定义脚本的 shortcode  支持基于 TypeIt 的打字动画 shortcode  支持基于 cookieconsent 的 Cookie 许可横幅  支持人物标签的 shortcode … ","date":"20249-49-110","objectID":"/about/:1:3","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"许可协议 LoveIt 根据 MIT 许可协议授权。 更多信息请查看 LICENSE 文件。 ","date":"20249-49-110","objectID":"/about/:2:0","tags":null,"title":"关于我","uri":"/about/"},{"categories":null,"content":"特别感谢 LoveIt 主题中用到了以下项目，感谢它们的作者： normalize.css Font Awesome Simple Icons Animate.css autocomplete Lunr.js algoliasearch lazysizes object-fit-images Twemoji emoji-data lightGallery clipboard.js Sharer.js TypeIt $\\KaTeX$ mermaid ECharts Mapbox GL JS APlayer MetingJS Gitalk Valine cookieconsent ","date":"20249-49-110","objectID":"/about/:3:0","tags":null,"title":"关于我","uri":"/about/"}]